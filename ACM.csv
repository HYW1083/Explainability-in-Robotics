entry_type,title,author,year,journal,booktitle,volume,number,pages,publisher,doi,url,abstract,address,articleno,isbn,issn,issue_date,keywords,location,note,numpages,series
article,Understanding Our Robots With the Help of Human-Centered Explainable AI,"Sanneman, Lindsay",2023,XRDS,,30,1,52?€?57,Association for Computing Machinery,10.1145/3611686,https://doi.org/10.1145/3611686,Insights from the field of human factors can help us design human-centered explanations that enable effective human-robot interaction. Studying explanation techniques according to these human factors will be critical in understanding their efficacy across diverse contexts.,"New York, NY, USA",,,1528-4972,Fall 2023,,,,6,
inproceedings,Understanding Preferred Robot Reaction Times for Human-Robot Handovers Supported by a Deep Learning System,"Leusmann, Jan and Felder, Ludwig and Wang, Chao and Mayer, Sven",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,4?€?13,IEEE Press,,,"Human-human handovers are natural and seamless. To be able to do this, humans optimize towards many factors. One of them is the timing when receiving an object. However, the preferred robot reaction time in Human-Robot handovers is currently unclear. To understand the preferred robot reaction time, we trained a Space-Time-Separable Graph Convolutional Network (STS-GCN) model using motion capture data of human-human handovers. We deployed this system on a robotic arm with live depth camera data. We conducted a user study (N=20) with five robot reaction times. We found that users perceived an early prediction as preferred. Furthermore, we found that designers can adapt this timing to their needs based on six sub-components of user perception. We contribute a ready-to-deploy handover classification model, a preferred handover time for our system, and an approach to determine the preferred robot reaction time for robotic systems.",,,,,,"cobot, handover, human-robot interaction, human-to-robot handover, preferred timing, robot","Melbourne, Australia",,10,HRI '25
inproceedings,3rd Workshop on Explainability in Human-Robot Collaboration: Real-World Concerns,"Yadollahi, Elmira and Dogan, Fethiye Irmak and Romeo, Marta and Kontogiorgos, Dimosthenis and Qian, Peizhu and Zhang, Yan",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1994?€?1996,IEEE Press,,,"Robots powered by AI and machine learning are increasingly capable of collaboration and social interaction with humans, leading to a demand to develop new approaches to ensure their transparency and explainable behaviour. As explainable AI (XAI) seeks to clarify AI decisions, its integration into physical robots often creates an illusion of explainability-raising questions about whether current approaches truly enhance understanding. The 3rd Workshop on Explainability in Human-Robot Collaboration aims to address the real-world concerns associated with developing explainable and transparent robots through a focused, multi-faceted panel discussion and a series of paper presentations. In this workshop, we will focus on refining when and how explanations should be provided, integrating human communication principles to enhance trust and transparency in human-robot collaboration through both technical and user-centred solutions.",,,,,,"explainable robotics, human-centered robot explanations, xai","Melbourne, Australia",,3,HRI '25
inproceedings,Personalised Explainable Robots Using LLMs,Gebell\'{\i,2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1304?€?1308,IEEE Press,,,"In the field of Human-Robot Interaction (HRI), a key challenge lies in enabling humans to comprehend the decisions and behaviours of robots. One promising approach involves leveraging Theory of Mind (ToM) frameworks, wherein a robot estimates the mental model that a user holds about its functioning and compares this with the representation of its internal mental model. This comparison allows the robot to identify potential mismatches and generate communicative actions to bridge such gaps. Effective communication requires the robot to maintain unique mental models for each user and personalise explanations based on past interactions. To address this, we propose an architecture grounded in Large Language Models (LLMs) that operationalises this theoretical framework. We demonstrate the feasibility of this approach through qualitative examples, showcasing responses provided by a robot patrolling a geriatric hospital.",,,,,,"explainability, llm, personalisation, xhri","Melbourne, Australia",,5,HRI '25
inproceedings,Evaluation of a Robot Navigator's Explanations,"Korpan, Raj and Tiourine, Daniel and Chen, Sami and Epstein, Susan L.",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1428?€?1432,IEEE Press,,,"To achieve trust and acceptance from people, robots that navigate complex human environments should be able to explain their behavior. In an online between-subjects study, this paper explores the impact of explanation detail on human understanding, trust, and comfort with robot navigators. It compares three conditions: no textual information, a simple action description, and a detailed explanation. Results indicate that while detailed explanations improve users' confidence in the robot's navigation ability, simpler descriptions are easier to understand. Participants who received detailed explanations, however, also indicated a better overall understanding of the robot's behavior and decision-making process. These findings highlight how varied levels of detail in explanations can build trust and comprehension.",,,,,,"explainable ai, explainable robotics, robot communication, robot navigation","Melbourne, Australia",,5,HRI '25
inproceedings,Explainable Guidance and Justification for Mental Model Alignment in Human-Robot Teams,"Luebbers, Matthew B. and Hayes, Bradley",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,118?€?120,Association for Computing Machinery,10.1145/3610978.3638364,https://doi.org/10.1145/3610978.3638364,"There is potential for humans and autonomous robots to perform tasks collaboratively as teammates, achieving greater performance than either could on their own. Productive teamwork, however, requires a great deal of coordination, with human and robot agents maintaining well-aligned mental models regarding the shared task and each agent's role within it. Achieving this requires live and effective communication, especially as plans change due to shifts in environment knowledge. Our work leverages augmented reality and natural language interfaces to recommend policies to human teammates, explain the rationale of those policies, and justify during times of mismatched expectation, facilitating plan synchronization in partially observable, collaborative human-robot domains.","New York, NY, USA",,9.7984E+12,,,"augmented reality, human-robot teaming, mental models","Boulder, CO, USA",,3,HRI '24
inproceedings,Questioning the Robot: Using Human Non-verbal Cues to Estimate the Need for Explanations,"Kontogiorgos, Dimosthenis and Shah, Julie",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,717?€?728,IEEE Press,,,"As black-box AI systems become increasingly complex, understanding when and how to provide explanations to users is crucial. Multimodal signals, such as facial expressions, offer novel insights into how frequently explanations should be given. This paper explores whether users' facial features can help estimate the need for explanations in a collaborative robot task. We applied three state-of-the-art eXplainable AI (XAI) methods, addressing how, why, and what-if questions, explaining the robot's failure detection model. Each explanation type conveyed information differently: how-explanations described how the model functions, why-explanations provided personalised insights into input-feature-related cues, and what-if-explanations explored alternative scenarios. In a mixed-design study (N = 33), participants performed a robot-assisted pick-and-place task, receiving different explanation types. Our results show that users responded differently to these explanations, with why-explanations being the most preferred and prompting closer alignment in facial expressions with the robot. Contrary to expectations, what-if explanations led to the least alignment and required greater vocal effort. These findings demonstrate how non-verbal cues can guide the frequency and type of explanations (personalised or general) and further highlight the importance of model transparency in human-robot collaboration.",,,,,,"explainability, failure detection, multimodality","Melbourne, Australia",,12,HRI '25
article,Explainable Goal-driven Agents and Robots - A Comprehensive Review,"Sado, Fatai and Loo, Chu Kiong and Liew, Wei Shiung and Kerzel, Matthias and Wermter, Stefan",2023,ACM Comput. Surv.,,55,10,,Association for Computing Machinery,10.1145/3564240,https://doi.org/10.1145/3564240,"Recent applications of autonomous agents and robots have brought attention to crucial trust-related challenges associated with the current generation of artificial intelligence (AI) systems. AI systems based on the connectionist deep learning neural network approach lack capabilities of explaining their decisions and actions to others, despite their great successes. Without symbolic interpretation capabilities, they are ?€?black boxes?€?, which renders their choices or actions opaque, making it difficult to trust them in safety-critical applications. The recent stance on the explainability of AI systems has witnessed several approaches to eXplainable Artificial Intelligence (XAI); however, most of the studies have focused on data-driven XAI systems applied in computational sciences. Studies addressing the increasingly pervasive goal-driven agents and robots are sparse at this point in time. This paper reviews approaches on explainable goal-driven intelligent agents and robots, focusing on techniques for explaining and communicating agents?€? perceptual functions (e.g., senses, vision) and cognitive reasoning (e.g., beliefs, desires, intentions, plans, and goals) with humans in the loop. The review highlights key strategies that emphasize transparency, understandability, and continual learning for explainability. Finally, the paper presents requirements for explainability and suggests a road map for the possible realization of effective goal-driven explainable agents and robots.","New York, NY, USA",211,,0360-0300,Oct-23,"Accountability, continual learning, deep neural network, explainability, explainable AI, goal-driven agents, transparency",,,41,
inproceedings,Toward Explainable Agent Behaviour,"Gimenez-Abalos, Victor",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2740?€?2742,International Foundation for Autonomous Agents and Multiagent Systems,,,"Agents are a special kind of AI-based software in that they interact in complex environments and have increased potential for emergent behaviour, even in isolation. Explaining such behaviour is key to deploying trustworthy AI, but the increasing complexity and opaqueness of agents makes this hard. Beyond narrow-task and instant-based goals, agents may exhibit durative behaviour and be required to have planning or deliberative capabilities, or even to reason over other's behaviours. This precludes machine learning explainability -i.e. explanations over single predictions or actions- from giving complete and useful explanations. There is a need for extending explainability tools. We split the capabilities of agents into several levels, each more abstract, and produce explanations by climbing these levels: from actions, tellic (ends), deliberation, and more. The first two have been solved through frequentist models (Policy-Graphs), and the third is work in progress. We intend to extend this work by adding components for explaining epistemology, agent-agent interaction, norms and values.","Richland, SC",,9.7984E+12,,,"deliberation, desires, explainability, intention inference, multi-agent systems, planning, theory of mind, values alignment","Auckland, New Zealand",,3,AAMAS '24
inproceedings,Towards Multimodal Co-Construction of Explanations for Robots: Combining Inductive Logic Programming and Large Language Models to Explain Robot Faults,"Youssef, Youssef Mahmoud and Hassan, Teena",2024,,Companion Proceedings of the 26th International Conference on Multimodal Interaction,,,228?€?230,Association for Computing Machinery,10.1145/3686215.3689204,https://doi.org/10.1145/3686215.3689204,"This paper explores a hybrid approach to the multimodal co-const-ruction of explanations for robot faults, integrating Inductive Logic Programming (ILP) and Large Language Models (LLMs). As AI and robotics continue to permeate various aspects of daily life, the ability of these systems to explain their actions and failures is crucial for fostering user trust and ensuring safe interactions. We propose a framework that combines the interpretability of ILP, which generates logical rules from data, with the linguistic capabilities of LLMs, which provide natural language explanations. This approach enables the generation of coherent, contextually appropriate explanations that can be tailored to the needs of users.","New York, NY, USA",,9.7984E+12,,,"Co-construc-ting Explanations, In-context Learning, Inductive Logic Programming, Large Language Models","San Jose, Costa Rica",,3,ICMI Companion '24
inproceedings,Understanding On-the-Fly End-User Robot Programming,"Stegner, Laura and Hwang, Yuna and Porfirio, David and Mutlu, Bilge",2024,,Proceedings of the 2024 ACM Designing Interactive Systems Conference,,,2468?€?2480,Association for Computing Machinery,10.1145/3643834.3660721,https://doi.org/10.1145/3643834.3660721,"Novel end-user programming (EUP) tools enable on-the-fly (i.e., spontaneous, easy, and rapid) creation of interactions with robotic systems. These tools are expected to empower users in determining system behavior, although very little is understood about how end users perceive, experience, and use these systems. In this paper, we seek to address this gap by investigating end-user experience with on-the-fly robot EUP. We trained 21 end users to use an existing on-the-fly EUP tool, asked them to create robot interactions for four scenarios, and assessed their overall experience. Our findings provide insight into how these systems should be designed to better support end-user experience with on-the-fly EUP, focusing on user interaction with an automatic program synthesizer that resolves imprecise user input, the use of multimodal inputs to express user intent, and the general process of programming a robot.","New York, NY, USA",,9.7984E+12,,,"End-user Programming, Programming Tools, Robot Programming, Service Robots, Usage Patterns, User Experience, User Study","Copenhagen, Denmark",,13,DIS '24
inproceedings,Towards Explainable Proactive Robot Interactions for Groups of People in Unstructured Environments,"Love, Tamlin and Andriella, Antonio and Aleny\`{a",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,697?€?701,Association for Computing Machinery,10.1145/3610978.3640734,https://doi.org/10.1145/3610978.3640734,"For social robots to be able to operate in unstructured public spaces, they need to be able to gauge complex factors such as human-robot engagement and inter-person social groups, and be able to decide how and with whom to interact. Additionally, such robots should be able to explain their decisions after the fact, to improve accountability and confidence in their behavior. To address this, we present a two-layered proactive system that extracts high-level social features from low-level perceptions and uses these features to make high-level decisions regarding the initiation and maintenance of human robot interactions. With this system outlined, the primary focus of this work is then a novel method to generate counterfactual explanations in response to a variety of contrastive queries. We provide an early proof of concept to illustrate how these explanations can be generated by leveraging the two-layer system.","New York, NY, USA",,9.7984E+12,,,"engagement, explainability, human-robot interaction, proactive decision-making","Boulder, CO, USA",,5,HRI '24
inproceedings,Adapting Robotic Explanations for Robotic Failures in Human Robot Collaboration,"Khanna, Parag",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1863?€?1865,IEEE Press,,,"My research focuses on adapting robotic failure explanations to enhance human-robot collaboration (HRC). I examine how different explanation types and explanation progression strategies impact failure resolution and user satisfaction by conducting a user study with multiple interaction rounds featuring repeated robotic failures and varying explanations. I also created a novel multimodal dataset of human responses to these failures and explanations. By analyzing human behavioral responses, I developed a predictor to anticipate user confusion following a specific robotic explanation at a robotic failure. This predictor enables an adaptive mechanism to dynamically adjust explanations based on user needs, fostering efficient and natural collaboration. This research aims to significantly improve overall user experience in HRC, making collaborations with robots smoother and more intuitive even when failures occur.",,,,,,"hrc, robotic failures and explanations","Melbourne, Australia",,3,HRI '25
inproceedings,A Theoretical Integration of Robot Explanations: From Mechanical and Teleological to Emotional Explanations,"Terada, Kazunori",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1675?€?1679,IEEE Press,,,"In human-robot collaboration, effective communication of robot intentions and actions is crucial for safety and efficiency. While various explanation methods have been proposed, a unified framework integrating different explanation approaches from a theoretical perspective remains a challenge. This paper presents a three-style framework that formally integrates mechanical, teleological, and emotional explanations for robot actions. Building upon traditional mechanical explanations describing causal chains and teleological explanations focusing on goals, we formalized emotional explanations based on utility changes, grounded in Appraisal theory. These three complementary styles were unified through a common mathematical foundation of state transitions and utility functions, enabling systematic selection and integration of explanations according to the situation. We provide guidelines for selecting appropriate explanation styles based on task phases and explanatory purposes, while discussing implementation challenges and future directions including personalization and real-time adaptation mechanisms.",,,,,,"appraisal theory, emotional explanation, explainable robot, human-robot interaction, mechanical explanation, teleological explanation","Melbourne, Australia",,5,HRI '25
inproceedings,Difficulties in Perceiving and Understanding Robot Reliability Changes in a Sequential Binary Task,"Furuya, Hiroshi and Battistel, Laura and Datta Choudhary, Zubin and Gottsacker, Matt and Bruder, Gerd and Welch, Gregory F",2024,,Proceedings of the 2024 ACM Symposium on Spatial User Interaction,,,,Association for Computing Machinery,10.1145/3677386.3682083,https://doi.org/10.1145/3677386.3682083,"Human-robot teams push the boundaries of what both humans and robots can accomplish. In order for the team to function well, the human must accurately assess the robot?€?s capabilities to calibrate the trust between the human and robot. In this paper, we use virtual reality (VR), a widely accepted tool in studying human-robot interaction (HRI), to study human behaviors affecting their detection and understanding of changes in a simulated robot?€?s reliability. We present a human-subject study to see how different reliability change factors may affect this process. Our results demonstrate that participants make judgements about robot reliability before they have accumulated sufficient evidence to make objectively high-confidence inferences about robot reliability. We show that this reliability change observation behavior diverges from behavior expectations based on the probability distribution functions used to describe observation outcomes.","New York, NY, USA",22,9.7984E+12,,,"Human-robot interaction, reliability, trust, virtual reality","Trier, Germany",,11,SUI '24
inproceedings,"Rube-Goldberg Machines, Transparent Technology, and the Morally Competent Robot","Mott, Terran and Williams, Tom",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,634?€?638,Association for Computing Machinery,10.1145/3568294.3580163,https://doi.org/10.1145/3568294.3580163,"Social robots of the future will need to perceive, reason about, and respond appropriately to ethically sensitive situations. At the same time, policymakers and researchers alike are advocating for increased transparency and explainability in robotics-design principles that help users build accurate mental models and calibrate trust. In this short paper, we consider how Rube Goldberg machines might offer a strong analogy on which to build transparent user interfaces for the intricate, but knowable inner workings of a cognitive architecture's moral reasoning. We present a discussion of these related concepts, a rationale for the suitability of this analogy, and early designs for an initial prototype visualization.","New York, NY, USA",,9.78145E+12,,,"cognitive architectures, robot ethics, transparency","Stockholm, Sweden",,5,HRI '23
inproceedings,Interactive Policy Shaping for Human-Robot Collaboration with Transparent Matrix Overlays,"Brawer, Jake and Ghose, Debasmita and Candon, Kate and Qin, Meiying and Roncone, Alessandro and V\'{a",2023,,Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,525?€?533,Association for Computing Machinery,10.1145/3568162.3576983,https://doi.org/10.1145/3568162.3576983,"One important aspect of effective human--robot collaborations is the ability for robots to adapt quickly to the needs of humans. While techniques like deep reinforcement learning have demonstrated success as sophisticated tools for learning robot policies, the fluency of human-robot collaborations is often limited by these policies' inability to integrate changes to a user's preferences for the task. To address these shortcomings, we propose a novel approach that can modify learned policies at execution time via symbolic if-this-then-that rules corresponding to a modular and superimposable set of low-level constraints on the robot's policy. These rules, which we call Transparent Matrix Overlays, function not only as succinct and explainable descriptions of the robot's current strategy but also as an interface by which a human collaborator can easily alter a robot's policy via verbal commands. We demonstrate the efficacy of this approach on a series of proof-of-concept cooking tasks performed in simulation and on a physical robot.","New York, NY, USA",,9.78145E+12,,,"human-robot collaboration, interactive robot learning, reinforcement learning, symbolic reasoning","Stockholm, Sweden",,9,HRI '23
inproceedings,Using Robot Social Agency Theory to Understand Robots' Linguistic Anthropomorphism,"Emnett, Cloe Z. and Mott, Terran and Williams, Tom",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,447?€?452,Association for Computing Machinery,10.1145/3610978.3640747,https://doi.org/10.1145/3610978.3640747,"Robots' use of natural language is one of the key factors that leads humans to anthropomorphize them. But it is not yet well understood what types of language most lead to such language-based anthropomorphization (or, Linguistic Anthropomorphism). In this paper, we present a brief literature survey that suggests six broad categories of linguistic factors that lead humans to anthropomorphize robots: autonomy, adaptability, directness, politeness, proportionality, and humor. By contextualizing these six factors through the lens of Jackson and Williams' Theory of Social Agency for Human-Robot Interaction, we are able to show how and why these particular factors are those responsible for language-based robot anthropomorphism.","New York, NY, USA",,9.7984E+12,,,"linguistic anthropomorphism, social agency","Boulder, CO, USA",,6,HRI '24
inproceedings,Semantic Scene Understanding for Human-Robot Interaction,"Patel, Maithili and Dogan, Fethiye Irmak and Zeng, Zhen and Baraka, Kim and Chernova, Sonia",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,941?€?943,Association for Computing Machinery,10.1145/3568294.3579960,https://doi.org/10.1145/3568294.3579960,"Service robots will be co-located with human users in an unstructured human-centered environment and will benefit from understanding the user's daily activities, preferences, and needs towards fully assisting them. This workshop aims to explore how abstract semantic knowledge of the user's environment can be used as a context in understanding and grounding information regarding the user's instructions, preferences, habits, and needs. While object semantics have primarily been investigated for robotics in the perception and manipulation domain, recent works have shown the benefits of semantic modeling in a Human-Robot Interaction (HRI) context toward understanding and assisting human users. This workshop focuses on semantic information that can be useful in generalizing and interpreting user instructions, modeling user activities, anticipating user needs, and making the internal reasoning processes of a robot more interpretable to a user. Therefore, the workshop builds on topics from prior workshops such as Learning in HRI, behavior adaptation for assistance, and learning from humans and aims at facilitating cross-pollination across these domains through a common thread of utilizing abstract semantics of the physical world towards robot autonomy in assistive applications. We envision the workshop to touch on research areas such as unobtrusive learning from observations, preference learning, continual learning, enhancing the transparency of autonomous robot behavior, and user adaptation. The workshop aims to gather researchers working on these areas and provide fruitful discussions towards autonomous assistive robots that can learn and ground scene semantics for enhancing HRI.","New York, NY, USA",,9.78145E+12,,,"human-centered autonomy, robot learning, scene semantics","Stockholm, Sweden",,3,HRI '23
inproceedings,GestLLM: Advanced Hand Gesture Interpretation via Large Language Models for Human-Robot Interaction,"Kobzarev, Oleg and Lykov, Artem and Tsetserukou, Dzmitry",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1413?€?1417,IEEE Press,,,"This paper introduces GestLLM, an advanced system for human-robot interaction that enables intuitive robot control through hand gestures. Unlike conventional systems, which rely on a limited set of predefined gestures, GestLLM leverages large language models and feature extraction via MediaPipe [1] to interpret a diverse range of gestures. This integration addresses key limitations in existing systems, such as restricted gesture flexibility and the inability to recognize complex or unconventional gestures commonly used in human communication.By combining state-of-the-art feature extraction and language model capabilities, GestLLM achieves performance comparable to leading vision-language models while supporting gestures underrepresented in traditional datasets. For example, this includes gestures from popular culture, such as the ''Vulcan salute'' from Star Trek, without any additional pretraining, prompt engineering, etc. This flexibility enhances the naturalness and inclusivity of robot control, making interactions more intuitive and user-friendly.GestLLM provides a significant step forward in gesture-based interaction, enabling robots to understand and respond to a wide variety of hand gestures effectively. This paper outlines its design, implementation, and evaluation, demonstrating its potential applications in advanced human-robot collaboration, assistive robotics, and interactive entertainment.",,,,,,"gesture recognition, llm, robot control","Melbourne, Australia",,5,HRI '25
inproceedings,Explainable Human-Robot Training and Cooperation with Augmented Reality,"Wang, Chao and Belardinelli, Anna and Hasler, Stephan and Stouraitis, Theodoros and Tanneberg, Daniel and Gienger, Michael",2023,,Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3544549.3583889,https://doi.org/10.1145/3544549.3583889,"The current spread of social and assistive robotics applications is increasingly highlighting the need for robots that can be easily taught and interacted with, even by users with no technical background. Still, it is often difficult to grasp what such robots know or to assess if a correct representation of the task is being formed. Augmented Reality (AR) has the potential to bridge this gap. We demonstrate three use cases where AR design elements enhance the explainability and efficiency of human-robot interaction: 1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the robot showing its plan for solving novel tasks in AR to a human for validation, and 3) a robot communicating its intentions via AR while assisting people with limited mobility during daily activities.","New York, NY, USA",449,9.78145E+12,,,"augmented reality, explainability, human-robot interaction","Hamburg, Germany",,5,CHI EA '23
inproceedings,Explainability for Human-Robot Collaboration,"Yadollahi, Elmira and Romeo, Marta and Dogan, Fethiye Irmak and Johal, Wafa and De Graaf, Maartje and Levy-Tzedek, Shelly and Leite, Iolanda",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,1364?€?1366,Association for Computing Machinery,10.1145/3610978.3638154,https://doi.org/10.1145/3610978.3638154,"In human-robot collaboration, explainability bridges the communication gap between complex machine functionalities and humans. An active area of investigation in robotics and AI is understanding and generating explanations that can enhance collaboration and mutual understanding between humans and machines. A key to achieving such seamless collaborations is understanding end-users, whether naive or expert, and tailoring explanation features that are intuitive, user-centred, and contextually relevant. Advancing on the topic not only includes modelling humans' expectations for generating the explanations but also requires the development of metrics to evaluate generated explanations and assess how effectively autonomous systems communicate their intentions, actions, and decision-making rationale. This workshop is designed to tackle the nuanced role of explainability in enhancing the efficiency, safety, and trust in human-robot collaboration. It aims to initiate discussions on the importance of generating and evaluating explainability features developed in autonomous agents. Simultaneously, it addresses various challenges, including bias in explainability and downsides of explainability and deception in human-robot interaction.","New York, NY, USA",,9.7984E+12,,,"XAI, explainable robotics, human-centered robot explanations","Boulder, CO, USA",,3,HRI '24
inproceedings,Effects of Incoherence in Multimodal Explanations of Robot Failures,"Pramanick, Pradip and Raggioli, Luca and Rossi, Alessandra and Rossi, Silvia",2024,,Companion Proceedings of the 26th International Conference on Multimodal Interaction,,,6?€?10,Association for Computing Machinery,10.1145/3686215.3690155,https://doi.org/10.1145/3686215.3690155,"Providing explanations of a robot?€?s behavior is a key enabler of trust in robots. Such explanations should be intuitive to people who are not experts in robotics. Prior research suggests that using multiple modalities to deliver explanations improves clarity. However, current methods for generating multimodal explanations neither assess nor ensure the coherence of the information across modalities. Here, we present an experiment to understand the effect of possible incoherence in multimodal explanations. We perform a user study asking participants to observe a series of robot failures and predict the reason for failure when provided with a controlled variation of multimodal explanations. Specifically, we present a methodology to compare incoherent and coherent explanations, aiming to understand their impact on perceiving robot failures.","New York, NY, USA",,9.7984E+12,,,"Explainable Artificial Intelligence, Human-Robot Interaction, Multimodal Explanation, Multimodal Interaction, User Studies","San Jose, Costa Rica",,5,ICMI Companion '24
inproceedings,Understanding Large-Language Model (LLM)-powered Human-Robot Interaction,"Kim, Callie Y. and Lee, Christine P. and Mutlu, Bilge",2024,,Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,371?€?380,Association for Computing Machinery,10.1145/3610977.3634966,https://doi.org/10.1145/3610977.3634966,"Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.","New York, NY, USA",,9.7984E+12,,,"human-robot interaction, large language models, social robots","Boulder, CO, USA",,10,HRI '24
inproceedings,An Exploratory Study on People's Intuitive Understanding of Expressive Robot Behavior,"Van Otterdijk, Marieke and Lindblom, Diana Saplacan and Laeng, Bruno and Torresen, Jim",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,1072?€?1076,Association for Computing Machinery,10.1145/3610978.3640568,https://doi.org/10.1145/3610978.3640568,"Robots are anticipated to communicate with humans in our social context. Using nonverbal communication by robots increases communication comprehension because humans intuitively understand such behavior due to their experience of human-human interaction. Thus, this behavior is suitable for studying what makes robot motion intuitive to understand in human-robot interaction (HRI). We study how robot features help humans intuitively grasp expressive robot gestures, concentrating on what makes behavior easy to interpret. After watching eighteen nine-second videos of three robot kinds demonstrating expressive robot actions, 50 participants completed an open-ended survey. Our findings highlight the inputs, mediating factors, and outputs that users reported based on observing examples of expressive robot behavior. These insights are a starting point for analyzing robot behavior from the perspective of intuition and provide a foundation for a theoretical framework for intuition in HRI.","New York, NY, USA",,9.7984E+12,,,"expressive movement, human-robot interaction, intuition, social robotics","Boulder, CO, USA",,5,HRI '24
article,The Journey or the Destination: The Impact of Transparency and Goal Attainment on Trust in Human-Robot Teams,"Kox, Esther S. and van den Boogaard, Juul and Turjaka, Vesa and Kerstholt, Jos\'{e",2024,J. Hum.-Robot Interact.,,14,2,,Association for Computing Machinery,10.1145/3702245,https://doi.org/10.1145/3702245,"As robots gain autonomy, human-robot task delegation can become more goal-oriented; specifying what to do rather than how. This can lead to unexpected robot behaviour. We investigated the effect of transparency and outcome on the perceived trustworthiness of a robot that deviates from the expected manner to reach a delegated goal. Participants (N (=) 82) engaged in a virtual military mission as a Human-Robot Team using a 2 (times) 2 between-subjects design (low vs. high transparency, positive vs. negative outcome). Participants received training on the expected manner to reach the mission?€?s goal. In the actual mission, the robot deviated from the planned path. We manipulated whether the robot explained its deviation and whether the outcome was better or worse than the original plan. Results showed that transparency contributed to higher and more stable levels of trust, without increasing subjective workload. While the robot?€?s deviation led to a violation of trust in the low transparency condition, trust remained stable in the high transparency condition, indicating a buffering effect of transparency on trust in case of unexpected behaviour. The impact of outcome on trust was consistent across transparency conditions. Our findings underscore the role of transparency as a tool for fostering human-robot trust.","New York, NY, USA",23,,,Jun-25,"Human-Autonomy Teaming, Trust, Delegation, Transparency, Outcome",,,23,
inproceedings,Towards Robotic Companions: Understanding Handler-Guide Dog Interactions for Informed Guide Dog Robot Design,"Hwang, Hochul and Jung, Hee-Tae and Giudice, Nicholas A and Biswas, Joydeep and Lee, Sunghoon Ivan and Kim, Donghyun",2024,,Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3613904.3642181,https://doi.org/10.1145/3613904.3642181,"Dog guides are favored by blind and low-vision (BLV) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. However, only a relatively small proportion of BLV individuals work with dog guides due to their limited availability and associated maintenance responsibilities. There is considerable recent interest in addressing this challenge by developing legged guide dog robots. This study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. We conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. Thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. Grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the BLV community.","New York, NY, USA",596,9.7984E+12,,,"Accessibility, Individuals with Disabilities &amp; Assistive Technologies, Interview, Robot","Honolulu, HI, USA",,20,CHI '24
inproceedings,REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations,"Khanna, Parag and Naoum, Andreas and Yadollahi, Elmira and Bj\",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1032?€?1036,IEEE Press,,,"This work presents REFLEX: Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings. It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions. By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures.",,,,,,"dataset, explainable ai, human robot interaction, robotic failures","Melbourne, Australia",,5,HRI '25
inproceedings,When Do People Want an Explanation from a Robot?,"Wachowiak, Lennart and Fenn, Andrew and Kamran, Haris and Coles, Andrew and Celiktutan, Oya and Canal, Gerard",2024,,Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,752?€?761,Association for Computing Machinery,10.1145/3610977.3634990,https://doi.org/10.1145/3610977.3634990,"Explanations are a critical topic in AI and robotics, and their importance in generating trust and allowing for successful human-robot interactions has been widely recognized. However, it is still an open question when and in what interaction contexts users most want an explanation from a robot. In our pre-registered study with 186 participants, we set out to identify a set of scenarios in which users show a strong need for explanations. Participants are shown 16 videos portraying seven distinct situation types, from successful human-robot interactions to robot errors and robot inabilities. Afterwards, they are asked to indicate if and how they wish the robot to communicate subsequent to the interaction in the video. The results provide a set of interactions, grounded in literature and verified empirically, in which people show the need for an explanation. Moreover, we can rank these scenarios by how strongly users think an explanation is necessary and find statistically significant differences. Comparing giving explanations with other possible response types, such as the robot apologizing or asking for help, we find that why-explanations are always among the two highest-rated responses, with the exception of when the robot simply acts normally and successfully. This stands in stark contrast to the other possible response types that are useful in a much more restricted set of situations. Lastly, we test for factors of an individual that might influence their response preferences, for example, their general attitude towards robots, but find no significant correlations. Our results can guide roboticists in designing more user-centered and transparent interactions and let explainability researchers develop more pinpointed explanations.","New York, NY, USA",,9.7984E+12,,,"error mitigation, explainability, hri, human-agent interaction, user study, user-centered ai, xai","Boulder, CO, USA",,10,HRI '24
inproceedings,NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features,"Miyake, Kentaro and Ito, Hiroyoshi and Faloutsos, Christos and Matsumoto, Hirotomo and Morishima, Atsuyuki",2024,,Proceedings of the ACM Web Conference 2024,,,2542?€?2551,Association for Computing Machinery,10.1145/3589334.3647982,https://doi.org/10.1145/3589334.3647982,"Predicting how social networks change in the future is important in many applications. Results in social network research have shown that the change in the network can be explained by a small number of concepts, such as","New York, NY, USA",,9.7984E+12,,,"multi-agent system, network science, reinforcement learning, time-series","Singapore, Singapore",,10,WWW '24
article,Human Understanding and Perception of Unanticipated Robot Action in the Context of Physical Interaction,"Abe, Naoko and Hu, Yue and Benallegue, Mehdi and Yamanobe, Natsuki and Venture, Gentiane and Yoshida, Eiichi",2024,J. Hum.-Robot Interact.,,13,1,,Association for Computing Machinery,10.1145/3643458,https://doi.org/10.1145/3643458,"Anticipating a future scenario where the robot initiates its own actions and behaves voluntarily when collaborating with humans, our research focuses on human understanding and perception of unanticipated robot actions during physical human-robot interaction. While the current literature searches for key factors that make the human-robot collaboration successful, the question of how people experience the robot?€?s unanticipated action as cooperative or uncooperative seems to remain open. We designed a game-based experiment (N = 35) where the participant played a ?€?catch-falling-coins?€? game by moving a robotic arm. Our experiment introduced unanticipated robot actions in an ?€?active session?€? where the robot targeted higher-valued coins without first informing the participants. Through semi-structured interviews and statistical analysis of questionnaires (Big Five Personality Test, SAM, NARS and CH33), we examined the participants?€? understanding of the robot?€?s ?€?intention?€? and their positive or negative perception of the robot as cooperative or uncooperative. Among the participants who understood that the robot?€?s ?€?intention?€? was to catch the higher-valued coins, the majority of them reported a positive perception of the robot (cooperative or helpful) while this was not the case among those who did not understand the robot?€?s intention. We also observed relevant relationships between some personality traits and a person?€?s understanding of the robot?€?s intention. Qualitative analysis of the interviews allowed us to structure the process of perception change during the game into three phases: confusion, investigation, and adaptation. We believe that our research contributes to the study of human perception, and particularly to the relationship between a human?€?s understanding of unanticipated robot actions and their positive or negative perception of the robot.","New York, NY, USA",9,,,Mar-24,"Unanticipated robot action, physical human-robot interaction, cooperativeness, human perception, human-robot collaboration",,,26,
inproceedings,Towards Adaptive Explanation with Social Robot in Human-XAI Interaction,"Fukuchi, Yosuke and Yamada, Seiji",2024,,Proceedings of the 12th International Conference on Human-Agent Interaction,,,353?€?355,Association for Computing Machinery,10.1145/3687272.3690879,https://doi.org/10.1145/3687272.3690879,"Communication robots have the potential to contribute to effective human-XAI interaction as an interface that goes beyond textual or graphical explanations. However, it is not clear how we can develop an adaptive strategy to use a robot?€?s physical and vocal expressions depending on the context in dynamic interactions. This paper proposes a method for a communication robot to decide where to emphasize XAI-generated explanations with physical expressions. In the method, a user model predicts the effect of emphasizing certain points on a user and aims to minimize the expected difference between predicted user decisions and AI-suggested ones. We conducted a user study to investigate how emphasis selection with our method affects the performance of user decisions. The results suggest that our method guides a part of users to better decisions when the performance of the AI suggestion is high.","New York, NY, USA",,9.7984E+12,,,"Explainable AI, Human-XAI interaction, Intelligent decision-support system, Saliency map, Stock trading","Swansea, United Kingdom",,3,HAI '24
article,Understanding Human Dynamic Sampling Objectives to Enable Robot-assisted Scientific Decision Making,"Liu, Shipeng and Wilson, Cristina G. and Krishnamachari, Bhaskar and Qian, Feifei",2024,J. Hum.-Robot Interact.,,13,1,,Association for Computing Machinery,10.1145/3623383,https://doi.org/10.1145/3623383,"Truly collaborative scientific field data collection between human scientists and autonomous robot systems requires a shared understanding of the search objectives and tradeoffs faced when making decisions. Therefore, critical to developing intelligent robots to aid human experts is an understanding of how scientists make such decisions and how they adapt their data collection strategies when presented with new information in situ. In this study, we examined the dynamic data collection decisions of 108 expert geoscience researchers using a simulated field scenario. Human data collection behaviors suggested two distinct objectives: an information-based objective to maximize information coverage and a discrepancy-based objective to maximize hypothesis verification. We developed a highly simplified quantitative decision model that allows the robot to predict potential human data collection locations based on the two observed human data collection objectives. Predictions from the simple model revealed a transition from information-based to discrepancy-based objective as the level of information increased. The findings will allow robotic teammates to connect experts?€? dynamic science objectives with the adaptation of their sampling behaviors and, in the long term, enable the development of more cognitively compatible robotic field assistants.","New York, NY, USA",4,,,Mar-24,"Decision making, robot-assisted scientific exploration, human cognitive model",,,17,
inproceedings,Contextual Understanding of Teen &amp; Youth Privacy Perceptions with Social Robots,"Levinson, Leigh",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,1869?€?1871,IEEE Press,,,"One of the biggest concerns voiced by families about child-centered robots in homes is privacy. My work uses co-design to better understand how youth, their family, and their clinicians formalize a youth's right to privacy with social robots. In particular, I highlight teen voices who are often overlooked in HRI. My main takeaways emphasize how the unique positioning of a robot as a non-authoritative, yet prosocial 'other' presence means stakeholders have lower perceptions of privacy when data collection and sharing is clearly defined by the robot's task. Furthermore, a social robot is situated among a teen's pre-existing relationships and responsibilities, framing privacy with robots from a relational perspective and elevating the need to clearly define the appropriate social roles for robots. In future work, I will couple these takeaways with youth's lived experiences with robots to see how privacy is contextualized further by the use of social robots in youth spaces.",,,,,,"adolescence, boundary management, child-robot interaction, co-design, mixed-method, privacy, social robots","Melbourne, Australia",,3,HRI '25
inproceedings,LayerShift: Reconfigurable Layer Expression Using Robotic Transparent Displays,"Kusabuka, Takahiro and Chigira, Hiroshi and Mochizuki, Takayoshi",2023,,Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,,,,Association for Computing Machinery,10.1145/3586182.3615812,https://doi.org/10.1145/3586182.3615812,"In this paper, we propose a new robotic display equipped with a transparent display, whose position and posture can be controlled, and a design space that uses it. Robotic displays have been researched in which the display is mounted on a movable robotic arm to extend the expression through motion expression in addition to visual expression. In this paper, we propose combining a transparent display with a robotic display to extend the design space of the robotic display through the characteristics of transparent display. We implement a prototype and propose a design space for a single display and multiple cooperating displays.","New York, NY, USA",58,9.7984E+12,,,"AR, Shape-changing display, robotic display, transparent display","San Francisco, CA, USA",,3,UIST '23 Adjunct
article,"A Human-Centered View of Continual Learning: Understanding Interactions, Teaching Patterns, and Perceptions of Human Users Toward a Continual Learning Robot in Repeated Interactions","Ayub, Ali and De Francesco, Zachary and Mehta, Jainish and Yaakoub Agha, Khaled and Holthaus, Patrick and Nehaniv, Chrystopher L. and Dautenhahn, Kerstin",2024,J. Hum.-Robot Interact.,,13,4,,Association for Computing Machinery,10.1145/3659110,https://doi.org/10.1145/3659110,"Continual learning (CL) has emerged as an important avenue of research in recent years, at the intersection of Machine Learning (ML) and Human?€?Robot Interaction (HRI), to allow robots to continually learn in their environments over long-term interactions with humans. Most research in CL, however, has been robot-centered to develop CL algorithms that can quickly learn new information on systematically collected static datasets. In this article, we take a human-centered approach to CL, to understand how humans interact with, teach, and perceive CL robots over the long term, and if there are variations in their teaching styles. We developed a socially guided CL system that integrates CL models for object recognition with a mobile manipulator robot and allows humans to directly teach and test the robot in real time over multiple sessions. We conducted an in-person study with 60 participants who interacted with the CL robot in 300 sessions with 5 sessions per participant. In this between-participant study, we used three different CL models deployed on a mobile manipulator robot. An extensive qualitative and quantitative analysis of the data collected in the study shows that there is significant variation among the teaching styles of individual users indicating the need for personalized adaptation to their distinct teaching styles. Our analysis shows that the constrained experimental setups that have been widely used to test most CL models are not adequate, as real users interact with and teach CL robots in a variety of ways. Finally, our analysis shows that although users have concerns about CL robots being deployed in our daily lives, they mention that with further improvements CL robots could assist older adults and people with disabilities in their homes.","New York, NY, USA",52,,,Dec-24,"Continual learning, perceptions of robots, robot learning from human teachers, long-term human?€?robot interaction",,,39,
inproceedings,JEDAI: A System for Skill-Aligned Explainable Robot Planning,"Shah, Naman and Verma, Pulkit and Angle, Trevor and Srivastava, Siddharth",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,1917?€?1919,International Foundation for Autonomous Agents and Multiagent Systems,,,"This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and helps improve their understanding of AI planning as well as the limits and capabilities of the underlying robot system.","Richland, SC",,9.78145E+12,,,"ai in education, explanations, robotics, task and motion planning","Virtual Event, New Zealand",,3,AAMAS '22
inbook,REX: Designing User-centered Repair and Explanations to Address Robot Failures,"Lee, Christine P and Praveena, Pragathi and Mutlu, Bilge",2024,,Proceedings of the 2024 ACM Designing Interactive Systems Conference,,,2911?€?2925,Association for Computing Machinery,,https://doi.org/10.1145/3643834.3661559,"Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors?€?safety, privacy, and complexity?€?that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.","New York, NY, USA",,9.7984E+12,,,,,,15,
inproceedings,Understanding Expectations for a Robotic Guide Dog for Visually Impaired People,"Kim, J. Taery and Byrd, Morgan and Crandell, Jack L and Walker, Bruce N. and Turk, Greg and Ha, Sehoon",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,262?€?271,IEEE Press,,,"Robotic guide dogs hold significant potential to enhance the autonomy and mobility of blind or visually impaired (BVI) individuals by offering universal assistance over unstructured terrains at affordable costs. However, the design of robotic guide dogs remains underexplored, particularly in systematic aspects such as gait controllers, navigation behaviors, interaction methods, and verbal explanations. Our study addresses this gap by conducting user studies with 18 BVI participants, comprising 15 cane users and three guide dog users. Participants interacted with a quadrupedal robot and provided both quantitative and qualitative feedback. Our study revealed several design implications, such as a preference for a learning-based controller and a rigid handle, gradual turns with asymmetric speeds, semantic communication methods, and explainability. The study also highlighted the importance of customization to support users with diverse backgrounds and preferences, along with practical concerns such as battery life, maintenance, and weather issues. These findings offer valuable insights and design implications for future research and development of robotic guide dogs.",,,,,,"legged robots, physically assistive devices","Melbourne, Australia",,10,HRI '25
inproceedings,Coupling of Task and Partner Model: Investigating the Intra-Individual Variability in Gaze during Human?€?Robot Explanatory Dialogue,"Singh, Amit and Rohlfing, Katharina J.",2024,,Companion Proceedings of the 26th International Conference on Multimodal Interaction,,,218?€?224,Association for Computing Machinery,10.1145/3686215.3689202,https://doi.org/10.1145/3686215.3689202,"In a successful dialogue in general and a successful explanation in specific, partners need to account for both, the task model (what is relevant for the task) and the partner model (what one can contribute). The phenomenon of coupling between task and the partner model becomes especially interesting in the context of Human?€?Robot Interaction where humans have to deal with unknown capabilities of the robot, which can momentarily be perceived when the robot is unable to contribute to the task. Following research on the path over manner prominence in an action [31, 32, 33], a robot explained actions to a human by emphasizing two aspects ?€? the path (","New York, NY, USA",,9.7984E+12,,,"Explanation, Eyetracking, HRI, Partner Model, Scaffolding","San Jose, Costa Rica",,7,ICMI Companion '24
inproceedings,Explainable AI for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery,"Das, Devleena and Banerjee, Siddhartha and Chernova, Sonia",2021,,Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,351?€?360,Association for Computing Machinery,10.1145/3434073.3444657,https://doi.org/10.1145/3434073.3444657,"With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, ??err, that explains the cause of an unexpected failure during an agent's plan execution to non-experts. In order for error explanations to be meaningful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identification. Additionally, we investigate how such explanations can be autonomously generated, extending an existing encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure and history of past actions, are the most effective for failure and solution identification among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen office environment, and are just as effective as the hand-scripted explanations.","New York, NY, USA",,9.78145E+12,,,"explainable ai, fault recovery","Boulder, CO, USA",,10,HRI '21
article,Mitigating Gender Stereotypes Toward AI Agents Through an eXplainable AI (XAI) Approach,"Duan, Wen and McNeese, Nathan and Freeman, Guo and Li, Lingyuan",2024,Proc. ACM Hum.-Comput. Interact.,,8,CSCW2,,Association for Computing Machinery,10.1145/3686969,https://doi.org/10.1145/3686969,"People often apply gender stereotypes toward computerized agents. Rather than challenging these stereotypes, modern AI technologies controversially use them in creating AI agents that underline stereotypical gendered roles. This approach thus further reinforces the often male-dominated societal gender norms and disenfranchises women and gender non-binary individuals. While this issue has raised concerns in the HCI and CSCW communities, still little is known regarding how to mitigate the negative impacts of embedding gender stereotypes in AI agents. In this paper, we propose an eXplainable AI (XAI) approach to mitigating individuals' gender stereotypes toward AI agents. We conducted an online video vignette experiment with 350 participants randomly assigned to one of the eighteen conditions of a 3 (gender of the agent: woman, man, gender-neutral) x 3 (task gender: feminine, masculine, neutral) x 2 (presence or absence of AI explanation) between-subjects design. Our findings indeed suggest that XAI helped participants avoid applying gender stereotypes toward gendered AI agents, by increasing their understanding of how the agent came to its decision and decreasing their rating of the agent's humanlikeness. We contribute to CSCW research by providing a timely investigation into individuals' gender stereotypes toward the state-of-the-art AI agents and by advancing the empirical understanding of the cognitive processes and mechanisms underlying these gender stereotypes. We also demonstrate how eXplainable AI can effectively suppress the application of social characteristics (i.e., gender stereotypes) toward AI agents by disrupting the said cognitive processes. Insights from this study can inform how future AI technologies should be designed to create a progressive gender reality that will gradually reshape humans' experience and ingrained gender ideologies.","New York, NY, USA",430,,,Nov-24,"anthropomorphism, artificial intelligence, chatbot, explainable ai (xai), gender stereotype, gendered ai",,,35,
article,Understanding the Interaction between Delivery Robots and Other Road and Sidewalk Users: A Study of User-generated Online Videos,"Yu, Xinyan and Hoggenm\",2024,J. Hum.-Robot Interact.,,13,4,,Association for Computing Machinery,10.1145/3677615,https://doi.org/10.1145/3677615,"The deployment of autonomous delivery robots in urban environments presents unique challenges in navigating complex traffic conditions and interacting with diverse road and sidewalk users. Effective communication between robots and road and sidewalk users is crucial to address these challenges. This study investigates real-world encounter scenarios where delivery robots and road and sidewalk users interact, seeking to understand the essential role of communication in ensuring seamless encounters. Following an online ethnography approach, we collected 117 user-generated videos from TikTok and their associated 2,067 comments. Our systematic analysis revealed several design opportunities to augment communication between delivery robots and road and sidewalk users, which include facilitating multi-party path negotiation, managing unexpected robot behaviour via transparency information, and expressing robot limitations to request human assistance. Moreover, the triangulation of video and comments analysis provides a set of design considerations to realise these opportunities. The findings contribute to understanding the operational context of delivery robots and offer insights for designing interactions with road and sidewalk users, facilitating their integration into urban spaces.","New York, NY, USA",59,,,Dec-24,"delivery robots, online ethnography, human-robot interaction",,,32,
inproceedings,Explainable Agents (XAg) by Design,"Rodriguez, Sebastian and Thangarajah, John",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2712?€?2716,International Foundation for Autonomous Agents and Multiagent Systems,,,"The likes of ChatGPT has propelled the use of AI techniques beyond our community's expectations. Along with this, the fear of AI has also risen, in particular around the ability, or lack thereof, of the AI system to explain its behaviours. Explainability is a key element of building trust and an important issue for our community. In this paper we advocate for agents that are explainable-by-design, that is, explainability is built into the development of agents rather than an afterthought. We propose key features of an explainable agent (XAg) system and propose a general framework that enables explainability. We advocate the use of design patterns to develop XAgs and propose a general design pattern that can be used for any agent architecture. We instantiate our framework for goal-based agents and implement the framework for the SARL agent programming language coupled with a state-of-the-art event management system. We make a call to the developers of other agent programming languages (APLs) in our community to follow suit by instantiating the general framework we propose into their APL, perhaps even enhancing the framework we present. We also propose an open repository of design patterns and examples for agent systems. If nothing else, we hope this paper will inspire further work on XAg from the design perspective as it is critical that multi agent systems are explainable by design!","Richland, SC",,9.7984E+12,,,"aose, emas, explainable ai","Auckland, New Zealand",,5,AAMAS '24
inproceedings,Interactively Explaining Robot Policies to Humans in Integrated Virtual and Physical Training Environments,"Qian, Peizhu and Unhelkar, Vaibhav Vasant",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,847?€?851,Association for Computing Machinery,10.1145/3610978.3640656,https://doi.org/10.1145/3610978.3640656,"Policy summarization is a computational paradigm for explaining the behavior and decision-making processes of autonomous robots to humans. It summarizes robot policies via exemplary demonstrations, aiming to improve human understanding of robotic behaviors. This understanding is crucial, especially since users often make critical decisions about robot deployment in the real world. Previous research in policy summarization has predominantly focused on simulated robots and environments, overlooking its application to physically embodied robots. Our work fills this gap by combining current policy summarization methods with a novel, interactive user interface that involves physical interaction with robots. We conduct human-subject experiments to assess our explanation system, focusing on the impact of different explanation modalities in policy summarization. Our findings underscore the unique advantages of combining virtual and physical training environments to effectively communicate robot behavior to human users.","New York, NY, USA",,9.7984E+12,,,"AI-assisted human training, explainable AI, value alignment","Boulder, CO, USA",,5,HRI '24
inproceedings,Children's Interpretation of Emotional Body Language Displayed by a Humanoid Robot: A Case Study,"Consoli, Ilaria and Mattutino, Claudio and Gena, Cristina",2024,,"Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization",,,207?€?210,Association for Computing Machinery,10.1145/3631700.3664888,https://doi.org/10.1145/3631700.3664888,"This paper presents an empirical study that examined how children interpret emotional body language displayed by the humanoid robot NAO. The purpose of the study is to provide insights into how children perceive and respond to emotional cues from robotic agents presenting an empirical evaluation that explores the effectiveness of using a humanoid robot to convey emotions to children. Through the examined results, the study aims to highlight the potential of using humanoid robots in educational and therapeutic contexts.","New York, NY, USA",,9.7984E+12,,,"affective interaction, emotional body language, human robot interaction","Cagliari, Italy",,4,UMAP Adjunct '24
inproceedings,Enhancing User Understanding of Reinforcement Learning Agents Through Visual Explanations,"Amitai, Yotam",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2937?€?2939,International Foundation for Autonomous Agents and Multiagent Systems,,,"With the rapid advancement of Artificial Intelligence, the frequency of interaction between people and autonomous agents is on the rise. Effective human-agent collaboration requires that people understand the agent's behavior. Failing to do so may cause reduced productiveness, misuse, frustration, and even danger. Current explainable AI methods prioritize interpreting the local decisions of an agent, putting less emphasis on the challenge of conveying global behavior. Furthermore, there is a growing demand for explanation methods for agents in sequential decision-making frameworks such as reinforcement learning. Agent strategy summarization methods are used to describe the strategy of an agent to its user through demonstration. The summary's purpose is to maximize the user's understanding of the agent's aptitude by showcasing its behavior in a set of world states, chosen by some importance criteria. Extracting the crucial states from the execution traces of the agent in such a way as to best portray the agent's behavior is a challenging task. My thesis tackles this objective by adding to the equation the context in which the user interacts with the agent. This research proposes novel methods for generating summary-based explanations for reinforcement learning agents","Richland, SC",,9.78145E+12,,,"contrastive, explainable reinforcement learning, interactive","London, United Kingdom",,3,AAMAS '23
inproceedings,Understanding demonstratives produced by agents,"Kimura, Hina and Kobayashi, Harumi and Yasuda, Tetsuya",2024,,Proceedings of the 12th International Conference on Human-Agent Interaction,,,387?€?389,Association for Computing Machinery,10.1145/3687272.3690892,https://doi.org/10.1145/3687272.3690892,"It is essential to investigate how humans interpret a communicative robot?€?s ?€?utterances.?€? This study examined how Japanese participants interpreted an agent?€?s use of demonstratives kono (this) and ano (that) when the agent was either human or robot, since demonstratives are important words for joint attention. In the experiment, participants observed an agent (robot or human agent) uttering an expression using demonstratives with or without eye contact. Then participants were asked to respond the intended referent. Participants?€? eye gaze was also recorded. The results were that, in the use of demonstratives, newly appearing referents were assumed to be referred. Shorter reaction times were observed when eye contact was initially established compared to when there was no eye contact, and when the robot agent spoke rather than the human agent. It suggested participants paid more attention to human agent?€?s referential intention. Interpretation of demonstratives seems to be influenced by agent types.","New York, NY, USA",,9.7984E+12,,,"Demonstratives, Eye Contact, Robot Agent","Swansea, United Kingdom",,3,HAI '24
inproceedings,Towards Sustainable Human-Agent Teams: A Framework for Understanding Human-Agent Team Dynamics,"Prada, Rui and Homan, Astrid C. and van Kleef, Gerben A.",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2696?€?2700,International Foundation for Autonomous Agents and Multiagent Systems,,,"Human-agent teamwork is a promising research stream with great potential to impact society. Research on collaborative AI and human-agent interaction has tackled the problem from several perspectives, but we argue that a focus on teams as a unit and a model for human-agent team dynamics is missing. Such a focus is particularly relevant if we aim at involving agents as active team members and at building sustainable teams over time. A team perspective on human-agent collaboration requires new models that pose challenges for AI and humans alike. AI needs new models to build an understanding of team level variables, such as team structure and cohesion, to be able to monitor the team and act on the team beyond performing the task. Humans, in turn, need to be able to incorporate agents as team members in their mental models of teamwork and integrate them into team processes. Such human-agent team dynamics models should be built taking into account four different levels: individual, interpersonal, team, and organisational. We believe that to fulfill this vision we need to bring together the different fields of AI and social sciences.","Richland, SC",,9.7984E+12,,,"collaborative ai, human-agent interaction, human-agent teams, team dynamics, teamwork","Auckland, New Zealand",,5,AAMAS '24
inproceedings,From Interaction to Impact: Towards Safer AI Agent Through Understanding and Evaluating Mobile UI Operation Impacts,"Zhang, Zhuohao (Jerry) and Schoop, Eldon and Nichols, Jeffrey and Mahajan, Anuj and Swearngin, Amanda",2025,,Proceedings of the 30th International Conference on Intelligent User Interfaces,,,727?€?744,Association for Computing Machinery,10.1145/3708359.3712153,https://doi.org/10.1145/3708359.3712153,"With advances in generative AI, there is increasing work towards creating autonomous agents that can manage daily tasks by operating user interfaces (UIs). While prior research has studied the mechanics of how AI agents might navigate UIs and understand UI structure, the effects of agents and their autonomous actions?€?particularly those that may be risky or irreversible?€?remain under-explored. In this work, we investigate the real-world impacts and consequences of mobile UI actions taken by AI agents. We began by developing a taxonomy of the impacts of mobile UI actions through a series of workshops with domain experts. Following this, we conducted a data synthesis study to gather realistic mobile UI screen traces and action data that users perceive as impactful. We then used our impact categories to annotate our collected data and data repurposed from existing mobile UI navigation datasets. Our quantitative evaluations of different large language models (LLMs) and variants demonstrate how well different LLMs can understand the impacts of mobile UI actions that might be taken by an agent. We show that our taxonomy enhances the reasoning capabilities of these LLMs for understanding the impacts of mobile UI actions, but our findings also reveal significant gaps in their ability to reliably classify more nuanced or complex categories of impact.","New York, NY, USA",,9.7984E+12,,,"AI, LLM, Agent, AI Safety, UI Understanding, UI Operation Impact",,,18,
inproceedings,Exploring the Impact of Explanation Representation on User Satisfaction in Robot Navigation,"Halilovic, Amar and Chandrayan, Vanchha and Krivic, Senka",2024,,Proceedings of the 2024 International Symposium on Technological Advances in Human-Robot Interaction,,,1?€?9,Association for Computing Machinery,10.1145/3648536.3648537,https://doi.org/10.1145/3648536.3648537,"The decisions made by autonomous robots hold substantial influence over how humans perceive their behavior. One way to alleviate potential negative impressions of such decisions by humans and enhance human comprehension of them is through explaining. We introduce visual and textual explanations integrated into robot navigation, considering the surrounding environmental context. To gauge the effectiveness of our approach, we conducted a comprehensive user study, assessing user satisfaction across different forms of explanation representation. Our empirical findings reveal a notable discrepancy in user satisfaction, with significantly higher levels observed for explanations that adopt a multimodal format, as opposed to those relying solely on unimodal representations.","New York, NY, USA",,9.7984E+12,,,"explainable robotics, human-robot interaction, robot navigation","Boulder, CO, USA",,9,TAHRI '24
inproceedings,A Generalizable Architecture for Explaining Robot Failures Using Behavior Trees and Large Language Models,"Tagliamonte, Christian and Maccaline, Daniel and LeMasurier, Gregory and Yanco, Holly A.",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,1038?€?1042,Association for Computing Machinery,10.1145/3610978.3640551,https://doi.org/10.1145/3610978.3640551,"As robots are more commonly being deployed in shared human-robot environments, the need for robots to communicate their failures and answer questions about them becomes increasingly important. This paper describes a generalizable architecture, using Behavior Trees and Large Language Models, to generate explanations and answer follow up questions. We compare responses from our new system to those from existing templated systems, and find that our system produces comparable and accurate results. Finally, we propose a set of user studies to evaluate the effectiveness and understandability of our new explanation architecture.","New York, NY, USA",,9.7984E+12,,,"behavior explanation, behavior trees, large language models, robot explanation generation, robot transparency","Boulder, CO, USA",,5,HRI '24
inproceedings,Understanding Emotional and Cognitive Dynamics in Older Adults Using Personalized Care Robots,"Choi, Narae and Park, Do-Hyung",2024,,Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI),,,163?€?170,Association for Computing Machinery,10.1145/3696271.3696298,https://doi.org/10.1145/3696271.3696298,"The aim of this paper is to comprehensively examine the emotional and cognitive aspects of older adult users who utilize personalized care service robots. Furthermore, this text aims to present a methodology for involving older adults, the end-users, in the design process of social robots. It explores conversation as a key to understanding user perspectives and emotions. Analyzing 191 users (60+), we employed a mixed-method approach, integrating pre-survey data, voice conversations, and function logs. Findings reveal correlations between education, life satisfaction, and robot conversations. Users with conversational experience exhibit increased interactions, revealing distinct clusters (","New York, NY, USA",,9.7984E+12,,,"Conversational interaction, HRI, Mixed methods, Older adult, Phenomenological analysis, Social robot, discriminant analysis",,,8,
inproceedings,MAF-ID: Multi-Agent Framework for Interactive Dubbing through Deep Video Understanding,"Hu, Zhanbin and He, Xiaodong and Pan, Renzhou and Zeng, Xianzhou and Fan, Chenming and Zhu, Qiang",2024,,Proceedings of the 32nd ACM International Conference on Multimedia,,,11261?€?11263,Association for Computing Machinery,10.1145/3664647.3684992,https://doi.org/10.1145/3664647.3684992,"In the domain of video generation, Text-to-video suffers from a notable application gap due to lack of audio that harmonizes with the visual content. Current solutions typically dubbing based solely on the original text used for generate video, which causes a mismatch between the video content and audio details, primarily stems from the lack of understanding of the video's visual modality. Leveraging advancements in multimodal large language model and LLM-based Agent, we propose MAF-ID, a multi-agent interactive framework for video dubbing based on deep video understanding. MAF-ID achieves agent collaboration through the autonomous interaction of three agents, to capture a deep understanding of the video visual content from macro to micro, progressively generate sound effects, voice-overs, and background music that is adaptive to the video. By deeply aligning text, video, and audio modalities, our method significantly enhances the fine-grained coordination between video and audio, making it widely available for AI-generated videos, VLOGs, and other video production scenarios requiring dubbing.","New York, NY, USA",,9.7984E+12,,,"audio generation, multi-agent, video understanding","Melbourne VIC, Australia",,3,MM '24
article,Design Metaphors for Understanding User Expectations of Socially Interactive Robot Embodiments,"Dennler, Nathaniel and Ruan, Changxiao and Hadiwijoyo, Jessica and Chen, Brenna and Nikolaidis, Stefanos and Matari\'{c",2023,J. Hum.-Robot Interact.,,12,2,,Association for Computing Machinery,10.1145/3550489,https://doi.org/10.1145/3550489,"The physical design of a robot suggests expectations of that robot?€?s functionality for human users and collaborators. When those expectations align with the robot?€?s true capabilities, users are more likely to adopt the technologies for their intended use. However, the relationship between expectations and socially interactive robot design is not well understood. This article applies the concept of design metaphors to robot design and contributes the Metaphors for Understanding Functional and Social Anticipated Affordances dataset of 165 extant robots and the expectations users place on them. We used Mechanical Turk to crowd-source user expectation over three user studies. The first study (N = 382) associated crowd-sourced design metaphors to different robot embodiments. The second study (N = 803) assessed initial social expectations of robot embodiments. The final study (N = 805) addressed the degree of abstraction of the design metaphors and the functional expectations projected on robot embodiments. We performed analyses to gain insights into how design metaphors can be used to understand social and functional expectations of robots and how these data can be visualized to be useful for study designers and robot designers. Together, these results can serve to guide robot designers toward aligning user expectations with true robot capabilities, facilitating positive human?€?robot interaction.","New York, NY, USA",21,,,Jun-23,"Socially interactive robots, robot morphology, social perceptions",,,41,
inproceedings,Improving Human-Robot Team Transparency with Eye-tracking based Situation Awareness Assessment,"Aderinto, Favour and Bhagat Smith, Josh and Giolando, Mark-Robin and Baskaran, Prakash and Adams, Julie A.",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,177?€?181,Association for Computing Machinery,10.1145/3610978.3640647,https://doi.org/10.1145/3610978.3640647,Human-robot interactions rely on transparency to foster effective collaboration. Transparency can be assessed through metrics associated with factors such as situation awareness. This manuscript presents an ocular metric to assess situation awareness for human-machine teams. Participants used a decision support system to select a grasp for underwater manipulation. The participants' gaze behavior and visual awareness was analyzed using a wearable eye tracker. An initial analysis that measures saccadic distance provides insight into the requirements of future techniques for objectively assessing situation awareness.,"New York, NY, USA",,9.7984E+12,,,"eye-tracking, human-robot teams, situation awareness, transparency","Boulder, CO, USA",,5,HRI '24
inproceedings,Technical Transparency for Robot Navigation Through AR Visualizations,"Dyck, Leonie and Beierling, Helen and Helmert, Robin and Vollmer, Anna-Lisa",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,720?€?724,Association for Computing Machinery,10.1145/3568294.3580181,https://doi.org/10.1145/3568294.3580181,"Since robots can facilitate our everyday life by assisting us in basic tasks, they are continuously integrated into our life. However, for a robot to establish itself, a user must accept and trust its doing. As the saying goes, you don't trust things you don't understand. Therefore, the base hypothesis of this paper is that providing technical transparency for users can increase understanding of the robot architecture and its behaviors as well as trust and acceptance towards it. In this work, we aim to improve a robot's understanding, trust, and acceptance by displaying transparent visualizations of its intention and perception in augmented reality. We conducted a user study where robot navigation with certain interruptions was demonstrated to two groups. The first group did not have AR visualizations displayed during the first demonstration; in the second demonstration, the visualizations were shown. The second group had the visualizations displayed throughout only one demonstration. Results showed that understanding increased with AR visualizations when prior knowledge had been gained in previous demonstrations.","New York, NY, USA",,9.78145E+12,,,"augmented reality, explainability, human-robot-interaction, transparency","Stockholm, Sweden",,5,HRI '23
article,A Transparency-Based Action Model Implemented in a Robotic Physical Trainer for Improved HRI,"Aharony, Naama and Krakovski, Maya and Edan, Yael",2024,J. Hum.-Robot Interact.,,14,1,,Association for Computing Machinery,10.1145/3700598,https://doi.org/10.1145/3700598,"Transparency is an important aspect of human?€?robot interaction (HRI), as it can improve system trust and usability, leading to improved communication and performance. However, most transparency models focus only on the amount of information given to users. In this article, we propose a bidirectional transparency model, termed the transparency-based action (TBA) model, which allows the robot to take actions based on transparency information received from the human (robot-of-human and human-to-robot), in addition to providing transparency information to the human (robot-to-human). To examine the impact of a three-level (High, Medium and Low) TBA model on acceptance and HRI, we first implemented the model on a robotic system trainer in two pilot studies (with students as participants). Based on the results of these studies, the Medium TBA level was not included in the subsequent main experiment, which was conducted with older adults (aged 75?€?85). In that experiment, two TBA levels were compared: Low (basic information including only robot-to-human transparency) and High (including additional information relating to predicted outcomes with robot-of-human and human-to-robot transparency). The results revealed a statistically significant difference between the two TBA levels of the model in terms of perceived usefulness, ease of use, and attitude. The High TBA level was preferred by users and yielded improved user acceptance.","New York, NY, USA",15,,,Mar-25,"Transparency, HRI, Robotic trainer system, User acceptance, Older adults",,,19,
inproceedings,Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control,"An, Zhiyu and Ding, Xianzhong and Du, Wan",2024,,Proceedings of the 61st ACM/IEEE Design Automation Conference,,,,Association for Computing Machinery,10.1145/3649329.3656234,https://doi.org/10.1145/3649329.3656234,"Recent research has shown the potential of Model-based Reinforcement Learning (MBRL) to enhance energy efficiency of Heating, Ventilation, and Air Conditioning (HVAC) systems. However, existing methods rely on black-box thermal dynamics models and stochastic optimizers, lacking reliability guarantees and posing risks to occupant health. In this work, we overcome the reliability bottleneck by redesigning HVAC controllers using decision trees extracted from existing thermal dynamics models and historical data. Our decision tree-based policies are deterministic, verifiable, interpretable, and more energy-efficient than current MBRL methods. First, we introduce a novel verification criterion for RL agents in HVAC control based on domain knowledge. Second, we develop a policy extraction procedure that produces a verifiable decision tree policy. We found that the high dimensionality of the thermal dynamics model input hinders the efficiency of policy extraction. To tackle the dimensionality challenge, we leverage importance sampling conditioned on historical data distributions, significantly improving policy extraction efficiency. Lastly, we present an offline verification algorithm that guarantees the reliability of a control policy. Extensive experiments show that our method saves 68.4% more energy and increases human comfort gain by 14.8% compared to the state-of-the-art method, in addition to an 1127\texttimes{","New York, NY, USA",86,9.7984E+12,,,,"San Francisco, CA, USA",,6,DAC '24
inproceedings,Design Patterns for Explainable Agents (XAg),"Rodriguez, Sebastian and Thangarajah, John and Davey, Andrew",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,1621?€?1629,International Foundation for Autonomous Agents and Multiagent Systems,,,"The ability to explain the behaviour of the AI systems is a key aspect of building trust, especially for autonomous agent systems - how does one trust an agent whose behaviour can not be explained? In this work, we advocate the use of design patterns for developing explainable-by-design agents (XAg), to ensure explainability is an integral feature of agent systems rather than an","Richland, SC",,9.7984E+12,,,"aose, design patterns, emas, explainable agents","Auckland, New Zealand",,9,AAMAS '24
inproceedings,Are Robots' Gestures Understood? A Study on the Factors Influencing how Humans Perceive Information Present in Robot Gestures,"Bohnenkamp, Lisa Michelle and Abramov, Olga and Kopp, Stefan",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,263?€?267,Association for Computing Machinery,10.1145/3610978.3640572,https://doi.org/10.1145/3610978.3640572,"Social robots become increasingly important in various social domains like healthcare, education, and industry. In this paper, we explore whether humans understand the meaning conveyed by robot gestures when they occur alongside speech and other co-speech gestures with no meaning. We analyzed human comprehension of basic shapes presented through robot gestures varying gesture size and verbal context. Our findings show that humans notice robot gestures but struggle to understand the information provided by them. Explicitly directing attention to robot gestures improves understanding. Moreover, providing indirect information about robot's capabilities to gesture enhances the human ability to extract the correct information from gestures, with the effect linearly increasing with the number of observations of the robot.","New York, NY, USA",,9.7984E+12,,,"robot gestures, speech-gesture integration","Boulder, CO, USA",,5,HRI '24
inproceedings,Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning,"d'Eon, Greg and Newman, Neil and Leyton-Brown, Kevin",2024,,Proceedings of the 25th ACM Conference on Economics and Computation,,,1102?€?1130,Association for Computing Machinery,10.1145/3670865.3673644,https://doi.org/10.1145/3670865.3673644,"Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to analyze, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in verifying convergence, and how to generate and interpret multiple equilibria. We illustrate the promise of our resulting approach by using it to evaluate a specific rule change to a clock auction, finding substantially different auction outcomes due to complex changes in bidders' behavior.","New York, NY, USA",,9.7984E+12,,,"mechanism design, approximate equilibria, multi-agent reinforcement learning","New Haven, CT, USA",,29,EC '24
inproceedings,Explainability and Interpretability in Agent based Modelling to Approximate Market Indexes,"Neri, Filippo",2023,,Proceedings of the 2023 8th International Conference on Machine Learning Technologies,,,139?€?143,Association for Computing Machinery,10.1145/3589883.3589904,https://doi.org/10.1145/3589883.3589904,"We will discuss the notions of explainability and interpretability when using agent based modeling to approximate market indexes. As working context we will use the L-FABS system [22, 28] where agent based modeling, whose parameters are learned by simulated annealing, is used to explain and predict financial time series like: SP500, DJIA, GLD, SLV, etc. We will assume the following definitions for interpretability: being able to make sense of system output, and explainability: understanding how that output was generated as in [18]. Novelty of the paper: the discussion of explainability and interpretability in agent based modelling as implemented in L-FABS. An empirical case study will be discussed. Please note that the goal of this paper is not to describe how L-FABS works.","New York, NY, USA",,9.78145E+12,,,"Genetic algorithms, Software Agents modeling.","Stockholm, Sweden",,5,ICMLT '23
inproceedings,I Need to Pass Through! Understandable Robot Behavior for Passing Interaction in Narrow Environment,"Fujioka, Yusuke and Liu, Yuyi and Kanda, Takayuki",2024,,Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,213?€?221,Association for Computing Machinery,10.1145/3610977.3634951,https://doi.org/10.1145/3610977.3634951,"We developed a motion control algorithm for a social mobile robot to intuitively convey its intent via social cues to pass through aisles and avoid misunderstanding in passing interactions with people, which frequently occur when a robot navigates in narrow shared environments. Inspired by observations of human behavior, the proposed algorithm estimates the extent to which a person understands the robot's intent on the basis of the person's reactions to the oncoming robot and provides the robot with corresponding motion strategies for effective passing interactions. We implemented the proposed algorithm onto an omni-directional humanoid robot and conducted a field study over six days in a store with 75 cm wide narrow aisles. The resulting behaviors of 50 customers demonstrated that our proposed method provided people with a clearer understanding of the robot's intent in passing interactions, and thus the robot had more opportunity (73.1%) to pass through aisles compared to 16.7% if the robot moved and then waited for people to make space.","New York, NY, USA",,9.7984E+12,,,"motion control, narrow space, social navigation, social robot","Boulder, CO, USA",,9,HRI '24
inproceedings,Visuo-Textual Explanations of a Robot's Navigational Choices,"Halilovic, Amar and Lindner, Felix",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,531?€?535,Association for Computing Machinery,10.1145/3568294.3580141,https://doi.org/10.1145/3568294.3580141,"With the rise in the number of robots in our daily lives, human-robot encounters will become more frequent. To improve human-robot interaction (HRI), people will require explanations of robots' actions, especially if they do something unexpected. Our focus is on robot navigation, where we explain why robots make specific navigational choices. Building on methods from the area of Explainable Artificial Intelligence (XAI), we employ a semantic map and techniques from the area of Qualitative Spatial Reasoning (QSR) to enrich visual explanations with knowledge-level spatial information. We outline how a robot can generate visual and textual explanations simultaneously and test our approach in simulation.","New York, NY, USA",,9.78145E+12,,,"explainable artificial intelligence, human-robot interaction, obstacle avoidance, robot navigation","Stockholm, Sweden",,5,HRI '23
inproceedings,The Role of Lexical Alignment in Human Understanding of Explanations by Conversational Agents,"Srivastava, Sumit and Theune, Mari\",2023,,Proceedings of the 28th International Conference on Intelligent User Interfaces,,,423?€?435,Association for Computing Machinery,10.1145/3581641.3584086,https://doi.org/10.1145/3581641.3584086,"Explainable Artificial Intelligence (XAI) focuses on research and technology that can explain an AI system?€?s functioning and its underlying methods, and also on making these explanations better through personalization. Our research study investigates a natural language personalization method called lexical alignment in understanding an explanation provided by a conversational agent. The study setup was online and navigated the participants through an interaction with a conversational agent. Participants faced either an agent designed to align its responses to those of the participants, a misaligned agent, or a control condition that did not involve any dialogue. The dialogue delivered an explanation based on a pre-defined set of causes and effects. The recall and understanding of the explanations was evaluated using a combination of Yes-No questions, a Cloze test (fill-in-the-blanks), and What-style questions. The analysis of the test scores revealed a significant advantage in information recall for those who interacted with an aligning agent against the participants who either interacted with a non-aligning agent or did not go through any dialogue. The Yes-No type questions that included probes on higher-order inferences (understanding) also reflected an advantage for the participants who had an aligned dialogue against both non-aligned and no dialogue conditions. The results overall suggest a positive effect of lexical alignment on understanding of explanations.","New York, NY, USA",,9.7984E+12,,,"explainable artificial intelligence, human-machine interaction, lexical alignment, lexical entrainment","Sydney, NSW, Australia",,13,IUI '23
inproceedings,Effects of Transparency in Humanoid Robots - A Pilot Study,"Mellmann, Heinrich and Arbuzova, Polina and Kontogiorgos, Dimosthenis and Yordanova, Magdalena and Haensel, Jennifer X. and Hafner, Verena V. and Bryson, Joanna J.",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,750?€?754,Association for Computing Machinery,10.1145/3610978.3640613,https://doi.org/10.1145/3610978.3640613,"Transparency is recognized as a vital feature for understanding and predicting robot behavior. Another feature that affects interaction with robots is their anthropomorphism. The relationship between these remains under-explored but is postulated to be negative. We present a pilot study investigating the effects of robot transparency in human-robot interactions, where the robot has an anthropomorphic appearance. We asked participants to evaluate and interact with the humanoid robot Pepper to examine whether visualizing the robot's goals and behavior affects perceived intelligence, anthropomorphism, and robot agency. Our preliminary findings suggest that users may attribute higher ratings of agency when interacting with a robot visualizing its goals. In this late-breaking report, we propose our experiment on the interplay between transparency and anthropomorphism in human-robot interaction and summarize insights from our preliminary pilot study.","New York, NY, USA",,9.7984E+12,,,"HRI, anthropomorphism, legibility, transparency","Boulder, CO, USA",,5,HRI '24
article,Delivering the Future: Understanding User Perceptions of Delivery Robots,"Shin, Hyorim and Choi, Junho and Oh, Changhoon",2024,Proc. ACM Hum.-Comput. Interact.,,8,CSCW1,,Association for Computing Machinery,10.1145/3653687,https://doi.org/10.1145/3653687,"Delivery robots are increasingly becoming part of our urban landscape. However, the general public is divided about their presence in public spaces; some welcome their usefulness, while others see them as intrusive or even threatening. This study aims to understand users' perceptions of these robots to provide concrete insights into their further development. First, we used text mining to analyze people's reactions to popular YouTube videos featuring delivery robots. Based on these findings, we applied the scenario-based design method to develop scenarios illustrating user interactions with delivery robots. We then conducted in-depth interviews with 30 participants to explore their views on these scenarios. Our analysis highlighted several design issues, including robots' aesthetics, interactions with pedestrians, and the broader physical and regulatory framework. We also identified common concerns and positive expectations for these robots. From these findings, we propose design implications for the future of delivery robots.","New York, NY, USA",196,,,Apr-24,"delivery robot, human-robot interaction, in-depth interview, scenario-based design, text mining, user experience",,,24,
inproceedings,Eliciting Understandable Architectonic Gestures for Robotic Furniture through Co-Design Improvisation,"Nguyen, Alex Binh Vinh Duc and Leusmann, Jan and Mayer, Sven and Vande Moere, Andrew",2025,,Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction,,,569?€?579,IEEE Press,,,"The vision of adaptive architecture proposes that robotic technologies could enable interior spaces to physically transform in a bidirectional interaction with occupants. Yet, it is still unknown how this interaction could unfold in an understandable way. Inspired by HRI studies where robotic furniture gestured intents to occupants by deliberately positioning or moving in space, we hypothesise that adaptive architecture could also convey intents through gestures performed by a mobile robotic partition. To explore this design space, we invited 15 multidisciplinary experts to join co-design improvisation sessions, where they manually manoeuvred a deactivated robotic partition to design gestures conveying six architectural intents that varied in purpose and urgency. Using a gesture elicitation method alongside motion-tracking data, a Laban-based questionnaire, and thematic analysis, we identified 20 unique gestural strategies. Through categorisation, we introduced architectonic gestures as a novel strategy for robotic furniture to convey intent by indexically leveraging its spatial impact, complementing the established deictic and emblematic gestures. Our study thus represents an exploratory step toward making the autonomous gestures of adaptive architecture more legible. By understanding how robotic gestures are interpreted based not only on their motion but also on their spatial impact, we contribute to bridging HRI with Human-Building Interaction research.",,,,,,"adaptive architecture, gesture elicitation, human-building interaction, human-robot interaction, robotic furniture, robotic intents, robotic interpretability, robotic understandability","Melbourne, Australia",,11,HRI '25
inproceedings,GANterfactual-RL: Understanding Reinforcement Learning Agents' Strategies through Visual Counterfactual Explanations,"Huber, Tobias and Demmler, Maximilian and Mertes, Silvan and Olson, Matthew L. and Andr\'{e",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,1097?€?1106,International Foundation for Autonomous Agents and Multiagent Systems,,,"Counterfactual explanations are a common tool to explain artificial intelligence models. For Reinforcement Learning (RL) agents, they answer","Richland, SC",,9.78145E+12,,,"explainable artificial intelligence, explainable deep reinforcement learning, interpretable machine learning","London, United Kingdom",,10,AAMAS '23
inproceedings,Understanding Differences in Human-Robot Teaming Dynamics between Deaf/Hard of Hearing and Hearing Individuals,"Dust, A'di and Gonzalez-Lebron, Carola and Connell, Shannon and Singh, Saurav and Bailey, Reynold and Alm, Cecilia Ovesdotter and Heard, Jamison",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,552?€?556,Association for Computing Machinery,10.1145/3568294.3580146,https://doi.org/10.1145/3568294.3580146,"With the development of industry 4.0, more collaborative robots are being implemented in manufacturing environments. Hence, research in human-robot interaction (HRI) and human-cobot interaction (HCI) is gaining traction. However, the design of how cobots interact with humans has typically focused on the general able-bodied population, and these interactions are sometimes ineffective for specific groups of users. This study's goal is to identify interactive differences between hearing and deaf and hard of hearing individuals when interacting with cobots. Understanding these differences may promote inclusiveness by detecting ineffective interactions, reasoning why an interaction failed, and adapting the framework's interaction strategy appropriately.","New York, NY, USA",,9.78145E+12,,,"cobot, deaf, hard of hearing, human-robot interaction","Stockholm, Sweden",,5,HRI '23
inproceedings,Transparency Classification for HRI with Humanoid Service Robots,"Nukovic, Lejla and Kirchhoff, J\'{e",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,798?€?802,Association for Computing Machinery,10.1145/3610978.3640739,https://doi.org/10.1145/3610978.3640739,"As humanoid service robots are increasingly becoming part of our everyday life, their acceptance in society requires a general understanding of those robots. Transparency can be a means to establish this. Complementary to the general IEEE Standard 7001-2021, a new transparency classification for humanoid service robots is proposed in this work as a baseline for the goal-oriented definition of concrete transparency mechanisms. Here, Content Transparency and Interaction Transparency are introduced, separating transparency of service content information from effective communication and robot usage. Interaction Transparency is further subdivided to include interaction aspects unique to humanoid robots. Further, it integrates Traceability and Privacy. Overall, it covers a larger range of interaction aspects than common transparency models. The proposed transparency classification also supports the systematic investigation of individual transparency mechanism's influence on the interaction. For this, novel classification-specific scales should be developed to assess the perception of, e.g., Interaction Transparency.","New York, NY, USA",,9.7984E+12,,,"content transparency, human-robot interaction, humanoid service robots, interaction transparency, transparency","Boulder, CO, USA",,5,HRI '24
article,The Author?€?s Journey?€?Understanding and Improving the Authoring Process of Theory-Driven Socially Intelligent Agents,Guimar\~{a,2025,ACM Trans. Interact. Intell. Syst.,,15,2,,Association for Computing Machinery,10.1145/3711672,https://doi.org/10.1145/3711672,"State-of-the-art agent-modelling tools support the creation of powerful Socially Intelligent Agents (SIAs) capable of engaging in social interactions with participants in various roles and environments. However, their deployment demands a labourious authoring task as it is necessary to manually define behaviour rules and create content for different interaction scenarios.While Socially Intelligent Agents (SIAs) research has centred on the user experience, we shift focus to the authors. To understand the challenges faced by authors who create these agents, we performed an innovative analysis of the authoring experience in modern agent modelling tools. One key finding is that, while SIA concepts are generally understandable, emotional-based concepts are not as easily comprehended or used by authors. We propose a hybrid solution approach that culminated in the development of Authoring-Assisted FAtiMA-Toolkit. The augmented agent modelling tool incorporates a data-driven Authoring Assistant to boost author productivity while promoting transparency and authorial control. To evaluate the impact of this framework on the authoring experience, we conducted a user study. Results showed that authors using the Authoring-Assisted FAtiMA-Toolkit were on average able to create more SIA-related content in less time.Our findings suggest that data-augmented, theory-grounded agent modelling tools can support the development of affective social agents by reducing the authoring burden without sacrificing the framework?€?s clarity or the authors?€? control over the content.","New York, NY, USA",8,,2160-6455,Jun-25,"intelligent agents, affective computing, cognitive architecture, emotions, social robots",,,40,
inproceedings,Clarifying Social Robot Expectation Discrepancy: Developing a Framework for Understanding How Users Form Expectations of Social Robots,"Berzuk, James M. and Young, James E.",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,231?€?233,Association for Computing Machinery,10.1145/3568294.3580078,https://doi.org/10.1145/3568294.3580078,"When engaging with a social robot, people form expectations about the robot that may not align with its real behaviour and abilities. This gap is known as expectation discrepancy, and can confuse and disappoint users. We are developing a framework that can be used to understand and compare instances of expectation discrepancy between robots by considering the sources of those expectations. In doing so, we aim to provide a structure and unified vocabulary that can be used to support description and comparison of robot designs and the expectations users form of them. We have begun by examining theoretical work on expectations in interactions between people, and are working to synthesize this into an initial foundation. We will then refine this into a final social robot expectation framework by conducting a survey of expectation formation and discrepancy in existing social robots and projects.","New York, NY, USA",,9.78145E+12,,,"expectation discrepancy, framework, human-robot interaction, social robots, survey","Stockholm, Sweden",,3,HRI '23
inproceedings,Reactive or Proactive? How Robots Should Explain Failures,"LeMasurier, Gregory and Gautam, Alvika and Han, Zhao and Crandall, Jacob W. and Yanco, Holly A.",2024,,Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,413?€?422,Association for Computing Machinery,10.1145/3610977.3634963,https://doi.org/10.1145/3610977.3634963,"As robots tackle increasingly complex tasks, the need for explanations becomes essential for gaining trust and acceptance. Explainable robotic systems should not only elucidate failures when they occur but also predict and preemptively explain potential issues. This paper compares explanations from Reactive Systems, which detect and explain failures after they occur, to Proactive Systems, which predict and explain issues in advance. Our study reveals that the Proactive System fosters higher perceived intelligence and trust and its explanations were rated more understandable and timely. Our findings aim to advance the design of effective robot explanation systems, allowing people to diagnose and provide assistance for problems that may prevent a robot from finishing its task.","New York, NY, USA",,9.7984E+12,,,"assumption checkers, behavior explanation, behavior trees, robot explanation generation, robot transparency","Boulder, CO, USA",,10,HRI '24
article,The Need for Verbal Robot Explanations and How People Would Like a Robot to Explain Itself,"Han, Zhao and Phillips, Elizabeth and Yanco, Holly A.",2021,J. Hum.-Robot Interact.,,10,4,,Association for Computing Machinery,10.1145/3469652,https://doi.org/10.1145/3469652,"Although non-verbal cues such as arm movement and eye gaze can convey robot intention, they alone may not provide enough information for a human to fully understand a robot?€?s behavior. To better understand how to convey robot intention, we conducted an experiment (N = 366) investigating the need for robots to explain, and the content and properties of a desired explanation such as timing, engagement importance, similarity to human explanations, and summarization. Participants watched a video where the robot was commanded to hand an almost-reachable cup and one of six reactions intended to show the unreachability : doing nothing (No Cue), turning its head to the cup (Look), or turning its head to the cup with the addition of repeated arm movement pointed towards the cup (Look &amp; Point), and each of these with or without a Headshake. The results indicated that participants agreed robot behavior should be explained across all conditions, in situ, in a similar manner as what human explain, and provide concise summaries and respond to only a few follow-up questions by participants. Additionally, we replicated the study again with N = 366 participants after a 15-month span and all major conclusions still held.","New York, NY, USA",36,,,Dec-21,"Robot explanation, behavior explanation, system transparency",,,42,
article,Communicating Missing Causal Information to Explain a Robot?€?s Past Behavior,"Han, Zhao and Yanco, Holly",2023,J. Hum.-Robot Interact.,,12,1,,Association for Computing Machinery,10.1145/3568024,https://doi.org/10.1145/3568024,"Robots need to explain their behavior to gain trust. Existing research has focused on explaining a robot?€?s current behavior, yet it remains unknown yet challenging how to provide explanations of past actions in an environment that might change after a robot?€?s actions, leading to critical missing causal information due to moved objects.We conducted an experiment (N = 665) investigating how a robot could help participants infer the missing causal information by replaying the past behavior physically, using verbal explanations, and projecting visual information onto the environment. Participants watched videos of the robot replaying its completion of an integrated mobile kitting task. During the replay, the objects are already gone, so participants needed to infer where an object was picked, where a ground obstacle had been, and where the object was placed.Based on the results, we recommend combining physical replay with speech and projection indicators (Replay-Project-Say) to help infer all the missing causal information (picking, navigation, and placement) from the robot?€?s past actions. This condition had the best outcome in both task-based?€?effectiveness, efficiency, and confidence?€?and team-based metrics?€?workload and trust. If one?€?s focus is efficiency, then we recommend projection markers for navigation inferences and verbal markers for placing inferences.","New York, NY, USA",10,,,Mar-23,"Robot explanation, behavior explanation, system transparency",,,45,
inproceedings,Resolving References in Natural Language Explanation Requests about Robot Behavior in HRI,Schr\,2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,772?€?774,Association for Computing Machinery,10.1145/3568294.3579981,https://doi.org/10.1145/3568294.3579981,"In HRI, users have been shown to request explanations when their interpretation of autonomous robot behavior fails. These requests can refer to the behavior either by open questions or with attributes of the behavior. The presented work aims to resolve these references in explanation requests by developing an episodic memory with a graph database that stores and queries representations of the internal execution. The reference resolution is done by the detection of temporal adverb and verb constraints in the syntactical dependency tree of utterances, the execution of a query in the episodic memory, and the scoring of the resulting entries to find the referred behavior. The explanation generation process of the original model is adapted to the new approach and can contain additional information such as detected constraints, a failed execution state, and the distinction between running and completed executions.","New York, NY, USA",,9.78145E+12,,,"episodic memory, explanation generation, nlu, social robots","Stockholm, Sweden",,3,HRI '23
inproceedings,Speaking Transparently: Social Robots in Educational Settings,"Cumbal, Ronald and Engwall, Olov",2024,,Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction,,,356?€?358,Association for Computing Machinery,10.1145/3610978.3640717,https://doi.org/10.1145/3610978.3640717,"The recent surge in popularity of Large Language Models, known for their inherent opacity, has increased the interest in fostering transparency in technology designed for human interaction. This concern is equally prevalent in the development of Social Robots, particularly when these are designed to engage in critical areas of our society, such as education or healthcare. In this paper we propose an experiment to investigate how users can be made aware of the automated decision processes when interacting in a discussion with a social robot. Our main objective is to assess the effectiveness of verbal expressions in fostering transparency within groups of individuals as they engage with a robot. We describe the proposed interactive settings, system design, and our approach to enhance the transparency in a robot's decision-making process for multi-party interactions.","New York, NY, USA",,9.7984E+12,,,"conversation, dialogue system, multi-party, transparency","Boulder, CO, USA",,3,HRI '24
inproceedings,Advancing Sample Efficiency and Explainability in Multi-Agent Reinforcement Learning,"Zhang, Zhicheng",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2791?€?2793,International Foundation for Autonomous Agents and Multiagent Systems,,,"Multi-Agent Reinforcement Learning (MARL) holds promise for complex real-world applications but faces challenges in sample efficiency and policy explainability. My dissertation aims to address these critical barriers, advancing MARL towards more practical and interpretable systems. To boost sample efficiency, it is crucial for agents to effectively learn from and generalize past experiences. We propose a meta-exploration technique to train meta-exploration policies that exploit the joint state-action space structure from meta-training tasks. This approach can be integrated with any off-policy MARL algorithm to improve learning efficiency. Complementing the efficiency gain, my research also focuses on augmenting the explainability of neural network policies' decision-making processes using techniques such as decision-tree extraction from MARL networks. In this extended abstract, I will summarize my research so far and outline promising future directions to further the deployability of MARL in complex real-world environments.","Richland, SC",,9.7984E+12,,,"explainability, multi-agent reinforcement learning, sample efficiency","Auckland, New Zealand",,3,AAMAS '24
article,Understanding is a Two-Way Street: User-Initiated Repair on Agent Responses and Hearing in Conversational Interfaces,"Moore, Robert J. and An, Sungeun and Marrese, Olivia H.",2024,Proc. ACM Hum.-Comput. Interact.,,8,CSCW1,,Association for Computing Machinery,10.1145/3641026,https://doi.org/10.1145/3641026,"Although methods for repairing prior turns in natural conversation are critical for enabling mutual understanding, or successful communication, these methods are seldom built into conversational user interfaces systematically. Chatbots and voice assistants tend to ask users to paraphrase what they said if it was not understood, but users cannot do the same if they encounter trouble in understanding what the agent said. Understanding is a one-way street in most (intent-based) conversation-like interfaces. An exception to this is Moore and Arar (2019), who demonstrate nine types of user-initiated repair on agent responses that are common in natural conversation and who have shown that users will employ these repair features correctly in text-based interfaces if taught. In this small-scale study, we test these user-initiated repairs (in second position) in a voice-based interface. With understanding-oriented repairs, we found that participants employed them much the same way in text and voice. In addition, we examine some hearing- and speaking-oriented repairs that emerged from the use of our novel multi-modal interface. We found that participants used them to manage troubles specific to the voice modality. Analysis of user logs and transcripts suggests that user-initiated repair features are valuable components of conversational interfaces.","New York, NY, USA",187,,,Apr-24,"chatbots, conversational agents, conversational ai, conversational user interfaces, conversational ux, user-initiated repair",,,26,
article,Mitigating Algorithm Aversion in Recruiting: A Study on Explainable AI for Conversational Agents,Flei\ss{,2024,SIGMIS Database,,55,1,56?€?87,Association for Computing Machinery,10.1145/3645057.3645062,https://doi.org/10.1145/3645057.3645062,"The use of conversational agents (CAs) based on artificial intelligence (AI) is becoming more common in the field of recruiting. Organizations are now adopting AI-based CAs for applicant (pre-)selection, but negative news coverage, especially the black-box character of AI, has hindered adoption. So far, little is known about the contextual factors influencing users' perception of AI-based CAs in general and the effect of provided explanations by explainable AI (XAI) in particular. While research on algorithm aversion provides some initial explanations, information regarding the effects of different XAI approaches on different types of decisions on the attitudes of (potential) applicants is scarce. Therefore, in this study, we use a quantitative, quota-representative study (n = 490) to assess the acceptance of CAs in recruiting. By applying an experimental within-subject design, we provide a more nuanced perspective on why and when providing explanations increases user acceptance. We also show that contextual factors such as the type of assessed skills are major determinants of this effect, and we conclude that XAI is not a","New York, NY, USA",,,0095-0033,Feb-24,"algorithm aversion, conversational agent., experiment, explainable artificial intelligence, recruiting",,,32,
inproceedings,A Social Robot for Explaining Medical Tests and Procedures: An Exploratory Study in the Wild,"Boumans, Roel and Melis, Ren\'{e",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,263?€?267,Association for Computing Machinery,10.1145/3568294.3580085,https://doi.org/10.1145/3568294.3580085,"Healthcare professionals often have little time to explain medical tests and procedures. Social robots capable of verbal dialogues may contribute to informing patients and the public in general about such tests and procedures, for example in general practitioner or hospital waiting rooms, nursery homes, as well as in public spaces. As an example of latter, an exploratory study was conducted at the Lowlands music festival in August 2022. A social robot explained a blood pressure measurement and a grip strength measurement to participants. Participants were asked to value the expected clarity of the explanation before the explanation, the experienced clarity after the robot explanation but before the actual physical measurements, and after the physical measurements. 172 participants completed the interaction (99 female, 57 male, 8 non-binary, 8 undisclosed). The mean interaction duration was 2.02 minutes (SD=0.40 minutes). Participants found the explanation after the interaction with Pepper clearer than they expected beforehand. Participants found the clarity of the explanation, after they had actually undergone the physical examination, even higher than before the physical examination. This study indicates that social robots are potentially useful to explain medical tests and procedures.","New York, NY, USA",,9.78145E+12,,,"explanation medical procedure, field study in the wild., health care, social robot","Stockholm, Sweden",,5,HRI '23
inproceedings,Increasing user trust in a fetching robot using explainable AI in a traded control paradigm,"Cassady, Jacob T. and Robinson, Chris and Popa, Dan O.",2020,,Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments,,,,Association for Computing Machinery,10.1145/3389189.3393740,https://doi.org/10.1145/3389189.3393740,"Recently, there has been an increase use of collaborative robots in manufacturing, healthcare, military, and personal use scenarios. Such robots operate under shared or traded control paradigms with their human operators or users. Therefore, it is important to understand how to address and improve issues of trust between the humans and collaborative robots. In this paper, we investigate the impact of robotic agent transparency to their subjective trust level by a human operator. Several experiments were conceived with the help of a fetching mobile robot under traded control, and data such as subjective trust level was collected during experimentation. Results indicate that trust is easier to lose than it is to gain. Furthermore, results also indicate that agent transparency's effect on operator trust is more significant in tasks of increasing complexity.","New York, NY, USA",9,9.78145E+12,,,"explainable autonomy, mobile manipulation","Corfu, Greece",,8,PETRA '20
inproceedings,Empirical Estimates on Hand Manipulation are Recoverable: A Step Towards Individualized and Explainable Robotic Support in Everyday Activities,"Wich, Alexander and Schultheis, Holger and Beetz, Michael",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,1382?€?1390,International Foundation for Autonomous Agents and Multiagent Systems,,,"A key challenge for robotic systems is to figure out the behavior of another agent. The capability to draw correct inferences is crucial to derive human behavior from examples.Processing correct inferences is especially challenging when (confounding) factors are not controlled experimentally (observational evidence). For this reason, robots that rely on inferences that are correlational risk a biased interpretation of the evidence.We propose equipping robots with the necessary tools to conduct observational studies on people. Specifically, we propose and explore the feasibility of structural causal models with non-parametric estimators to derive empirical estimates on hand behavior in the context of object manipulation in a virtual kitchen scenario. In particular, we focus on inferences under (the weaker) conditions of partial confounding (the model covering only some factors) and confront estimators with hundreds of samples instead of the typical order of thousands. Studying these conditions explores the boundaries of the approach and its viability.Despite the challenging conditions, the estimates inferred from the validation data are correct. Moreover, these estimates are stable against three refutation strategies where four estimators are in agreement. Furthermore, the causal quantity for two individuals reveals the sensibility of the approach to detect positive and negative effects.The validity, stability, and explainability of the approach are encouraging and serve as the foundation for further research.","Richland, SC",,9.78145E+12,,,"causal inference, human behavior, robotics, treatment effect estimation","Virtual Event, New Zealand",,9,AAMAS '22
inproceedings,Establishing Shared Query Understanding in an Open Multi-Agent System,"Kondylidis, Nikolaos and Tiddi, Ilaria and ten Teije, Annette",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,281?€?289,International Foundation for Autonomous Agents and Multiagent Systems,,,"We propose a method that allows to develop shared understanding between two agents for the purpose of performing a task that requires cooperation. Our method focuses on efficiently establishing successful task-oriented communication in an open multi-agent system, where the agents do not know anything about each other and can only communicate via grounded interaction. The method aims to assist researchers that work on human-machine interaction or scenarios that require a human-in-the-loop, by defining interaction restrictions and efficiency metrics. To that end, we point out the challenges and limitations of such a (diverse) setup, while also restrictions and requirements which aim to ensure that high task performance truthfully reflects the extent to which the agents correctly understand each other. Furthermore, we demonstrate a use-case where our method can be applied for the task of cooperative query answering. We design the experiments by modifying an established ontology alignment benchmark. In this example, the agents want to query each other, while representing different databases, defined in their own ontologies that contain different and incomplete knowledge. Grounded interaction here has the form of examples that consists of common instances, for which the agents are expected to have similar knowledge. Our experiments demonstrate successful communication establishment under the required restrictions, and compare different agent policies that aim to solve the task in an efficient manner.","Richland, SC",,9.78145E+12,,,"collaborative query answering, open multi-agent systems, task-oriented communication establishment","London, United Kingdom",,9,AAMAS '23
inproceedings,?€?You've Got a Friend in Me?€?: A Formal Understanding of the Critical Friend Agent,"Wester, Joel and Br\",2023,,Proceedings of the 11th International Conference on Human-Agent Interaction,,,443?€?445,Association for Computing Machinery,10.1145/3623809.3623957,https://doi.org/10.1145/3623809.3623957,"State-of-the-art intelligent and interactive agents, such as Alexa or Siri, often present overly conforming behaviour during interactions with humans. This can result in a misalignment between end-user expectations and agent behaviour. To overcome this barrier in human-AI interactions, we introduce the Critical Friend (CF), a conceptual idea that guides critical behaviour in human-human interactions. We present our results as a formal understanding that can be described through description logic and utilised for reasoning capabilities, enabling implementations of the CF as an intelligent interactive agent.","New York, NY, USA",,9.7984E+12,,,"Critical Friend, agent, characterisation, formal understanding","Gothenburg, Sweden",,3,HAI '23
inproceedings,Self-Explainable Robots in Remote Environments,"Chiyah Garcia, Francisco J. and Smith, Sim\'{o",2021,,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,662?€?664,Association for Computing Machinery,10.1145/3434074.3447275,https://doi.org/10.1145/3434074.3447275,"As robots and autonomous systems become more adept at handling complex scenarios, their underlying mechanisms also become increasingly complex and opaque. This lack of transparency can give rise to unverifiable behaviours, limiting the use of robots in a number of applications including high-stakes scenarios, e.g. self-driving cars or first responders. In this paper and accompanying video, we present a system that learns from demonstrations to inspect areas in a remote environment and to explain robot behaviour. Using semi-supervised learning, the robot is able to inspect an offshore platform autonomously, whilst explaining its decision process both through both image-based and natural language-based interfaces.","New York, NY, USA",,9.78145E+12,,,"autonomous control, explainable robot, nlg, remote location, semi-supervised learning, transparent interfaces","Boulder, CO, USA",,3,HRI '21 Companion
inproceedings,Causal Explanations for Sequential Decision-Making in Multi-Agent Systems,"Gyevnar, Balint and Wang, Cheng and Lucas, Christopher G. and Cohen, Shay B. and Albrecht, Stefano V.",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,771?€?779,International Foundation for Autonomous Agents and Multiagent Systems,,,"We present CEMA: Causal Explanations in Multi-A gent systems; a framework for creating causal natural language explanations of an agent's decisions in dynamic sequential multi-agent systems to build more trustworthy autonomous agents. Unlike prior work that assumes a fixed causal structure, CEMA only requires a probabilistic model for forward-simulating the state of the system. Using such a model, CEMA simulates counterfactual worlds that identify the salient causes behind the agent's decisions. We evaluate CEMA on the task of motion planning for autonomous driving and test it in diverse simulated scenarios. We show that CEMA correctly and robustly identifies the causes behind the agent's decisions, even when a large number of other agents is present, and show via a user study that CEMA's explanations have a positive effect on participants' trust in autonomous vehicles and are rated as high as high-quality baseline explanations elicited from other participants. We release the collected explanations with annotations as the HEADD dataset.","Richland, SC",,9.7984E+12,,,"autonomous vehicles, causal explanations, dataset, explainable ai, human-centric xai, multi-agent systems","Auckland, New Zealand",,9,AAMAS '24
inproceedings,Fairness and Transparency in Human-Robot Interaction,"Claure, Houston and Chang, Mai Lee and Kim, Seyun and Omeiza, Daniel and Brandundefinedo, Martim and Lee, Min Kyung and Jung, Malte",2022,,Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction,,,1244?€?1246,IEEE Press,,,"As robots become more ubiquitous across human spaces, it is becoming increasingly relevant for researchers to ask the question, ''how can we ensure that we are designing robots to be sufficiently equipped to treat people fairly?''. This workshop brings together researchers across the fields of Human-Robot Interaction (HRI), fairness in machine learning, design, and transparency in AI to shed light on the relevant methodological challenges surrounding issues of fairness and transparency in HRI. In our workshop, we will attempt to identify synergies between these various fields. In particular, we will focus on how HRI can leverage these existing rich body of work to guide the formalization of fairness metrics and methodologies. Another goal of the workshop is to foster a community of interdisciplinary researchers to encourage collaboration. The complexity in defining fairness lies in its context sensitive nature, as such we look to the influx of definitions from the field of fairness in artificial intelligence, design, and organizational psychology to derive a set of definitions that could serve as guidelines for researchers in HRI.",,,,,,"ethics in hri, fairness in hri, transparency in ai","Sapporo, Hokkaido, Japan",,3,HRI '22
inproceedings,InnoGuideGPT: Integrating conversational interface and command interpretation for navigation robots,"Sundar, Rahul and Gadgil, Shreyash and Satya Sai, Tankala and Reddy, Sathi Sai Krishna and B, Gautam and Mittal, Ishita and Guduguntla, Jyotsna Sree and Pujala, Shanmukesh",2024,,Proceedings of the Third International Conference on AI-ML Systems,,,,Association for Computing Machinery,10.1145/3639856.3639915,https://doi.org/10.1145/3639856.3639915,"Integrating natural language understanding, voice command interpretation and natural language generation for realtime inference is a challenging problem. However, developing a proof of concept is now possible in just a few lines of code which was otherwise unimaginable a few years ago. Thanks to the democratization of LLMs and the availability of high-quality pre-trained models through APIs. It is now possible to quickly build effective use cases by just integrating multiple models without even having to pre-train/fine-tune the models on custom data. This is due to their zero-shot learning ability. Although, there are many recent works in this regard oriented towards software&nbsp;[1, 5], edge implementations and their applications to Robotics are yet to be explored in their entirety&nbsp;[6]. Recently, Koubaa&nbsp;[2] proposed RoboGPT, where a ChatGPT was prompt-tuned for command interpretation and subsequently determining the robot?€?s actions. In this work, we explore the possibility of integrating a real-time conversational voice interface within a navigation robot. This is achieved using an underlying LLM interface to respond to user queries based on a prior context and additionally interpret voice commands for the robot to navigate. As a proof of concept, we evaluate the effectiveness of the planned workflow using a robot simulation. For simulating a Robot?€?s environment, there exist various tools but in this work, Gazebo&nbsp;[3] has been adopted to simulate a differential drive robot moving in an indoor environment and Rviz is used for visualization. We utilized the TurtleBot3&nbsp;[4] software packages to implement motion planning and navigation algorithms (see figure 2). For the speech recognition, we use OpenAI?€?s Whisper API, while for text to speech we use the freely available google text-to-speech python package. We then use OpenAI?€?s GPT-3.5-turbo API within the Langchain framework for building a context aware conversational interface and an additional command interpretation module under the hood to guide the navigation bot. It is remarkable how there is a significant reduction in development time for proof of concepts through the availability of high-quality models in addition to high-quality outcomes. Once the workflow is validated on Gazebo, the workflow is then implemented onto a NVIDIA Jetson Nano which will be used to send and process the cloud based LLM/speech API requests and responses (see figure 3.","New York, NY, USA",57,9.7984E+12,,,"Command interpretation, Conversational AI, Large language models, Robot navigation, Speech recognition, Speech synthesis","Bangalore, India",,3,AIMLSystems '23
inproceedings,"What Do Children and Parents Want and Perceive in Conversational Agents? Towards Transparent, Trustworthy, Democratized Agents","Van Brummelen, Jessica and Kelleher, Maura and Tian, Mingyan Claire and Nguyen, Nghi",2023,,Proceedings of the 22nd Annual ACM Interaction Design and Children Conference,,,187?€?197,Association for Computing Machinery,10.1145/3585088.3589353,https://doi.org/10.1145/3585088.3589353,"Historically, researchers have focused on analyzing Western, Educated, Industrialized Rich and Democratic (WEIRD), adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries?€? perspectives on an emerging technology: conversational agents. We aim to better understand participants?€? trust of agents, partner models, and their ideas of ?€?ideal future agents?€? such that researchers can better design for these users?€?for instance, by ensuring children do not overtrust agents. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how overall participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents?€? competence and predictability indicators, as well as increasing transparency in terms of agents?€? information sources.","New York, NY, USA",,9.7984E+12,,,"agent personification, chatbots, computational action, conversational AI, conversational agents, non-WEIRD and WEIRD, parents, partner models, technology democratization, trust, virtual assistants","Chicago, IL, USA",,11,IDC '23
inproceedings,Explainable Hybrid CNN and FNN Approach Applied on Robotic Wall-Following Behaviour Learning,"Kwiatkowski, Jakub and Ou, Liang and Chang, Yu-Cheng and Lin, Chin-Teng",2022,,Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition,,,623?€?628,Association for Computing Machinery,10.1145/3488933.3489026,https://doi.org/10.1145/3488933.3489026,"Fuzzy Neural Network (FNN) applied to robotic control tasks has proved to be effective by previous researchers. However, FNN has an inherent deficiency in dealing with inputs of large dimensions, such as images. Therefore, this research utilizes a Convolutional Neural Network (CNN) model to convert image into distance values and delivers these values to FNN based robot controller as inputs. The proposed hybrid CNN+FNN are tested with both a regression model and a multi-task model. Results show that the multi-task method performs better with less information loss from input images. This paper also proved that the proposed hybrid approach can be generalized into an unknown robotic simulation environment and performs better than its FNN counterpart. By utilizing state of the art explainable analysis method, both the CNN part and the FNN part of the hybrid approach can be explained in a human-understandable way.","New York, NY, USA",,9.78145E+12,,,"Additional Key Words and Phrases: Explainable AI, Fuzzy System, Robotic Navigation","Xiamen, China",,6,AIPR '21
inproceedings,Transparency as Delayed Observability in Multi-Agent Systems,"Dwarakanath, Kshama and Vyetrenko, Svitlana and Balch, Tucker and Oyebode, Toks",2024,,Proceedings of the Winter Simulation Conference,,,279?€?290,IEEE Press,,,"Is transparency always beneficial in complex systems such as traffic networks and stock markets? How is transparency defined in multi-agent systems, and what is its optimal degree at which social welfare is highest? We take an agent-based view to define transparency (or its lacking) as delay in agent observability of environment states, and utilize simulations to analyze the impact of delay on social welfare. To model the adaptation of agent strategies with varying delays, we model agents as learners maximizing the same objectives under different delays in a simulated environment. Focusing on two agent types - constrained and unconstrained, we use multi-agent reinforcement learning to evaluate the impact of delay on agent outcomes and social welfare. Empirical demonstration of our framework in simulated financial markets shows opposing trends in outcomes of the constrained and unconstrained agents with delay, with an optimal partial transparency regime at which social welfare is maximal.",,,9.79835E+12,,,,"San Antonio, Texas, USA",,12,WSC '23
inproceedings,"Personalized Agent Explanations for Human-Agent Teamwork: Adapting Explanations to User Trust, Workload, and Performance","Verhagen, Ruben S. and Neerincx, Mark A. and Parlar, Can and Vogel, Marin and Tielman, Myrthe L.",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2316?€?2318,International Foundation for Autonomous Agents and Multiagent Systems,,,"For human-agent teams to be successful, agent explanations are crucial. These explanations should ideally be personalized by adapting them to intended human users. So far, little work has been conducted on personalized agent explanations during human-agent teamwork. Therefore, an online experiment (n = 60) was conducted to compare personalized agent explanations against a baseline of non-personalized explanations. We implemented four agents who adapted their explanations during a search and rescue task randomly, or based on human workload, performance, or trust. Results show that personalized explanations can increase explanation satisfaction and trust in the agent, but also decrease performance. Therefore, we conclude that personalized agent explanations can be beneficial to human-agent teamwork, but that user modelling and personalization techniques should be carefully considered.","Richland, SC",,9.78145E+12,,,"explainable ai, human-agent teamwork, personalization","London, United Kingdom",,3,AAMAS '23
article,GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents,"Zeng, Xin and Wang, Xiaoyu and Zhang, Tengxiang and Yu, Chun and Zhao, Shengdong and Chen, Yiqiang",2024,Proc. ACM Hum.-Comput. Interact.,,8,ISS,,Association for Computing Machinery,10.1145/3698145,https://doi.org/10.1145/3698145,"Existing gesture interfaces only work with a fixed set of gestures defined either by interface designers or by users themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans, on the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and common sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding procedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language Model agents to manage and synthesize gesture and context information, then infers the interaction intent by associating the gesture with an interface function. More specifically, our triple-agent framework includes a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the Gesture Inference Agent discerns the user?€?s intent by grounding it to an interactive function. We validated our framework offline under two real-world scenarios: smart home control and online video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks. We also provide an extensive discussion that includes rationale for model selection, generalizability, and future research directions for a practical system etc.","New York, NY, USA",545,,,Dec-24,"Free-Form Gesture, Gesture Recognition, Interaction Context, Zero-Shot",,,38,
inproceedings,Effects of Referring to Robot vs. User Needs in Self-Explanations of Undesirable Robot Behavior,"Stange, Sonja and Kopp, Stefan",2021,,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,271?€?275,Association for Computing Machinery,10.1145/3434074.3447174,https://doi.org/10.1145/3434074.3447174,"Autonomous or lively social robots will often exhibit behavior that is surprising to users and calls for explanation. However, it is not clear how such robot behavior should be explained best. Our previous work showed that different types of a robot's self-explanations, citing its actions, intentions, or needs - alone or in causal relations - have different effects on users (Stange &amp; Kopp, 2020). Further analysis of the data from the cited study implies that explanations in terms of robot needs (e.g. for energy or social contact) did not adequately justify the robot's behavior. In this paper we study the effects of a robot citing the user's needs to explain its behavior. Our study is based on the assumption that users may feel more connected to a robot that aims to recognize and incorporate the users' needs in its decision-making, even when the resulting behavior turns out to be undesirable. Results show that explaining robot behavior with user needs generally did neither lead to higher gains in understanding or desirability of the behaviors, nor did it help to justify them better than explaining it with robot needs. Further, a robot referring to user needs was not perceived as more likable, trustworthy or mindful, nor were users' contact intentions increased. However, an in-depth analysis showed different effects of explanations for different behaviors. We discuss these differences in order to clarify which factors should inform content and form of a robot's behavioral self-explanations.","New York, NY, USA",,9.78145E+12,,,"empirical study, explainability, human-robot interaction","Boulder, CO, USA",,5,HRI '21 Companion
inproceedings,PPS: Personalized Policy Summarization for Explaining Sequential Behavior of Autonomous Agents,"Qian, Peizhu and Huang, Harrison and Unhelkar, Vaibhav",2025,,"Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society",,,1167?€?1179,AAAI Press,,,"AI-enabled agents designed to assist humans are gaining traction in a variety of domains such as healthcare and disaster response. It is evident that, as we move forward, these agents will play increasingly vital roles in our lives. To realize this future successfully and mitigate its unintended consequences, it is imperative that humans have a clear understanding of the agents that they work with. Policy summarization methods help facilitate this understanding by showcasing key examples of agent behaviors to their human users. Yet, existing methods produce",,,,,,,"San Jose, California, USA",,13,AIES '24
article,Building the Foundation of Robot Explanation Generation Using Behavior Trees,"Han, Zhao and Giger, Daniel and Allspaw, Jordan and Lee, Michael S. and Admoni, Henny and Yanco, Holly A.",2021,J. Hum.-Robot Interact.,,10,3,,Association for Computing Machinery,10.1145/3457185,https://doi.org/10.1145/3457185,"As autonomous robots continue to be deployed near people, robots need to be able to explain their actions. In this article, we focus on organizing and representing complex tasks in a way that makes them readily explainable. Many actions consist of sub-actions, each of which may have several sub-actions of their own, and the robot must be able to represent these complex actions before it can explain them. To generate explanations for robot behavior, we propose using Behavior Trees (BTs), which are a powerful and rich tool for robot task specification and execution. However, for BTs to be used for robot explanations, their free-form, static structure must be adapted. In this work, we add structure to previously free-form BTs by framing them as a set of semantic sets {goal, subgoals, steps, actions","New York, NY, USA",26,,,Sep-21,"Behavior explanation, behavior trees, robot explanation generation, robot transparency, state summarization",,,31,
inproceedings,Explaining Sequences of Actions in Multi-agent Deep Reinforcement Learning Models,"Wai, Khaing Phyo and Geng, Minghong and Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-Hwee",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2537?€?2539,International Foundation for Autonomous Agents and Multiagent Systems,,,"This paper introduces a method to explain MADRL agents' behaviors by abstracting their actions into high-level strategies. Particularly, a spatio-temporal neural network model is applied to encode the agents' sequences of actions as memory episodes wherein an aggregating memory retrieval can generalize them into a concise abstract representation of collective strategies. To assess the effectiveness of our method, we applied it to explain the actions of QMIX MADRL agents playing a StarCraft Multi-agent Challenge (SMAC) video game. A user study on the perceived explainability of the extracted strategies indicates that our method can provide comprehensible explanations at various levels of granularity.","Richland, SC",,9.7984E+12,,,"explainable artificial intelligence, multi-agent deep reinforcement learning, sequential decision making","Auckland, New Zealand",,3,AAMAS '24
inproceedings,?€?An Error Occurred!?€? - Trust Repair With Virtual Robot Using Levels of Mistake Explanation,"Hald, Kasper and Weitz, Katharina and Andr\'{e",2021,,Proceedings of the 9th International Conference on Human-Agent Interaction,,,218?€?226,Association for Computing Machinery,10.1145/3472307.3484170,https://doi.org/10.1145/3472307.3484170,"Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes.","New York, NY, USA",,9.78145E+12,,,"XAI, human-robot collaboration, human-robot trust, proximity, robot mistakes, virtual reality","Virtual Event, Japan",,9,HAI '21
inproceedings,Culture-Based Explainable Human-Agent Deconfliction,"Raymond, Alex and Gunes, Hatice and Prorok, Amanda",2020,,Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,,,1107?€?1115,International Foundation for Autonomous Agents and Multiagent Systems,,,"Law codes and regulations help organise societies for centuries, and as AI systems gain more autonomy, we question how human-agent systems can operate as peers under the same norms, especially when resources are contended. We posit that agents must be accountable and explainable by referring to which rules justify their decisions. The need for explanations is associated with user acceptance and trust. This paper's contribution is twofold:i) we propose an argumentation-based human-agent architecture to map human regulations into aculture for artificial agents with explainable behaviour. Our architecture leans on the notion of argumentative dialogues and generates explanations from the history of such dialogues; andii) we validate our architecture with a user study in the context of human-agent path deconfliction. Our results show that explanations provide a significantly higher improvement in human performance when systems are more complex. Consequently, we argue that the criteria defining the need of explanations should also consider the complexity of a system. Qualitative findings show that when rules are more complex, explanations significantly reduce the perception of challenge for humans.","Richland, SC",,9.78145E+12,,,"argumentation, explanation, human-agent systems, user studies","Auckland, New Zealand",,9,AAMAS '20
inproceedings,User Perception on Personalized Explanation by Science Museum Docent Robot,"Park, Jeyoung and Kim, Jeeyeon and Kim, Da-Young and Kim, Juhyun and Kim, Min-Gyu and Choi, Jihwan and Lee, WonHyong",2022,,Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction,,,973?€?975,IEEE Press,,,"As the number of docent robots in museums has increased, robot personalization services have become important. A survey-based experiment was conducted to catch the difference in perceptions of personalized service for exhibition visitors. As a result, it was found that the background knowledge of the visitors listening to the explanation had an effect on the perception of the personalized service. The finding gives us a set of design criteria for personalized museum guide robot services.",,,,,,"background knowledge, docent robots, guidance, hri, personalization services","Sapporo, Hokkaido, Japan",,3,HRI '22
inproceedings,Transparency in HRI: Trust and Decision Making in the Face of Robot Errors,"Nesset, Birthe and Robb, David A. and Lopes, Jos\'{e",2021,,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,313?€?317,Association for Computing Machinery,10.1145/3434074.3447183,https://doi.org/10.1145/3434074.3447183,"Robots are rapidly gaining acceptance in recent times, where the general public, industry and researchers are starting to understand the utility of robots, for example for delivery to homes or in hospitals. However, it is key to understand how to instil the appropriate amount of trust in the user. One aspect of a trustworthy system is its ability to explain actions and be transparent, especially in the face of potentially serious errors. Here, we study the various aspects of transparency of interaction and its effect in a scenario where a robot is performing triage when a suspected Covid-19 patient arrives at a hospital. Our findings consolidate prior work showing a main effect of robot errors on trust, but also showing that this is dependent on the level of transparency. Furthermore, our findings indicate that high interaction transparency leads to participants making better informed decisions on their health based on their interaction. Such findings on transparency could inform interaction design and thus lead to greater adoption of robots in key areas, such as health and well-being.","New York, NY, USA",,9.78145E+12,,,"hospital triage, robot errors, social robotics, transparency, trust","Boulder, CO, USA",,5,HRI '21 Companion
inproceedings,Explaining Agent Preferences and Behavior: Integrating Reward Decomposition and Contrastive Highlights,"Septon, Yael and Amitai, Yotam and Amir, Ofra",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2295?€?2297,International Foundation for Autonomous Agents and Multiagent Systems,,,"Explainable reinforcement learning methods aim to help elucidate agent policies and their underlying decision-making processes. One such method is reward decomposition, which aims to reveal an agent's preferences in a specific world-state by presenting its expected utility decomposed to different components of the reward function. While this approach quantifies the expected decomposed rewards for alternative actions, it does not demonstrate the outcomes of these alternative actions in terms of the behavior of the agent. This work introduces ''Contrastive Highlights'', a novel local explanation method that visually compares the agent's chosen behavior to an alternative choice of action in a contrastive manner. We conducted user studies comparing participants' understanding of agents' preferences based on either reward decomposition, contrastive highlights, or a combination of both approaches. Our results show that integrating reward decomposition with contrastive highlights significantly improved participants' performance compared to using each of the approaches separately.","Richland, SC",,9.78145E+12,,,"deep reinforcement learning, explainable ai, human-ai interaction","London, United Kingdom",,3,AAMAS '23
inproceedings,Explainability in Multi-Agent Path/Motion Planning: User-study-driven Taxonomy and Requirements,"Brandao, Martim and Mansouri, Masoumeh and Mohammed, Areeb and Luff, Paul and Coles, Amanda",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,172?€?180,International Foundation for Autonomous Agents and Multiagent Systems,,,"Multi-Agent Path Finding (MAPF) and Multi-Robot Motion Planning (MRMP) are complex problems to solve, analyze and build algorithms for. Automatically-generated explanations of algorithm output, by improving human understanding of the underlying problems and algorithms, could thus lead to better user experience, developer knowledge, and MAPF/MRMP algorithm designs. Explanations are contextual, however, and thus developers need a good understanding of the questions that can be asked about algorithm output, the kinds of explanations that exist, and the potential users and uses of explanations in MAPF/MRMP applications. In this paper we provide a first step towards establishing a taxonomy of explanations, and a list of requirements for the development of explainable MAPF/MRMP planners. We use interviews and a questionnaire with expert developers and industry practitioners to identify the kinds of questions, explanations, users, uses, and requirements of explanations that should be considered in the design of such explainable planners. Our insights cover a diverse set of applications: warehouse automation, computer games, and mining.","Richland, SC",,9.78145E+12,,,"explainable AI, explainable planning, multi-agent path finding, multi-robot motion planning","Virtual Event, New Zealand",,9,AAMAS '22
inproceedings,Towards Explaining Sequences of Actions in Multi-Agent Deep Reinforcement Learning Models,"Wai, Khaing Phyo and Geng, Minghong and Subagdja, Budhitama and Pateria, Shubham and Tan, Ah-Hwee",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2325?€?2327,International Foundation for Autonomous Agents and Multiagent Systems,,,"Although Multi-agent Deep Reinforcement Learning (MADRL) has shown promising results in solving complex real-world problems, the applicability and reliability of MADRL models are often limited by a lack of understanding of their inner workings for explaining the decisions made. To address this issue, this paper proposes a novel method for explaining MADRL by generalizing the sequences of action events performed by agents into high-level abstract strategies using a spatio-temporal neural network model. Specifically, an interval-based memory retrieval procedure is developed to generalize the encoded sequences of action events over time into short sequential patterns. In addition, two abstraction algorithms are introduced, one for abstracting action events across multiple agents and the other for further abstracting the episodes over time into short sequential patterns, which can then be translated into symbolic form for interpretation. We evaluate the proposed method using the StarCraft Multi Agent Challenge (SMAC) benchmark task, which shows that the method is able to derive high-level explanations of MADRL models at various levels of granularity.","Richland, SC",,9.78145E+12,,,"explainable artificial intelligence, explainable deep reinforcement learning, multi agent deep reinforcement learning","London, United Kingdom",,3,AAMAS '23
inproceedings,Explainable Multi Agent Path Finding,"Almagor, Shaull and Lahijanian, Morteza",2020,,Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,,,34?€?42,International Foundation for Autonomous Agents and Multiagent Systems,,,"Multi Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In safety-critical systems, the plan is typically checked by a human supervisor, who decides on whether to allow its execution. In such cases, we wish to convince the human that the plan is indeed collision free.To this end, we propose an explanation scheme for MAPF, which bases explanations on simplicity of visual verification by human's cognitive process. The scheme decomposes a plan into segments such that within each segment, the paths of the agents are disjoint. Then, we can convince the supervisor that the plan is collision free using a small number of images (dubbed an explanation). In addition, we can measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme, as well as the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also provide experimental results of our scheme both in a continuous and in a discrete setting.","Richland, SC",,9.78145E+12,,,"explainability, mapf, motion planning, multi-agent systems, path finding, path planning","Auckland, New Zealand",,9,AAMAS '20
article,Co-Designing with Users the Explanations for a Proactive Auto-Response Messaging Agent,"Jain, Pranut and Farzan, Rosta and Lee, Adam J.",2023,Proc. ACM Hum.-Comput. Interact.,,7,MHCI,,Association for Computing Machinery,10.1145/3604248,https://doi.org/10.1145/3604248,"Explanations of AI Agents' actions are considered to be an important factor in improving users' trust in the decisions made by autonomous AI systems. However, as these autonomous systems evolve from reactive, i.e., acting on user input, to proactive, i.e., acting without requiring user intervention, there is a need to explore how the explanation for the actions of these agents should evolve. In this work, we explore the design of explanations through participatory design methods for a proactive auto-response messaging agent that can reduce perceived obligations and social pressure to respond quickly to incoming messages by providing unavailability-related context. We recruited 14 participants who worked in pairs during collaborative design sessions where they reasoned about the agent's design and actions. We qualitatively analyzed the data collected through these sessions and found that participants' reasoning about agent actions led them to speculate heavily on its design. These speculations significantly influenced participants' desire for explanations and the controls they sought to inform the agents' behavior. Our findings indicate a need to transform users' speculations into accurate mental models of agent design. Further, since the agent acts as a mediator in human-human communication, it is also necessary to account for social norms in its explanation design. Finally, user expertise in understanding their habits and behaviors allows the agent to learn from the user their preferences when justifying its actions.","New York, NY, USA",201,,,Sep-23,"co-design, explanations, messaging awareness, proactive agents",,,23,
inproceedings,Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,"Xing, Mingzhe and Zhang, Rongkai and Xue, Hui and Chen, Qi and Yang, Fan and Xiao, Zhen",2024,,Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,,6061?€?6072,Association for Computing Machinery,10.1145/3637528.3671650,https://doi.org/10.1145/3637528.3671650,"Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.","New York, NY, USA",,9.7984E+12,,,"ai agent, large language model, task planning","Barcelona, Spain",,12,KDD '24
inproceedings,Understanding Design Preferences for Robots for Pain Management: A Co-Design Study,"Zhang, Feiran and Broz, Frank and Dertien, Edwin and Kousi, Nefeli and van Gurp, Jules A. M. and Ferrari, Oriana Isabella and Malagon, Ignacio and Barakova, Emilia I.",2022,,Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction,,,1124?€?1129,IEEE Press,,,"There is growing interest in psychological interventions using socially assistive robots to mitigate distress and pain in the pediatric population. This work seeks to address the deficit in understanding of what features and functionality young children and their parents desire to help with pain management by using co-design, a common approach to exploring participants' imaginations and gathering design requirements. To close this gap, we carried out a co-design workshop involving seven families (with children aged between 4-6 and their parents) to understand their expectations and design preferences for a robot designed for pain management in children. Data were collected from surveys, video and audio recordings, interviews, and field notes. We present the robot prototypes constructed during the workshops and derive several preferences of the children (e.g., zoomorphic shape, distractors and emotional expressions as behaviors). Additionally, we report methodological insights regarding the involvement of young children and their parents in the co-design process. Based on the findings of this co-design study, we discuss personalization as a possible design concept for future child-robot interaction development.",,,,,,"child, child-robot interaction, co-design, pain management, parent, social robots","Sapporo, Hokkaido, Japan",,6,HRI '22
inproceedings,Interdisciplinary Explorations of Processes of Mutual Understanding in Interaction with Assistive Shopping Robots,"Yamazaki, Akiko and Krummheuer, Antonia Lina and Imai, Michita",2022,,Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction,,,1293?€?1295,IEEE Press,,,"The main goal of this workshop is to establish awareness of the emerging field of socially assistive shopping robots in human-robot interaction (HRI) and, simultaneously, to foster interdisciplinary approaches that combine the development of social robots with a sequential and embodied perspective regarding mutual processes of understanding in shopping interactions with assistive robots.",,,,,,"assistance, conversation analysis, embodiment, ethnomethodology, hri, interaction, shopping, social robot, understanding","Sapporo, Hokkaido, Japan",,3,HRI '22
inproceedings,Anomaly Detection for Dynamic Human-Robot Assembly: Application of an LSTM-based Autoencoder to interpret uncertain human behavior in HRC,"Schirmer, Fabian and Kranz, Philipp and Schmitt, Jan and Kaupp, Tobias",2023,,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,,333?€?337,Association for Computing Machinery,10.1145/3568294.3580100,https://doi.org/10.1145/3568294.3580100,"Human-Robot Collaboration (HRC) requires humans and robots to work on the same product in the same work environment at the same time. Therefore, the robotic system needs to understand human behavior so it can assist the human appropriately. Since the human is an uncertain variable in this system, human action recognition is one of the key challenges when it comes to HRC. To address this problem, we developed an anomaly detection framework for the dynamic assembly of complex products. We used an Long-Short-Term-Memory (LSTM)-based autoencoder to detect anomalies in human behavior and post-process the output to categorize it as a green or red anomaly. A green anomaly represents a deviation from the intended order but a valid assembly sequence. A red anomaly represents an invalid sequence. In both cases, the worker is guided to complete the assembly process. We demonstrate our proposed framework using an appropriate industrial use case.","New York, NY, USA",,9.78145E+12,,,"dynamic assembly sequence, human action recognition, human robot collaboration, lstm-based autoencoder","Stockholm, Sweden",,5,HRI '23
inproceedings,pgeon applied to Overcooked-AI to explain agents' behaviour,"Tormos, Adrian and Gimenez-Abalos, Victor and V\'{a",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,2821?€?2823,International Foundation for Autonomous Agents and Multiagent Systems,,,"Policy Graphs (PGs) are a method for representing the behaviour of opaque agents by observing them in the environment and producing graphs where the state and action spaces are discretised into predicates. We present pgeon, a Python library that demonstrates the effectiveness of PGs in providing explanations for the behaviour of agents and we showcase it by applying it to a multi-agent cooperative environment: Overcooked-AI. This library illustrates how PGs can create transparent and explainable surrogate agents that closely mimic the behavior of the original agents. These features can help improving trust in environments where humans and AI systems collaborate by improving the explainability of all agents, even opaque or human.","Richland, SC",,9.7984E+12,,,"explainability, multi-agent systems, reinforcement learning","Auckland, New Zealand",,3,AAMAS '24
article,Explainable Embodied Agents Through Social Cues: A Review,Wallk\,2021,J. Hum.-Robot Interact.,,10,3,,Association for Computing Machinery,10.1145/3457188,https://doi.org/10.1145/3457188,"The issue of how to make embodied agents explainable has experienced a surge of interest over the past 3 years, and there are many terms that refer to this concept, such as transparency and legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that different authors use these terms in different ways. Hence, we review the existing literature on explainability and organize it by (1) providing an overview of existing definitions, (2) showing how explainability is implemented and how it exploits different social cues, and (3) showing how the impact of explainability is measured. Additionally, we present a list of open questions and challenges that highlight areas that require further investigation by the community. This provides the interested reader with an overview of the current state of the art.","New York, NY, USA",27,,,Sep-21,"Transparency, accountability, embodied social agents, explainability, explainable agency, expressive behavior, intelligibility, interpretability, legibility, predictability, robots",,,24,
inproceedings,Trust and Reliance in Consensus-Based Explanations from an Anti-Misinformation Agent,"Ueno, Takane and Kim, Yeongdae and Oura, Hiroki and Seaborn, Katie",2023,,Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3544549.3585713,https://doi.org/10.1145/3544549.3585713,"The illusion of consensus occurs when people believe there is consensus across multiple sources, but the sources are the same and thus there is no","New York, NY, USA",296,9.78145E+12,,,"consensus, explainable AI, intelligent agent, misinformation, reliance, trust, user experience","Hamburg, Germany",,7,CHI EA '23
inproceedings,ELVIRA: An Explainable Agent for Value and Utility-Driven Multiuser Privacy,"Mosca, Francesca and Such, Jose M.",2021,,Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems,,,916?€?924,International Foundation for Autonomous Agents and Multiagent Systems,,,"Online social networks fail to support users to adequately share co-owned content, which leads to privacy violations. Scholars proposed collaborative mechanisms to support users, but they did not satisfy one or more requirements needed according to empirical evidence in this domain, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven. We present ELVIRA, an agent that supports multiuser privacy, whose design meets all these requirements. By considering the sharing preferences and the moral values of users, ELVIRA identifies the optimal sharing policy. Furthermore, ELVIRA justifies the optimality of the solution through explanations based on argumentation. We prove via simulations that ELVIRA provides solutions with the best trade-off between individual utility and value adherence. We also show through a user study that ELVIRA suggests solutions that are more acceptable than existing approaches and that its explanations are also more satisfactory.","Richland, SC",,9.78145E+12,,,"explainable agents, human-agent interaction, multiuser privacy, practical reasoning, value-based agents","Virtual Event, United Kingdom",,9,AAMAS '21
inproceedings,,"Hou, Yuki and Tamoto, Haruki and Miyashita, Homei",2024,,Extended Abstracts of the CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3613905.3650839,https://doi.org/10.1145/3613905.3650839,"In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model (LLM)-based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user?€?s interaction history in a database that encapsulates each memory?€?s content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.","New York, NY, USA",7,9.7984E+12,,,"Intelligent Agents, Large Language Models, Memory Retrieval Models, User Experience, User Interface","Honolulu, HI, USA",,7,CHI EA '24
article,Integrity-based Explanations for Fostering Appropriate Trust in AI Agents,"Mehrotra, Siddharth and Jorge, Carolina Centeio and Jonker, Catholijn M. and Tielman, Myrthe L.",2024,ACM Trans. Interact. Intell. Syst.,,14,1,,Association for Computing Machinery,10.1145/3610578,https://doi.org/10.1145/3610578,"Appropriate trust is an important component of the interaction between people and AI systems, in that ?€?inappropriate?€? trust can cause disuse, misuse, or abuse of AI. To foster appropriate trust in AI, we need to understand how AI systems can elicit appropriate levels of trust from their users. Out of the aspects that influence trust, this article focuses on the effect of showing integrity. In particular, this article presents a study of how different integrity-based explanations made by an AI agent affect the appropriateness of trust of a human in that agent. To explore this, (1) we provide a formal definition to measure appropriate trust, (2) present a between-subject user study with 160 participants who collaborated with an AI agent in such a task. In the study, the AI agent assisted its human partner in estimating calories on a food plate by expressing its integrity through explanations focusing on either honesty, transparency, or fairness. Our results show that (a) an agent who displays its integrity by being explicit about potential biases in data or algorithms achieved appropriate trust more often compared to being honest about capability or transparent about the decision-making process, and (b) subjective trust builds up and recovers better with honesty-like integrity explanations. Our results contribute to the design of agent-based AI systems that guide humans to appropriately trust them, a formal method to measure appropriate trust, and how to support humans in calibrating their trust in AI.","New York, NY, USA",4,,2160-6455,Mar-24,"HCI, intelligent agents, artificial agents, fairness, transparency, honesty, explanations, trust, appropriate trust, Integrity",,,36,
article,"Inform, Explain, or Control: Techniques to Adjust End-User Performance Expectations for a Conversational Agent Facilitating Group Chat Discussions","Do, Hyo Jin and Kong, Ha-Kyung and Tetali, Pooja and Karahalios, Karrie and Bailey, Brian P.",2023,Proc. ACM Hum.-Comput. Interact.,,7,CSCW2,,Association for Computing Machinery,10.1145/3610192,https://doi.org/10.1145/3610192,"A conversational agent (CA) effectively facilitates online group discussions at scale. However, users may have expectations about how well the CA would perform that do not match with the actual performance, compromising technology acceptance. We built a facilitator CA that detects a member who has low contribution during a synchronous group chat discussion and asks the person to participate more. We designed three techniques to set end-user expectations about how accurately the CA identifies an under-contributing member: 1)information: explicitly communicating the accuracy of the detection algorithm, 2)explanation: providing an overview of the algorithm and the data used for the detection, and 3)adjustment: enabling users to gain a feeling of control over the algorithm. We conducted an online experiment with 163 crowdworkers in which each group completed a collaborative decision-making task and experienced one of the techniques. Through surveys and interviews, we found that the explanation technique was the most effective strategy overall as it reduced user embarrassment, increased the perceived intelligence of the CA, and helped users better understand the detection algorithm. In contrast, the information technique reduced members' contributions and the adjustment technique led to a more negative perceived discussion experience. We also discovered that the interactions with other team members diluted the effects of the techniques on users' performance expectations and acceptance of the CA. We discuss implications for better designing expectation-setting techniques for AI-team collaboration such as ways to improve collaborative decision outcomes and quality of contributions.","New York, NY, USA",343,,,Oct-23,"collaborative decision-making task, conversational agent, facilitator, group chat discussion, participation balance, user acceptance of the agent",,,26,
article,Understanding the Performance of AI Algorithms in Text-Based Emotion Detection for Conversational Agents,"Kusal, Sheetal D. and Patil, Shruti G. and Choudrie, Jyoti and Kotecha, Ketan V.",2024,ACM Trans. Asian Low-Resour. Lang. Inf. Process.,,23,8,,Association for Computing Machinery,10.1145/3643133,https://doi.org/10.1145/3643133,"Current industry trends demand automation in every aspect, where machines could replace humans. Recent advancements in conversational agents have grabbed a lot of attention from industries, markets, and businesses. Building conversational agents that exhibit human communication characteristics is a need in today's marketplace. Thus, by accumulating emotions, we can build emotionally aware conversational agents. Emotion detection in text-based dialogues has turned into a pivotal component of conversational agents, enhancing their ability to understand and respond to users?€? emotional states. This article extensively compares various artificial intelligence techniques adapted to text-based emotion detection for conversational agents. The study covers a wide range of methods, from machine learning models to cutting-edge pre-trained models and deep learning models. We evaluate the performance of these techniques on the benchmark unbalanced Topical-Chat and balanced Empathetic Dialogue datasets. This article offers an overview of the practical implications of emotion detection techniques in conversational systems and their impact on user response. The outcomes of this work contribute to the ongoing development of empathetic conversational agents, emphasizing natural human-machine interactions.","New York, NY, USA",121,,2375-4699,Aug-24,"Artificial intelligence, natural language processing, machine learning, deep learning, text-based emotion detection, pre-trained models",,,26,
inproceedings,Automatic CDT Scoring Using Machine Learning with Interpretable Feature,"Chen, Bo-Lin and Hu, Kuan-Ting and Cheng, Kuo-Sheng and Chen, Chien-Yu",2024,,"Proceedings of the 2024 14th International Conference on Bioscience, Biochemistry and Bioinformatics",,,55?€?59,Association for Computing Machinery,10.1145/3640900.3640910,https://doi.org/10.1145/3640900.3640910,"The Clock Drawing Test (CDT) is a widely used cognitive assessment tool in clinical practice. However, it requires a trained neuropsychologist to evaluate the drawings, and the scoring process may be subjective due to the experience of the neuropsychologist. In this paper, we propose a novel automatic CDT scoring method based on interpretable features using machine learning. First, we use image processing techniques to extract features associated with the scoring guideline. Then, we combine these features as an input vector for training the machine learning classifier. Our experimental results demonstrate that our method achieves an accuracy of 82%, which is superior to that of deep learning methods.","New York, NY, USA",,9.7984E+12,,,,"Kyoto, Japan",,5,ICBBB '24
inproceedings,"Understanding User Perceptions of Robot's Delay, Voice Quality-Speed Trade-off and GUI during Conversation","Peng, Zhenhui and Mo, Kaixiang and Zhu, Xiaogang and Chen, Junlin and Chen, Zhijun and Xu, Qian and Ma, Xiaojuan",2020,,Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems,,,1?€?8,Association for Computing Machinery,10.1145/3334480.3382792,https://doi.org/10.1145/3334480.3382792,"Conversational robots face the practical challenge of providing timely responses to ensure smooth interactions with users. Thus, those who design and implement robots will need to understand how different levels of delay in response may affect users' satisfaction with the conversation, how to balance the trade-off between a robot's quality of voice and response time, and how to design strategies to mitigate possible negative effects of a long delay. Via an online video-prototype study on a service robot with 94 Chinese participants, we find that users could tolerate up to 4s delay but their satisfaction drops at the 8s delay during both information-retrieval conversations and chitchats. We gain an in-depth understanding of users' preference for the trade-off between the voice quality and the response speed, as well as their opinions on possible robot graphic user interface (GUI) design to alleviate negative user experience with response latency.","New York, NY, USA",,9.78145E+12,,,"delay, graphic user interface, human-robot interaction, speech synthesis, voice","Honolulu, HI, USA",,8,CHI EA '20
inproceedings,Evaluating a Mechanism for Explaining BDI Agent Behaviour,"Winikoff, Michael and Sidorenko, Galina",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2283?€?2285,International Foundation for Autonomous Agents and Multiagent Systems,,,"We conducted a survey to evaluate a previously proposed mechanism for explaining Belief-Desire-Intention (BDI) agents using folk psychological concepts (belief, desires, and valuings). We also consider the relationship between trust in the specific autonomous system, and general trust in technology. We find that explanations that include valuings are particularly likely to be preferred by the study participants. We also found evidence that single-factor explanations, as used in some previous work, are too short.","Richland, SC",,9.78145E+12,,,"belief-desire-intention (bdi), explainable agency, explanation","London, United Kingdom",,3,AAMAS '23
inproceedings,An EEG-based Automatic Classification Model for Epilepsy with Explainable Artificial Intelligence,"Wei, Lan and Mooney, Catherine",2024,,Proceedings of the 2024 14th International Conference on Biomedical Engineering and Technology,,,44?€?50,Association for Computing Machinery,10.1145/3678935.3678943,https://doi.org/10.1145/3678935.3678943,"Effective monitoring of patients?€? conditions is crucial in medical practice. Machine learning methods hold promise for automating disease detection, including epilepsy. However, the opacity of these black-box models presents significant challenges. In this study, we propose a LightGBM-based automatic classification model for epilepsy trained using TUH EEG data. Ten channels were employed, and 22 features from both the time and frequency domains were estimated from each channel. The model can distinguish between normal EEG, focal epilepsy, and generalised epilepsy. We trained the model on a dataset comprising 600 adult EEG records. Five-fold cross-validation yielded a mean accuracy of 89.46% and a mean F1 score of 0.8907 on the training set. To assess the model?€?s generalization performance, we independently tested it on 456 EEGs, achieving an accuracy of 71.49% and a weighted F1-score of 0.7386. Furthermore, we employed permutation feature importance, SHAP and LIME, to provide explanations for the model?€?s decisions. This model has the potential to gain the trust of clinicians and facilitate its adoption in clinical settings.","New York, NY, USA",,9.7984E+12,,,"EEG, Epilepsy, Explainable AI, Machine Learning","Seoul, Republic of Korea",,7,ICBET '24
inproceedings,On the Influence of Autonomy and Transparency on Blame and Credit in Flawed Human-Robot Collaboration,"Arntz, Alexander and Eimler, Sabrina C. and Stra\ss{",2021,,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,377?€?381,Association for Computing Machinery,10.1145/3434074.3447196,https://doi.org/10.1145/3434074.3447196,"The collaboration between humans and autonomous AI-driven robots in industrial contexts is a promising vision that will have an impact on the sociotechnical system. Taking research from the field of human teamwork as guiding principles as well as results from human robot collaboration studies this study addresses open questions regarding the design and impact of communicative transparency and behavioral autonomy in a human robot collaboration. In an experimental approach, we tested whether an AI-narrative and communication panels of a robot-arm trigger the attribution of more human like traits and expectations going along with a changed attribution of blame and failure in a flawed collaboration.","New York, NY, USA",,9.78145E+12,,,"perception of intelligence, online study, human-robot collaboration, attribution of blame","Boulder, CO, USA",,5,HRI '21 Companion
inproceedings,"Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agents","Sun, Lingyun and Li, Zhuoshu and Zhang, Yuyang and Liu, Yanzhen and Lou, Shanghua and Zhou, Zhibin",2021,,Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3411763.3451819,https://doi.org/10.1145/3411763.3451819,"With the increasing prevalence of Artificial Intelligence (AI) agents, the transparency of agents becomes vital in addressing the interaction issues (e.g., explainability and trust). The existing body of research provides valuable theoretical and practical studies in this field. However, determining the transparency of AI agents requires the systematic consideration of the application categories and automation level, which is hardly considered by the prior literature. We thus apply the bibliometric analysis to gain insights from the published literature. Our work outlines the trend of how the number of studies about AI agent transparency increased over the years. We also identify the major application topics and issues in designing transparent AI agents. Furthermore, we categorize the identified applications according to the specific dimensions (risk and timeliness) and put forward potential strategies for designing different agents. Besides, we suggest the possible transparency degree corresponding to the automation level.","New York, NY, USA",346,9.78145E+12,,,"AI, Transparency, Transparent agent, Trend","Yokohama, Japan",,8,CHI EA '21
inproceedings,Agency Aspirations: Understanding Users' Preferences And Perceptions Of Their Role In Personalised News Curation,"Rezk, Anna Marie and Simkute, Auste and Luger, Ewa and Vines, John and Elsden, Chris and Evans, Michael and Jones, Rhianne",2024,,Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3613904.3642634,https://doi.org/10.1145/3613904.3642634,"Recommender systems are increasingly employed by journalistic outlets to deliver personalised news, transforming news curation into a reciprocal yet insufficiently defined process influenced by editors, recommender systems, and individual user actions. To understand the tension in this dynamic and users?€? preferences and perceptions of their role in personalised news curation, we conducted a study with UK participants aged 16-34. Building on a preliminary survey and interview study, which revealed a strong desire from participants for increased agency in personalisation, we designed an interactive news recommender provotype (provocative design artefact) which probed the role of agency in news curation with participants (n=16). Findings highlighted a behaviour-intention gap, indicating participants desire for agency yet reluctance to intervene actively in personalisation. Our research offers valuable insights into how users perceive their agency in personalised news curation, underscoring the importance for systems to be designed to support individuals becoming active agents in news personalisation.","New York, NY, USA",190,9.7984E+12,,,"Interaction Design, Personalisation, User Experience Design","Honolulu, HI, USA",,16,CHI '24
inproceedings,How Does the General Population Understand Robot State?,"Agnihotri, Abhijeet and Tsui, Katherine M.",2021,,Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,136?€?140,Association for Computing Machinery,10.1145/3434074.3447145,https://doi.org/10.1145/3434074.3447145,"With an increasing number of home and social robot products, it is essential for the general population to feel comfortable in using and understanding these robots in their homes. The goal of this research is to understand the general population's definition of","New York, NY, USA",,9.78145E+12,,,"robot state, human-robot interaction, home robots","Boulder, CO, USA",,5,HRI '21 Companion
inproceedings,"Effective Human-Machine Teaming through Communicative Autonomous Agents that Explain, Coach, and Convince","Tabrez, Aaquib",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,3008?€?3010,International Foundation for Autonomous Agents and Multiagent Systems,,,"Effective communication is essential for human-robot collaboration to improve task efficiency, fluency, and safety. Good communication between teammates provides shared situational awareness, allowing them to adapt and improvise successfully during uncertain situations, and helps identify and remedy any potential misunderstandings in the case of incongruous mental models. This doctoral proposal focuses on improving human-agent communication by leveraging explainable AI techniques to empower autonomous agents to 1) communicate insights into their capabilities and limitations to a human collaborator, 2) coach and influence human teammates' behavior during joint task execution, and 3) successfully convince and mediate trust in human-robot interactions.","Richland, SC",,9.78145E+12,,,"augmented reality, explainable ai, human-agent collaboration, policy explanations, reinforcement learning, shared mental models","London, United Kingdom",,3,AAMAS '23
inproceedings,Chain-of-Event: Interpretable Root Cause Analysis for Microservices through Automatically Learning Weighted Event Causal Graph,"Yao, Zhenhe and Pei, Changhua and Chen, Wenxiao and Wang, Hanzhang and Su, Liangfei and Jiang, Huai and Xie, Zhe and Nie, Xiaohui and Pei, Dan",2024,,Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering,,,50?€?61,Association for Computing Machinery,10.1145/3663529.3663827,https://doi.org/10.1145/3663529.3663827,"This paper presents Chain-of-Event (CoE), an interpretable model for root cause analysis in microservice systems that analyzes causal relationships of events transformed from multi-modal observation data. CoE distinguishes itself by its interpretable parameter design that aligns with the operation experience of Site Reliability Engineers (SREs), thereby facilitating the integration of their expertise directly into the analysis process. Furthermore, CoE automatically learns event-causal graphs from history incidents and accurately locates root cause events, eliminating the need for manual configuration. Through evaluation on two datasets sourced from an e-commerce system involving over 5,000 services, CoE achieves top-tier performance, with 79.30% top-1 and 98.8% top-3 accuracy on the Service dataset and 85.3% top-1 and 96.6% top-3 accuracy on the Business dataset. An ablation study further explores the significance of each component within the CoE model, offering insights into their individual contributions to the model?€?s overall effectiveness. Additionally, through real-world case analysis, this paper demonstrates how CoE enhances interpretability and improves incident comprehension for SREs. Our codes are available at https://github.com/NetManAIOps/Chain-of-Event.","New York, NY, USA",,9.7984E+12,,,"Event-based Root Cause Analysis, Interpretable Root Causal Localization","Porto de Galinhas, Brazil",,12,FSE 2024
inproceedings,Towards a Value-driven Explainable Agent for Collective Privacy,"Mosca, Francesca and Such, Jose M. and McBurney, Peter",2020,,Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,,,1937?€?1939,International Foundation for Autonomous Agents and Multiagent Systems,,,"Online social networks lack support for the collaborative management of access control. This is crucial for content that may involve multiple users such as photos, as this lack of support causes conflicts that lead to privacy violations. Previous research proposed collaborative mechanisms to support users in these cases, but most of these attempts fail to satisfy some desirable requirements, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven at the same time. In this paper, we outline an agent architecture that has been designed to meet all these requirements.","Richland, SC",,9.78145E+12,,,"multiuser privacy, morally-aligned agents, explainable agents","Auckland, New Zealand",,3,AAMAS '20
article,Using User-Generated YouTube Videos to Understand Unguided Interactions with Robots in Public Places,"Nielsen, Sara and Skov, Mikael B. and Hansen, Karl Damkj\ae{",2023,J. Hum.-Robot Interact.,,12,1,,Association for Computing Machinery,10.1145/3550280,https://doi.org/10.1145/3550280,"Professional service robots are increasingly being deployed in public places, which thus increases user exposure. However, we lack an empirical understanding of complex encounters taking place in dynamic and often crowded environments as well as how people overcome breakdowns during unguided interaction with a robot in a real-world scenario. In this paper, we conducted a covert, digital ethnographic study analyzing 104 user-generated YouTube videos focusing on people?€?s unguided interactions with robots in several public places. We identified several types of interaction breakdowns pertaining to someone (person-initiated interaction breakdown, IB) or something (environmental disturbances, ED) having a direct, negative effect on an ongoing unguided interaction. Our findings have implications for the design and development of service robots facing multi-user scenarios entertaining active (primary and secondary) users, inactive (commentators and observers) ?€?users?€?, and Incidentally Co-present Persons (InCoPs). Furthermore, we contribute to and built on the limited prior use of YouTube videos and digital ethnography in HRI research, thereby demonstrating its effectiveness in studying unguided interactions in public places, while supplementing and adding to the existing knowledge base of service robots in public places.","New York, NY, USA",5,,,Mar-23,"robots in public places, digital ethnography, interaction breakdowns, unguided interactions, Service robots",,,40,
inproceedings,Towards Robust Contrastive Explanations for Human-Neural Multi-agent Systems,"Leofante, Francesco and Lomuscio, Alessio",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2343?€?2345,International Foundation for Autonomous Agents and Multiagent Systems,,,Generating explanations of high quality is fundamental to the development of trustworthy human-AI interactions. We here study the problem of generating contrastive explanations with formal robustness guarantees. We formalise a new notion of robustness and introduce two novel verification-based algorithms to (i) identify non-robust explanations generated by other methods and (ii) generate contrastive explanations augmented with provable robustness certificates. We present an implementation and evaluate the utility of the approach on two case studies concerning neural agents trained on credit scoring and image classification tasks.,"Richland, SC",,9.78145E+12,,,"explainable ai, formal verification","London, United Kingdom",,3,AAMAS '23
inproceedings,Automatic Interpretable Personalized Learning,"Prihar, Ethan and Haim, Aaron and Sales, Adam and Heffernan, Neil",2022,,Proceedings of the Ninth ACM Conference on Learning @ Scale,,,1?€?11,Association for Computing Machinery,10.1145/3491140.3528267,https://doi.org/10.1145/3491140.3528267,"Personalized learning stems from the idea that students benefit from instructional material tailored to their needs. Many online learning platforms purport to implement some form of personalized learning, often through on-demand tutoring or self-paced instruction, but to our knowledge none have a way to automatically explore for specific opportunities to personalize students' education nor a transparent way to identify the effects of personalization on specific groups of students. In this work we present the Automatic Personalized Learning Service (APLS). The APLS uses multi-armed bandit algorithms to recommend the most effective support to each student that requests assistance when completing their online work, and is currently used by ASSISTments, an online learning platform. The first empirical study of the APLS found that Beta-Bernoulli Thompson Sampling, a popular and effective multi-armed bandit algorithm, was only slightly more capable of selecting helpful support than randomly selecting from the relevant support options. Therefore, we also present Decision Tree Thompson Sampling (DTTS), a novel contextual multi-armed bandit algorithm that integrates the transparency and interpretability of decision trees into Thomson sampling. In simulation, DTTS overcame the challenges of recommending support within an online learning platform and was able to increase students' learning by as much as 10% more than the current algorithm used by the APLS. We demonstrate that DTTS is able to identify qualitative interactions that not only help determine the most effective support for students, but that also generalize well to new students, problems, and support content. The APLS using DTTS is now being deployed at scale within ASSISTments and is a promising tool for all educational learning platforms.","New York, NY, USA",,9.78145E+12,,,"contextual bandits, intelligent tutoring systems, multi-armed bandits, personalized learning","New York City, NY, USA",,11,L@S '22
inproceedings,Bridging Multimedia Modalities: Enhanced Multimodal AI Understanding and Intelligent Agents,"Gautam, Sushant",2023,,Proceedings of the 25th International Conference on Multimodal Interaction,,,695?€?699,Association for Computing Machinery,10.1145/3577190.3614225,https://doi.org/10.1145/3577190.3614225,"With the increasing availability of multimodal data, especially in the sports and medical domains, there is growing interest in developing Artificial Intelligence (AI) models capable of comprehending the world in a more holistic manner. Nevertheless, various challenges exist in multimodal understanding, including the integration of multiple modalities and the resolution of semantic gaps between them. The proposed research aims to leverage multiple input modalities for the multimodal understanding of AI models, enhancing their reasoning, generation, and intelligent behavior. The research objectives focus on developing novel methods for multimodal AI, integrating them into conversational agents with optimizations for domain-specific requirements. The research methodology encompasses literature review, data curation, model development and implementation, evaluation and performance analysis, domain-specific applications, and documentation and reporting. Ethical considerations will be thoroughly addressed, and a comprehensive research plan is outlined to provide guidance. The research contributes to the field of multimodal AI understanding and the advancement of sophisticated AI systems by experimenting with multimodal data to enhance the performance of state-of-the-art neural networks.","New York, NY, USA",,9.7984E+12,,,"AI Understanding, Conversational Agents, Multimedia, Multimodal Fusion","Paris, France",,5,ICMI '23
inproceedings,Domestic Robots and the Dream of Automation: Understanding Human Interaction and Intervention,"Schneiders, Eike and Kanstrup, Anne Marie and Kjeldskov, Jesper and Skov, Mikael B.",2021,,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3411764.3445629,https://doi.org/10.1145/3411764.3445629,"Domestic robots such as vacuum cleaners or lawnmowers are becoming popular consumer products in private homes, but while current HCI research on domestic robots has highlighted for example personalisation, long-term effects, or design guidelines, little attention has been paid to automation. To address this, we conducted a qualitative study with 24 participants in private households using interviews, contextual technology tours, and robot deployment. Through thematic analysis we identified three themes related to 1) work routines and automation, 2) domestic robot automation and the physical environment, as well as 3) interaction and breakdown intervention. We present an empirical understanding of how task automation using domestic robots can be implemented in the home. Lastly, we discuss our findings in relation to existing literature and highlight three opportunities for improved task automation using domestic robots for future research.","New York, NY, USA",241,9.78145E+12,,,"interventions to domestic robot breakdown, human-robot interaction, domestic robots, domestic robot automation","Yokohama, Japan",,13,CHI '21
inproceedings,What is a robot? an artistic approach to understand children's imaginaries about robots,"Malinverni, Laura and Valero, Cristina",2020,,Proceedings of the Interaction Design and Children Conference,,,250?€?261,Association for Computing Machinery,10.1145/3392063.3394415,https://doi.org/10.1145/3392063.3394415,"The article describes an ideographic study conducted with 10 to 11 years old students to investigate their perceptions, ideas and imaginaries about robots. Its objective is to use this understanding to expand the ways of thinking the pedagogy of educational robotics. The study employed an art-based research approach and focused on involving students in the process of producing a fictional audiovisual narrative about robots. We analyzed their creative process and the resulting video through a multimodal approach. This analysis allowed identifying the different imaginaries, discourses and ideas that the participants have around the concept of","New York, NY, USA",,9.78145E+12,,,"robots, qualitative analysis, pedagogy, imaginaries, educational robotics, art-based research","London, United Kingdom",,12,IDC '20
inproceedings,Neuroevolution of self-interpretable agents,"Tang, Yujin and Nguyen, Duong and Ha, David",2020,,Proceedings of the 2020 Genetic and Evolutionary Computation Conference,,,414?€?424,Association for Computing Machinery,10.1145/3377930.3389847,https://doi.org/10.1145/3377930.3389847,"Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail.1","New York, NY, USA",,9.78145E+12,,,,Canc\'{u,,11,GECCO '20
inproceedings,The Heuristic of Sufficient Explanation: Implications for Human-Agent Interaction,"Vonasch, Andrew James",2022,,Proceedings of the 10th International Conference on Human-Agent Interaction,,,3,Association for Computing Machinery,10.1145/3527188.3561943,https://doi.org/10.1145/3527188.3561943,"The heuristic of sufficient explanation (HOSE) is a process by which people learn hidden information about other agents. If the publicly available reasons for behaviour seem insufficient to explain the agent's behaviour, people look for hidden reasons that would explain it. I will present evidence for HOSE across several contexts, including economic decision-making, belief in conspiracy theories, judgments of other agents?€? bad intentions, and romantic attraction. I will discuss implications for Human-Agent Interaction more broadly, including for perceptions of non-human agents?€? motives.","New York, NY, USA",,9.78145E+12,,,,"Christchurch, New Zealand",,1,HAI '22
inproceedings,Explaining the Behavior of POMDP-based Agents Through the Impact of Counterfactual Information,"Mahmud, Saaduddin and Vazquez-Chanlatte, Marcell and Witwicki, Stefan and Zilberstein, Shlomo",2024,,Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,,,1346?€?1354,International Foundation for Autonomous Agents and Multiagent Systems,,,"In this work, we consider AI agents operating in Partially Observable Markov Decision Processes (POMDPs)-a widely-used framework for sequential decision making with incomplete state information. Agents operating with partial information take actions not only to advance their underlying goals but also to seek information and reduce uncertainty. Despite rapid progress in explainable AI, research on separating information-driven vs. goal-driven behaviors remains sparse. To address this gap, we introduce a novel explanation generation framework called Sequential Information Probing (SIP), to investigate the direct impact of state information, or its absence, on agent behavior. To quantify the impact we also propose two metrics under this SIP framework called Value of Information (VoI) and Influence of Information (IoI). We then theoretically derive several properties of these metrics. Finally, we present several experiments, including a case study on an autonomous vehicle, that illustrate the efficacy of our method.","Richland, SC",,9.7984E+12,,,"explainability, pomdps, value of information","Auckland, New Zealand",,9,AAMAS '24
inproceedings,Trust Repair in Human-Agent Teams: The Effectiveness of Explanations and Expressing Regret,"Kox, E.S. and Kerstholt, J.H. and Hueting, T.F. and de Vries, P.W.",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,1944?€?1946,International Foundation for Autonomous Agents and Multiagent Systems,,,"Trust is a fundamental aspect of teamwork in Human-Agent Teams (HATs). Trust violations are an inevitable aspect of the cycle of trust, so effective trust repair strategies are needed to ensure durable and successful team performance. This study explores the effectiveness of four trust repair strategies. In a first-person shooter resembling HAT task, a trust violation was provoked when the robotic agent failed to detect an approaching enemy. After this, the agent offered an apology composed of an explanation and/or an expression of regret (either one alone, both or neither). Our results indicated that expressing regret was crucial for effective trust repair, and that trust repair was most effective when the apology contained both components.","Richland, SC",,9.78145E+12,,,"trust repair, trust, human-agent teaming","Virtual Event, New Zealand",,3,AAMAS '22
inproceedings,Explaining BDI Agent Behaviour through Dialogue,"Dennis, Louise A. and Oren, Nir",2021,,Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems,,,429?€?437,International Foundation for Autonomous Agents and Multiagent Systems,,,"BDI agents act in response to external inputs and their internal plan library. Understanding the root cause of BDI agent action is often difficult, and in this paper we present a dialogue based approach for explaining the behaviour of a BDI agent. We consider two dialogue participants who may have different views regarding the beliefs, plans and external events which drove agent action (encoded via traces). These participants make utterances which incrementally reveal their traces to each other, allowing them to identify divergences in the traces, or to conclude that their traces agree. In practice, we envision a human taking on the role of a dialogue participant, with the BDI agent itself acting as the other participant. The dialogue then facilitates explanation, understanding and debugging of BDI agent behaviour. After presenting our formalism and its properties, we describe our implementation of the system and provide an example of its use in a simple scenario.","Richland, SC",,9.78145E+12,,,"bdi, dialogues, explanation","Virtual Event, United Kingdom",,9,AAMAS '21
inproceedings,"User Study on the Trustworthiness, Usability and Explainability of Intent-based and Large Language Model-based Career Planning Conversational Agents","Zylowski, Thorsten and Sautchuk-Patricio, Nathalia and Hettmann, Wladimir and Anderer, Katharina and Fischer, Karl and W\",2025,,Proceedings of the 2024 16th International Conference on Education Technology and Computers,,,46?€?53,Association for Computing Machinery,10.1145/3702163.3702409,https://doi.org/10.1145/3702163.3702409,"Choosing a career and educational path is a challenging decision for young people. Career planning conversational agents (CAs) can assist by identifying suitable occupations and educational paths. Trustworthiness is an important dimension for the acceptance of a career planning CA and is influenced by several factors. We conducted a user study with n=114 participants across three schools in Germany to explore the trustworthiness of different career planning CAs. We examined the correlation between trustworthiness and perceived competence, autonomy, and social relatedness from self-determination theory (SDT), as well as the explainability of interactions and several usability dimensions of the assistants. These dimensions included the ability to guide the conversation, onboarding quality, error tolerance, and information relevance. We tested three different variants of the career planning assistant: a form-based assistant, an intent-based CA, and a large language model (LLM)-based CA. The results showed that the LLM-based CA was on average significantly more trustworthy and was perceived as more explainable than the intent-based CA. Key trust factors included conversation flexibility, chatbot credibility, intent recognition, and maintenance of a secure conversation. Additionally, perceived autonomy was crucial for trust across all types of assistants and perceived relatedness for the two CAs. Our findings highlight key areas essential for developing trustworthy CAs.","New York, NY, USA",,9.7984E+12,,,"Career Planning Conversational Agents, Explainable Artificial Intelligence, Self-Determination Theory, Trustworthy Artificial Intelligence, Usability",,,8,
inproceedings,Towards Understanding the Effect of Voice on Human-Agent Negotiation,"Mania, Joanna and Miedema, Fieke and Browne, Rose and Broekens, Joost and Oertel, Catharine",2020,,Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents,,,,Association for Computing Machinery,10.1145/3383652.3423896,https://doi.org/10.1145/3383652.3423896,"Virtual agents are increasingly being used for communication training such as public speaking-, job interviews-, as well as negotiation training. In these use-cases the agent is generally taking on the role of interviewer and its behaviour is altered according to the nonverbal cues of its human interlocutor. However, understanding how the agent's non-verbal cues influence human behaviour, perception or interactions outcomes is equally important. This contributes to appropriate behaviour generation in agents, but also to our understanding of the intricate interplay of non-verbal behaviours on human perception and interaction outcomes.This study focuses specifically on the perception of vocal dominance in human-agent negotiation. Earlier research showed that the perception of dominance influences decision making in the course of negotiations, as do concessions tactics. However, the effect of voice as well as the effect of the negotiator type in this regard have been so far under-explored. To close this gap, an online experiment was conducted, in which a total number of 121 participants negotiated with conversational agents displaying either low or high vocal dominance, and an individualistic or neutral concession tactic. The results showed that when taking into account the self-reported type of negotiator, significant differences caused by vocal dominance were evident in regard to the number of negotiation rounds and perceived fairness. The number of rounds was significantly higher for the competitive participant type negotiating with the low vocal dominance agent, and the perceived fairness was significantly lower with the collaborative participant type negotiating with the high vocal dominance agent.","New York, NY, USA",38,9.78145E+12,,,"Vocal Dominance, Virtual Agents, Negotiation, Concession Tactic","Virtual Event, Scotland, UK",,8,IVA '20
inproceedings,Automatic Fuzzy Cognitive Maps for Explainable Image-based Pneumonia Detection,"Sovatzidi, Georgia and Vasilakakis, Michael and Iakovidis, Dimitris",2024,,Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics,,,74?€?78,Association for Computing Machinery,10.1145/3635059.3635071,https://doi.org/10.1145/3635059.3635071,"Pneumonia has a significant impact on morbidity and mortality worldwide and is associated with serious diseases, such as coronavirus disease 2019 (COVID-19). Pneumonia diagnosis is typically performed by medical experts, trained to evaluate chest x-rays, which is usually a difficult and time-consuming task. To address this problem, in this paper, a novel classification scheme based on a Fuzzy Cognitive Map (FCM) is introduced. The proposed FCM model is applied for the detection of foci of consolidation, which is a common radiographic manifestation of pneumonia, while enabling the explanation of the outcome using linguistic terms. Also, unlike most FCM models, it is automatic, in the sense that it does not require any manual intervention for the construction of the fuzzy graph. Experimental results using publicly available datasets demonstrate the effectiveness of the introduced model.","New York, NY, USA",,9.7984E+12,,,"Fuzzy logic, explainability, fuzzy cognitive maps, pneumonia, x-rays","Lamia, Greece",,5,PCI '23
inproceedings,Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents,"Qian, Peizhu and Unhelkar, Vaibhav",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,1083?€?1091,International Foundation for Autonomous Agents and Multiagent Systems,,,"Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user's mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users.","Richland, SC",,9.78145E+12,,,"shared mental models, machine teaching, human-AI collaboration, explainable AI, Monte-Carlo tree search","Virtual Event, New Zealand",,9,AAMAS '22
inproceedings,Understanding the Role of Inequality in Creating and Sustaining the Alcohol Harm Paradox using Agent-Based Modelling,"Boyd, Jennifer",2021,,Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems,,,1797?€?1798,International Foundation for Autonomous Agents and Multiagent Systems,,,"This PhD applies a different perspective and novel methods to attempt to understand a complex health phenomenon, the Alcohol Harm Paradox (AHP). This is the consistent finding that poorer people experience greater risk of alcohol harm despite drinking the same or less as richer people. Thus far a variable-centric approach focused on testing the relationships between health behaviours and harm has dominated research on the AHP. This thesis shifts to a mechanism-based approach rooted in the theories of health inequality to understand how the determinants of health inequality; from the actions and interactions of individuals to the impact of society, contribute to the AHP. Simulation methods, specifically agent-based models, are implemented to test the explanatory value of health inequality theory in creating and sustaining the AHP.","Richland, SC",,9.78145E+12,,,"health inequality theory, modelling and simulation of societies, social simulation","Virtual Event, United Kingdom",,2,AAMAS '21
inproceedings,Automatic Matching of Processes and Employees Based on Transformer and Multi-level Understanding,"Wu, Hongjian and Wang, Junjie and Zhang, Yunhui and Cao, Zhigang and Zhang, Xiaofeng and Jin, Gui",2025,,Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum,,,495?€?499,Association for Computing Machinery,10.1145/3718491.3718571,https://doi.org/10.1145/3718491.3718571,"When implementing processes, power companies need to identify process risks and assign suitable employees to complete the corresponding tasks. Traditional process matching strategies rely on manual matching of employees and processes, which results in high labor costs. With the advancement of artificial intelligence, automated matching of processes and employees based on AI has become possible. In this paper, we propose a novel method for automatic matching of processes and employees based on Transformer and multi-level understanding. This method first uses a text encoder to extract embedding vectors from the descriptions related to employees and processes. Then, we design a multi-level understanding module to help the model further focus on risk-related semantic information, improving the quality of the embedding vectors. Finally, we use Transformer to obtain the CLS embedding vectors corresponding to employees and processes, and calculate similarity using these vectors to achieve precise matching. Experimental results on a proprietary dataset demonstrate the effectiveness of our method.","New York, NY, USA",,9.7984E+12,,,"Automatic Matching, Multi-level understanding, Transformer",,,5,
inproceedings,Understanding Interviewees?€? Perceptions and Behaviour towards Verbally and Non-verbally Expressive Virtual Interviewing Agents,"Thakkar, Jinal Hitesh and Rao, Pooja S B. and Shubham, Kumar and Jain, Vaibhav and Jayagopi, Dinesh Babu",2022,,Companion Publication of the 2022 International Conference on Multimodal Interaction,,,61?€?69,Association for Computing Machinery,10.1145/3536220.3558802,https://doi.org/10.1145/3536220.3558802,"Recent technological advancements have boosted the usage of virtual interviewing platforms where the candidates interact with a virtual interviewing agent or an avatar that has human-like behavior instead of face-to-face interviews. As a result, it is essential to understand how candidates perceive these virtual interviewing avatars and whether adding features to boost the system?€?s interaction makes a difference. In this work, we present the results of two studies in which a virtual interviewing avatar with verbal and non-verbal interaction capabilities was used to conduct employment interviews. We add two interactive capabilities to the avatar, namely the non-verbal gestures and the verbal follow-up questioning and compare it with a simple interviewing avatar. We analyze the differences in perception with self-rated measures and behaviour with automatically extracted audiovisual behavioural cues. The results show that the candidates speak for a longer time, feel less stressed and have a better chance to perform with verbally and non-verbally expressive virtual interviewing agents.","New York, NY, USA",,9.78145E+12,,,,"Bengaluru, India",,9,ICMI '22 Companion
inproceedings,Value-Aligned and Explainable Agents for Collective Decision Making: Privacy Application,"Mosca, Francesca",2020,,Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,,,2199?€?2200,International Foundation for Autonomous Agents and Multiagent Systems,,,"Multiuser privacy (MP) is reported to cause concern among the users of online services, such as social networks, which do not support collective privacy management. In this research, informed by previous work and empirical studies in privacy, artificial intelligence and social science, we model a new multi-agent architecture that will support users in the resolution of MP conflicts. We design agents which are value-aligned, i.e. able to behave according to their users' moral preference, and explainable, i.e. able to justify their outputs. We will validate the efficacy of our model through user studies, oriented also to gather further insights about the usability of automated explanations.","Richland, SC",,9.78145E+12,,,"multiuser privacy, morally-aligned autonomous agents, explainable autonomous agents","Auckland, New Zealand",,2,AAMAS '20
inproceedings,Increasing Transparency to Design Inclusive Conversational Agents (CAs): Perspectives and Open Issues,"Motta, Isabela and Quaresma, Manuela",2023,,Proceedings of the 5th International Conference on Conversational User Interfaces,,,,Association for Computing Machinery,10.1145/3571884.3604304,https://doi.org/10.1145/3571884.3604304,"Artificial Intelligence (AI)-based Conversational Agents (CAs) have a great potential to include marginalized and vulnerable populations. However, some issues still make these interfaces exclusive for some users. This paper proposes to discuss how increasing CAs?€? transparency can contribute to these systems?€? inclusiveness and indicates open issues that must be addressed to make AI-based CAs more transparent and inclusive. We argue that adding more guidance to users on how CAs work, what they can do, and how they may be operated might alleviate older adults?€? misperceptions about functioning and privacy that hamper CAs?€? adoption, facilitate its usage for people with impairments, and help identify possible prejudicial biases. As challenges, researchers and practitioners should investigate how to determine appropriate levels of transparency through personalization, produce human-centered knowledge on transparency, and study new methods, tools, and processes to support CA development that considers inclusiveness.","New York, NY, USA",31,9.7984E+12,,,"Artificial Intelligence, Conversational Agents, Inclusive Design, Transparency","Eindhoven, Netherlands",,4,CUI '23
article,Explaining in Time: Meeting Interactive Standards of Explanation for Robotic Systems,"Arnold, Thomas and Kasenberg, Daniel and Scheutz, Matthias",2021,J. Hum.-Robot Interact.,,10,3,,Association for Computing Machinery,10.1145/3457183,https://doi.org/10.1145/3457183,"Explainability has emerged as a critical AI research objective, but the breadth of proposed methods and application domains suggest that criteria for explanation vary greatly. In particular, what counts as a good explanation, and what kinds of explanation are computationally feasible, has become trickier in light of oqaque ?€?black box?€? systems such as deep neural networks. Explanation in such cases has drifted from what many philosophers stipulated as having to involve deductive and causal principles to mere ?€?interpretation,?€? which approximates what happened in the target system to varying degrees. However, such post hoc constructed rationalizations are highly problematic for social robots that operate interactively in spaces shared with humans. For in such social contexts, explanations of behavior, and, in particular, justifications for violations of expected behavior, should make reference to socially accepted principles and norms. In this article, we show how a social robot?€?s actions can face explanatory demands for how it came to act on its decision, what goals, tasks, or purposes its design had those actions pursue and what norms or social constraints the system recognizes in the course of its action. As a result, we argue that explanations for social robots will need to be accurate representations of the system?€?s operation along causal, purposive, and justificatory lines. These explanations will need to generate appropriate references to principles and norms?€?explanations based on mere ?€?interpretability?€? will ultimately fail to connect the robot?€?s behaviors to its appropriate determinants. We then lay out the foundations for a cognitive robotic architecture for HRI, together with particular component algorithms, for generating explanations and engaging in justificatory dialogues with human interactants. Such explanations track the robot?€?s actual decision-making and behavior, which themselves are determined by normative principles the robot can describe and use for justifications.","New York, NY, USA",25,,,Sep-21,"normative HRI, architectural requirements, Explainability",,,23,
inproceedings,Automatic Dance Video Segmentation for Understanding Choreography,"Endo, Koki and Tsuchida, Shuhei and Fukusato, Tsukasa and Igarashi, Takeo",2024,,Proceedings of the 9th International Conference on Movement and Computing,,,,Association for Computing Machinery,10.1145/3658852.3659076,https://doi.org/10.1145/3658852.3659076,"Segmenting dance video into short movements is a popular way to easily understand dance choreography. However, it is currently done manually and requires a significant amount of effort by experts. That is, even if many dance videos are available on social media (e.g., TikTok and YouTube), it remains difficult for people, especially novices, to casually watch short video segments to practice dance choreography. In this paper, we propose a method to automatically segment a dance video into each movement. Given a dance video as input, we first extract visual and audio features: the former is computed from the keypoints of the dancer in the video, and the latter is computed from the Mel spectrogram of the music in the video. Next, these features are passed to a Temporal Convolutional Network (TCN), and segmentation points are estimated by picking peaks of the network output. To build our training dataset, we annotate segmentation points to dance videos in the AIST Dance Video Database, which is a shared database containing original street dance videos with copyright-cleared dance music. The evaluation study shows that the proposed method (i.e., combining the visual and audio features) can estimate segmentation points with high accuracy. In addition, we developed an application to help dancers practice choreography using the proposed method.","New York, NY, USA",6,9.7984E+12,,,"dance practice, temporal convolutional networks, video segmentation","Utrecht, Netherlands",,9,MOCO '24
inproceedings,Why doesn't the conversational agent understand me? a language analysis of children speech,"Monarca, Ivonne and Cibrian, Franceli L. and Mendoza, Angel and Hayes, Gillian and Tentori, Monica",2020,,Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers,,,90?€?93,Association for Computing Machinery,10.1145/3410530.3414401,https://doi.org/10.1145/3410530.3414401,"Conversation agents have shifted the way we communicate with ubiquitous services by enabling the use of natural language communication and the analysis of acoustic and linguistic language patterns. Speech skills of children are not yet fully developed; therefore, most conversational agents frequently misunderstand them. In this research, we examined if conversational agents can uncover instances of language discrimination in children. We developed Bolita, a conversational agent using a Google Home and a Sphero Robot to encourage children to practice how to tell a joke. The results of a two week study of the use of Bolita by 37 Mexican children showed a conversational agent is more likely to misunderstand children with speech skills below average. Our results indicate that they speak less, use fewer words, and need more time to answer when interacting with a conversational agent, which may explain the challenges with the conversational agent understanding them. We close discussing the potential of conversational agents to uncover digital markers in children with language differences and suggest ways that conversational agents could be built to be more inclusive.","New York, NY, USA",,9.78145E+12,,,"joke-telling, conversational agents, children","Virtual Event, Mexico",,4,UbiComp/ISWC '20 Adjunct
inproceedings,Robots on Campus: Understanding Public Perception of Robots using Social Media,"Lee, Ahreum and Toombs, Austin L.",2020,,Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing,,,305?€?309,Association for Computing Machinery,10.1145/3406865.3418321,https://doi.org/10.1145/3406865.3418321,"As robots engage more in society in various forms, it is important to understand the public perception of robots. In this poster, we focus on a campus-centric subreddit to explore online discourse about delivery robots on university campus. We specifically identify how people share their experiences with robots and how people perceive robots in society by analyzing Reddit posts. In so doing, we raise existing concerns about the robots which give insights into acceptance and sociability in human-robot interaction.","New York, NY, USA",,9.78145E+12,,,"social analysis of robotics, human-robot interaction, delivery robots","Virtual Event, USA",,5,CSCW '20 Companion
inproceedings,Counterfactual Explanations for Reinforcement Learning Agents,"Gajcin, Jasmina",2023,,Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,,,2925?€?2927,International Foundation for Autonomous Agents and Multiagent Systems,,,"Reinforcement learning (RL) algorithms often use neural networks to represent agent's policy, making them difficult to interpret. Counterfactual explanations are human-friendly explanations which offer users actionable advice on how to change their features to obtain a desired output from a black-box model. However, methods for generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks, and can generate counterfactuals which are difficult to obtain, affecting user effort and trust. My dissertation focuses on developing methods that take into account the complexities of RL framework and provide counterfactual explanations that are easy to reach and confidently produce the desired output","Richland, SC",,9.78145E+12,,,"causality, contrastive explanations, counterfactual explanations, explainability, reinforcement learning","London, United Kingdom",,3,AAMAS '23
inproceedings,User Involvement in Training Smart Home Agents: Increasing Perceived Control and Understanding,"Sieger, Leonie Nora and Hermann, Julia and Schom\",2022,,Proceedings of the 10th International Conference on Human-Agent Interaction,,,76?€?85,Association for Computing Machinery,10.1145/3527188.3561914,https://doi.org/10.1145/3527188.3561914,"Smart home systems contain plenty of features that enhance wellbeing in everyday life through artificial intelligence (AI). However, many users feel insecure because they do not understand the AI?€?s functionality and do not feel they are in control of it. Combining technical, psychological and philosophical views on AI, we rethink smart homes as interactive systems where users can partake in an intelligent agent?€?s learning. Parallel to the goals of explainable AI (XAI), we explored the possibility of user involvement in supervised learning of the smart home to have a first approach to improve acceptance, support subjective understanding and increase perceived control. In this work, we conducted two studies: In an online pre-study, we asked participants about their attitude towards teaching AI via a questionnaire. In the main study, we performed a Wizard of Oz laboratory experiment with human participants, where participants spent time in a prototypical smart home and taught activity recognition to the intelligent agent through supervised learning based on the user?€?s behaviour. We found that involvement in the AI?€?s learning phase enhanced the users?€? feeling of control, perceived understanding and perceived usefulness of AI in general. The participants reported positive attitudes towards training a smart home AI and found the process understandable and controllable. We suggest that involving the user in the learning phase could lead to better personalisation and increased understanding and control by users of intelligent agents for smart home automation.","New York, NY, USA",,9.78145E+12,,,"supervised learning, smart homes, participation, human-agent interaction","Christchurch, New Zealand",,10,HAI '22
inproceedings,"Why, Who, What, When and How about Explainability in Human-Agent Systems","Rosenfeld, Avi and Richardson, Ariella",2020,,Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,,,2161?€?2164,International Foundation for Autonomous Agents and Multiagent Systems,,,"This paper presents a survey of issues relating to explainability in Human-Agent Systems. We consider fundamental questions about the Why, Who, What, When and How of explainability. First, we define explainability and its relationship to the related terms of interpretability, transparency, explicitness, and faithfulness. These definitions allow us to answer why explainability is needed in the system, whom it is geared to and what explanations can be generated to meet this need. We then consider when the user should be presented with this information. Last, we consider how objective and subjective measures can be used to evaluate the entire system. This last question is the most encompassing as it needs to evaluate all other issues regarding explainability.","Richland, SC",,9.78145E+12,,,"xai, machine learning transparency, machine learning interpretability, human-agent systems","Auckland, New Zealand",,4,AAMAS '20
article,The Perceptual Belief Problem: Why Explainability Is a Tough Challenge in Social Robotics,"Thellman, Sam and Ziemke, Tom",2021,J. Hum.-Robot Interact.,,10,3,,Association for Computing Machinery,10.1145/3461781,https://doi.org/10.1145/3461781,"The explainability of robotic systems depends on people?€?s ability to reliably attribute perceptual beliefs to robots, i.e., what robots know (or believe) about objects and events in the world based on their perception. However, the perceptual systems of robots are not necessarily well understood by the majority of people interacting with them. In this article, we explain why this is a significant, difficult, and unique problem in social robotics. The inability to judge what a robot knows (and does not know) about the physical environment it shares with people gives rise to a host of communicative and interactive issues, including difficulties to communicate about objects or adapt to events in the environment. The challenge faced by social robotics researchers or designers who want to facilitate appropriate attributions of perceptual beliefs to robots is to shape human?€?robot interactions so that people understand what robots know about objects and events in the environment. To meet this challenge, we argue, it is necessary to advance our knowledge of when and why people form incorrect or inadequate mental models of robots?€? perceptual and cognitive mechanisms. We outline a general approach to studying this empirically and discuss potential solutions to the problem.","New York, NY, USA",29,,,Sep-21,"understandability, social robotics, predictability, mental state attribution, intentionality, intentional stance, explainability, common ground, belief attribution, Human-robot interaction",,,15,
inproceedings,Explainable Automatic Evaluation of the Trail Making Test for Dementia Screening,"Prange, Alexander and Barz, Michael and Heimann-Steinert, Anika and Sonntag, Daniel",2021,,Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3411764.3445046,https://doi.org/10.1145/3411764.3445046,"The Trail Making Test (TMT) is a frequently used neuropsychological test for assessing cognitive performance. The subject connects a sequence of numbered nodes by using a pen on normal paper. We present an automatic cognitive assessment tool that analyzes samples of the TMT which we record using a digital pen. This enables us to analyze digital pen features that are difficult or impossible to evaluate manually. Our system automatically measures several pen features, including the completion time which is the main performance indicator used by clinicians to score the TMT in practice. In addition, our system provides a structured report of the analysis of the test, for example indicating missed or erroneously connected nodes, thereby offering more objective, transparent and explainable results to the clinician. We evaluate our system with 40 elderly subjects from a geriatrics daycare clinic of a large hospital.","New York, NY, USA",382,9.78145E+12,,,"multimodal, explainable, digital pen, cognitive assessment, automatic scoring, Trail Making Test","Yokohama, Japan",,9,CHI '21
inproceedings,CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents,"McCalmon, Joe and Le, Thai and Alqahtani, Sarra and Lee, Dongwon",2022,,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,889?€?897,International Foundation for Autonomous Agents and Multiagent Systems,,,"As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent's policy, called policy-level explanations, can lead to important insights about both the task and the agent's behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS, that summarizes an agent's policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensibility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on five RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80% of the time, compared to 10% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research findings: https://github.com/mccajl/CAPS","Richland, SC",,9.78145E+12,,,"user study, reinforcement learning, policy-level explanations, policy summary, explainable reinforcement learning, explainable machine learning","Virtual Event, New Zealand",,9,AAMAS '22
article,?€?I See What You Did There?€?: Understanding People?€?s Social Perception of a Robot and Its Predictability,"Schadenberg, Bob R. and Reidsma, Dennis and Heylen, Dirk K. J. and Evers, Vanessa",2021,J. Hum.-Robot Interact.,,10,3,,Association for Computing Machinery,10.1145/3461534,https://doi.org/10.1145/3461534,"Unpredictability in robot behaviour can cause difficulties in interacting with robots. However, for social interactions with robots, a degree of unpredictability in robot behaviour may be desirable for facilitating engagement and increasing the attribution of mental states to the robot. To generate a better conceptual understanding of predictability, we looked at two facets of predictability, namely, the ability to predict robot actions and the association of predictability as an attribute of the robot. We carried out a video human-robot interaction study where we manipulated whether participants could either see the cause of a robot?€?s responsive action or could not see this, because there was no cause, or because we obstructed the visual cues. Our results indicate that when the cause of the robot?€?s responsive actions was not visible, participants rated the robot as more unpredictable and less competent, compared to when it was visible. The relationship between seeing the cause of the responsive actions and the attribution of competence was partially mediated by the attribution of unpredictability to the robot. We argue that the effects of unpredictability may be mitigated when the robot identifies when a person may not be aware of what the robot wants to respond to and uses additional actions to make its response predictable.","New York, NY, USA",28,,,Sep-21,"social perception, responsive actions, human-robot interaction, Predictability",,,28,
inproceedings,Explaining the Behaviour of Game Agents Using Differential Comparison,"Castellano, Ezequiel and Zhang, Xiao-Yi and Arcaini, Paolo and Takisaka, Toru and Ishikawa, Fuyuki and Ikehata, Nozomu and Iwakura, Kosuke",2023,,Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,,,,Association for Computing Machinery,10.1145/3551349.3560503,https://doi.org/10.1145/3551349.3560503,"The difficulty in exploring the game balance has been increasing, especially in Game-as-a-Service (GaaS) with updates in every few weeks, and due to the complexity in game design and business models. In the limited time available for testing, using automated game agents enables much more test plays than using human test players does, and it has been accelerated by the recent progress of deep reinforcement learning. However, understanding specific behaviours of each agent is hard due to their ?€?black-box?€? nature. In this paper, we propose a method for explaining the behaviour of game agents using differential comparison between agents. This comparison approach is motivated by our experience with existing explanation techniques that often extracted uninteresting, common aspects of the behaviour. In addition, there are large potentials for the application of the comparison: between agents with different learning algorithms, between human agents and automated agents, and between test agents and users. We applied our technique to a prototype of a commercial GaaS and confirmed our technique can extract specific differences between agents.","New York, NY, USA",210,9.78145E+12,,,"testing, reinforcement learning, games, explainability","Rochester, MI, USA",,8,ASE '22
inproceedings,Beyond Fairness and Explanation: Foundations of Trustworthiness of Artificial Agents,"Malle, Bertram F.",2022,,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,4,Association for Computing Machinery,10.1145/3514094.3539570,https://doi.org/10.1145/3514094.3539570,"The topics of fairness and explainability have dominated recent discussions of ethical AI. However, these are only two criteria that would make artificial agents anywhere close to ethical. I frame the question of ethical AI, and especially ethical social robots, as the question of what would make them worthy of human trust and actually eliciting human trust. Relying on a recent investigation of the multi-dimensionality of human trust, I lay out five criteria of trustworthiness-being competent, reliable, transparent, benevolent, and having ethical integrity. I will argue that an essential ingredient of such trustworthiness is norm competence-the ability to represent, comply with, and learn relevant social-moral norms (including fairness as one among many). I discuss the challenges to implementing norm competence and the critical role that justification, not just explanation, will play in providing evidence for such competence.","New York, NY, USA",,9.78145E+12,,,"xai, trustworthiness, trust, robotethics, norms, fairness, explainability, ethical ai","Oxford, United Kingdom",,1,AIES '22
inproceedings,Localized Explanations for Automatically Synthesized Network Configurations,"Nazari, Amirmohammad and Zhang, Yongzheng and Raghothaman, Mukund and Chen, Haoxian",2024,,Proceedings of the 23rd ACM Workshop on Hot Topics in Networks,,,52?€?59,Association for Computing Machinery,10.1145/3696348.3696888,https://doi.org/10.1145/3696348.3696888,"Network synthesis simplifies network management by automatically generating distributed configurations that fulfill high-level intents. However, typical network synthesizers operate as monolithic algorithms, obscuring the internal workings of the synthesis process and showing no clear connection between the generated configurations and the global intents. Given the critical role of networks as infrastructure, it is crucial for network operators to understand the synthesized configurations to establish trust in these automatic tools. To address this challenge, we propose using subspecifications localized to each component in the network topology to enhance the interpretability of network synthesis. These subspecifications provide insights into the workings of synthesizers by connecting each component's functionalities with the global configuration intents.We propose a potential solution based on constraint simplification techniques to make the explanation of network configurations manageable. Preliminary results confirm the feasibility of simplifying constraints to a manageable size, though generating high-level subspecifications that fully capture global intents remains future work. This work highlights the importance of interpretability in network synthesis and sets the stage for future research to develop more robust and trustworthy network synthesis tools.","New York, NY, USA",,9.7984E+12,,,"Explainability, Modular Reasoning, Network Synthesis","Irvine, CA, USA",,8,HotNets '24
inproceedings,Method of automatic biomedical signals interpretation for safety supervision and optimisation of the exoskeleton-aided physiotherapy of lower extremity,"Falkowski, Piotr and Oleksiuk, Jan and Jeznach, Kajetan and Aktan, Mehmet Emin",2024,,Proceedings of the 2024 4th International Conference on Robotics and Control Engineering,,,57?€?63,Association for Computing Machinery,10.1145/3674746.3674793,https://doi.org/10.1145/3674746.3674793,"Rehabilitation robots help physiotherapists in performing repetitive and tiring tasks. Moreover, they are also capable of automating therapy. However, for such an application, they require algorithms assessing patient?€?s performance and monitoring their safety. In therapy of lower extremities aided with exoskeletons, biosignal measurements can be used additionally to dynamics parameters to provide the mentioned analyses. The paper presents a concept of assessing the treatment based on electromyography of selected muscular groups and assessing the patient?€?s emotional states based on electroencephalography. The preliminary measurements enabled defining expected muscular activity dependent on the extremity configuration and emotional states based on the frequency analysis of the scalp regions. The presented methodology contains measurement flow, interpreting algorithms, and reactions of the systems to the defined states. Use of this can contribute to the development of minimally supervised systems for home-like environment use. The presented universal method is easily transferable to the exoskeleton-based systems, even different in terms of kinematics. Furthermore, it can also be analogically introduced to the systems dedicated to the upper extremities.","New York, NY, USA",,9.7984E+12,,,"EEG, EMG, automation, exoskeleton, rehabilitation robotics","Edinburgh, United Kingdom",,7,RobCE '24
inproceedings,"Automatic Fruit Picking: An Entry-level, in Scale, Approach for Cyber Physical Systems Understanding","Loukatos, Dimitrios and Kondoyanni, Maria and G. Arvanitis, Konstantinos",2022,,Proceedings of the 25th Pan-Hellenic Conference on Informatics,,,167?€?171,Association for Computing Machinery,10.1145/3503823.3503855,https://doi.org/10.1145/3503823.3503855,"The growing demand for food supplies, caused by the global population growth, and the continuously diminishing natural resources are posing many serious challenges. Furthermore, the economic and social uncertainty and the restrictions incurred by the recent COVID-19 pandemic are favouring the research for further fruit harvesting alternatives. The modernization of agriculture, involving interoperation among networking, machine learning, robotics, and big data entities seems to provide a promising direction to tackle the abovementioned problems. These systems, officially called cyber physical ones, can have a key role in the digital transformation of the agricultural sector, provided that people getting involved, from students and scientists of agricultural engineering to farmers and consulting professionals, are capable of understanding and using the diverse set of innovative applications belonging in this area. In this regard, taking advantage of the wide availability of cheap electronic components and suitable programming environments, modern educational practices can be further updated to provide tailored agricultural engineering solutions. The work presented herein describes the efforts to utilize a small robotic arm to identify and pick small fruits, assisted by a smart camera and a voice-commanded module. Despite the selection of low cost commercial off-the-self components and the small size of the final implementation, the main challenges towards the realization of a full-scale system, suitable for farm use, are fluently outlined while additional evaluation results are provided, from the educational point of view, as well.","New York, NY, USA",,9.78145E+12,,,"Robotic arm, Fruit picking, Engineering education, Cyber physical systems, Arduino","Volos, Greece",,5,PCI '21
inproceedings,iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries,"Coscia, Adam and Holmes, Langdon and Morris, Wesley and Choi, Joon Suh and Crossley, Scott and Endert, Alex",2024,,Proceedings of the 29th International Conference on Intelligent User Interfaces,,,787?€?802,Association for Computing Machinery,10.1145/3640543.3645142,https://doi.org/10.1145/3640543.3645142,"The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM?€?s score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.","New York, NY, USA",,9.7984E+12,,,"Data visualization, educational technology, explainable AI, large language models, visual analytics","Greenville, SC, USA",,16,IUI '24
article,Automatic generation of plausible co-occurring causes for effects explanation or prediction,"Pietrantuono, Roberto and Russo, Stefano",2025,ACM Trans. Intell. Syst. Technol.,,,,,Association for Computing Machinery,10.1145/3725855,https://doi.org/10.1145/3725855,"In numerous contexts, ranging from systems safety assessment to finance and medical diagnosis, a relevant causal inference task is to predict unseen rare events ?€? the so-called black swans. These are plausible, high-impact, but unexpected events for whose prediction a probabilistic-based causal inference falls short. For instance, a safety analyst needs to hypothesize potential rare co-causes that could lead to an accident, so as to manage the most unexpected failures besides the more obvious ones. Given an effect, we use abduction to support the generation of a plausible set of explanatory hypotheses for its causes. We present a generative evolutionary strategy - called Evolutionary Abduction (EVA) - for automating abductive inference by repeatedly constructing hypothetical cause-effect instances, and then automatically assessing their plausibility as well as their novelty with respect to already known instances - a mechanism mimicking the human reasoning employed whenever we need to select the best candidates from a set of hypotheses. Experiments with four datasets confirm that EVA can construct new and realistic multiple-cause hypotheses for a given effect. EVA outperforms alternative strategies based on probabilistic-based causal inference as well as state-of-the-art evolutionary algorithms, generating closer-to-real instances in most settings and datasets.","New York, NY, USA",,,2157-6904,,"Abduction, Evolutionary Computation, Hypotheses generation",,Just Accepted,,
inproceedings,How does HCI Understand Human Agency and Autonomy?,"Bennett, Dan and Metatla, Oussama and Roudaut, Anne and Mekler, Elisa D.",2023,,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,,,,Association for Computing Machinery,10.1145/3544548.3580651,https://doi.org/10.1145/3544548.3580651,"Human agency and autonomy have always been fundamental concepts in HCI. New developments, including ubiquitous AI and the growing integration of technologies into our lives, make these issues ever pressing, as technologies increase their ability to influence our behaviours and values. However, in HCI understandings of autonomy and agency remain ambiguous. Both concepts are used to describe a wide range of phenomena pertaining to sense-of-control, material independence, and identity. It is unclear to what degree these understandings are compatible, and how they support the development of research programs and practical interventions. We address this by reviewing 30 years of HCI research on autonomy and agency to identify current understandings, open issues, and future directions. From this analysis, we identify ethical issues, and outline key themes to guide future work. We also articulate avenues for advancing clarity and specificity around these concepts, and for coordinating integrative work across different HCI communities.","New York, NY, USA",375,9.78145E+12,,,"Autonomy, Self Determination Theory, agency, boundary objects, delegation, mixed initiative, theory, user experience","Hamburg, Germany",,18,CHI '23
inproceedings,Better Understanding the Costs and Benefits of Automatic Memory Management,"Sareen, Kunal and Blackburn, Stephen Michael",2022,,Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes,,,29?€?44,Association for Computing Machinery,10.1145/3546918.3546926,https://doi.org/10.1145/3546918.3546926,"Automatic memory management relieves programmers of the burden of having to reason about object lifetimes in order to soundly reclaim allocated memory. However, this automation comes at a cost. The cost and benefits of garbage collection relative to manual memory management have been the subject of contention for a long time, and will likely remain so. However, until now, the question is surprisingly under-studied. We examine the costs and benefits of garbage collection through four studies, exploring: We conduct this study in a contemporary setting using recent CPU microarchitectures, and novel methodologies including a mark-sweep collector built upon off-the-shelf free-list allocators, allowing us to shed new light on garbage collection overheads in a modern context. We find that: The costs and benefits of garbage collection are likely to remain subject to contentious discussion. However, the methodologies and evaluations we present here provide a deeper understanding of the differences in costs between manual memory management and garbage collection.","New York, NY, USA",,9.78145E+12,,,"Automatic Memory Management, Garbage Collection, Manual Memory Management, Memory Management","Brussels, Belgium",,16,MPLR '22
inproceedings,Automatic analysis of infant engagement during play: An end-to-end learning and Explainable AI pilot experiment,"Fraile, Marc and Lindblad, Joakim and Fawcett, Christine and Sladoje, Nata\v{s",2021,,Companion Publication of the 2021 International Conference on Multimodal Interaction,,,403?€?407,Association for Computing Machinery,10.1145/3461615.3485443,https://doi.org/10.1145/3461615.3485443,"Infant engagement during play is an active area of research, related to the development of cognition. Automatic detection of engagement could benefit the research process, but existing techniques used for automatic affect detection are unsuitable for this scenario, since they rely on the automatic extraction of facial and postural features trained on clear video capture of adults. This study shows that end-to-end Deep Learning methods can successfully detect engagement of infants, without the need of clear facial video, when trained for a specific interaction task. It further shows that attention mapping techniques can provide explainability, thereby enabling trust and insight into a model?€?s reasoning process.","New York, NY, USA",,9.78145E+12,,,"video analysis, infant engagement, explainable artificial intelligence, end-to-end learning, deep learning, class activation mapping","Montreal, QC, Canada",,5,ICMI '21 Companion
inproceedings,AI-VQA: Visual Question Answering based on Agent Interaction with Interpretability,"Li, Rengang and Xu, Cong and Guo, Zhenhua and Fan, Baoyu and Zhang, Runze and Liu, Wei and Zhao, Yaqian and Gong, Weifeng and Wang, Endong",2022,,Proceedings of the 30th ACM International Conference on Multimedia,,,5274?€?5282,Association for Computing Machinery,10.1145/3503161.3548387,https://doi.org/10.1145/3503161.3548387,"Visual Question Answering (VQA) serves as a proxy for evaluating the scene understanding of an intelligent agent by answering questions about images. Most VQA benchmarks to date are focused on those questions that can be answered through understanding visual content in the scene, such as simple counting, visual attributes, and even a little challenging questions that require extra encyclopedic knowledge. However, humans have a remarkable capacity to reason dynamic interaction on the scene, which is beyond the literal content of an image and has not been investigated so far. In this paper, we propose Agent Interaction Visual Question Answering (AI-VQA), a task investigating deep scene understanding if the agent takes a certain action. For this task, a model not only needs to answer action-related questions but also to locate the objects in which the interaction occurs for guaranteeing it truly comprehends the action. Accordingly, we make a new dataset based on Visual Genome and ATOMIC knowledge graph, including more than 19,000 manually annotated questions, and will make it publicly available. Besides, we also provide an annotation of the reasoning path while developing the answer for each question. Based on the dataset, we further propose a novel method, called ARE, that can comprehend the interaction and explain the reason based on a given event knowledge base. Experimental results show that our proposed method outperforms the baseline by a clear margin.","New York, NY, USA",,9.78145E+12,,,"visual question answer, vision and language, dataset","Lisboa, Portugal",,9,MM '22
inproceedings,Understanding ASL Learners?€? Preferences for a Sign Language Recording and Automatic Feedback System to Support Self-Study,"Hassan, Saad and Lee, Sooyeon and Metaxas, Dimitris and Neidle, Carol and Huenerfauth, Matt",2022,,Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,,Association for Computing Machinery,10.1145/3517428.3550367,https://doi.org/10.1145/3517428.3550367,"Advancements in AI will soon enable tools for providing automatic feedback to American Sign Language (ASL) learners on some aspects of their signing, but there is a need to understand their preferences for submitting videos and receiving feedback. Ten participants in our study were asked to record a few sentences in ASL using software we designed, and we provided manually curated feedback on one sentence in a manner that simulates the output of a future automatic feedback system. Participants responded to interview questions and a questionnaire eliciting their impressions of the prototype. Our initial findings provide guidance to future designers of automatic feedback systems for ASL learners.","New York, NY, USA",85,9.78145E+12,,,"American Sign Language, Automatic feedback, Education, Feedback, Interface design, Language learning, Sign languages","Athens, Greece",,5,ASSETS '22
inproceedings,Automatically Generating an Abstract Interpretation-Based Optimizer from a DSL,"Ooi, Ken Jin",2024,,"Companion Proceedings of the 2024 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity",,,28?€?30,Association for Computing Machinery,10.1145/3689491.3689968,https://doi.org/10.1145/3689491.3689968,"Just-in-Time (JIT) compilers can gain information at run time that are not available to Ahead-of-Time (AOT) compilers. As such, abstract interpretation baseline JIT compilers are common in many dynamic language implementations. Yet the reference implementation of Python --- CPython, has largely avoided implementing a baseline JIT compiler, likely due to the prohibitive maintenance costs associated with one. This paper implements an abstract-interpretation based optimizer for CPython bytecode that is easy to maintain and less error-prone by automatically generating the optimizer from a pre-existing Domain Specific Language (DSL) --- reusing the same DSL used to specify the interpreter. The key insight presented in this paper is that the very same DSL used to generate a concrete interpreter can also generate an abstract interpreter, providing multiple benefits such as being less error-prone and greater extensibility. The proposed abstract interpreter has been accepted into CPython 3.13 and forms a part of its experimental JIT compiler.","New York, NY, USA",,9.7984E+12,,,"Abstract Interpretation, DSL, JIT compiler","Pasadena, CA, USA",,3,SPLASH Companion '24
inproceedings,Towards Automatic Recognition of Perceived Level of Understanding on Online Lectures using Earables,"Kim, Dongwoo and Min, Chulhong and Kang, Seungwoo",2021,,Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers,,,158?€?164,Association for Computing Machinery,10.1145/3460418.3479323,https://doi.org/10.1145/3460418.3479323,"The COVID-19 pandemic has seriously impacted education and forced the whole education system to shift to online learning. Such a transition has been readily made by virtue of today?€?s Internet technology and infrastructure, but online learning also has limitations compared to traditional face-to-face lectures. One of the biggest hurdles is that it is challenging for teachers to instantly keep track of students?€? learning status. In this paper, we envision earables as an opportunity to automatically estimate learner?€?s understanding of learning material for effective learning and teaching, e.g., to pinpoint the part for which learners need to put more effort to understand. To this end, we conduct a small-scale exploratory study with 8 participants for 24 lectures in total and investigate learner?€?s behavioral characteristics that indicate the level of understanding. We demonstrate that those behaviors can be captured from a motion signal on earables. We discuss challenges that need to be further addressed to realize our vision.","New York, NY, USA",,9.78145E+12,,,"Understanding Level, Online Learning, Earable, Automatic Recognition","Virtual, USA",,7,UbiComp/ISWC '21 Adjunct
inproceedings,Automatically Explaining a Model: Using Deep Neural Networks to Generate Text from Causal Maps,"Shrestha, Anish and Mielke, Kyle and Nguyen, Tuong Anh and Giabbanelli, Philippe J.",2023,,Proceedings of the Winter Simulation Conference,,,2629?€?2640,IEEE Press,,,"Simulation models start as conceptual models, which list relevant factors and their relationships. In complex socio-environmental problems, these conceptual models are routinely created with participants, via a 'participatory modeling' approach. Transparency is a tenet of participatory modeling: participants should easily provide their input into the model-building process and see how that input is utilized. Although several elicitation methods are transparent, the resulting conceptual model can become too large and difficult to interpret. Usability studies have shown that participants struggle to interact with such large conceptual models, even if they contributed to creating parts of it. In this paper, we propose to automatically transform these large conceptual models into a more familiar format for participants: textual reports. We designed and implemented a process combining Natural Language Generation (via the deep learning GPT-3 model) and Network Science. Two case studies demonstrate that our prototype generates sentences that perform satisfactorily on several metrics.",,,,,,,"Singapore, Singapore",,12,WSC '22
inproceedings,Blackbox adversarial attacks and explanations for automatic speech recognition,"Wu, Xiaoliang",2022,,Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,,,1765?€?1769,Association for Computing Machinery,10.1145/3540250.3558906,https://doi.org/10.1145/3540250.3558906,"Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. The computational core of ASRs are Deep Neural Networks (DNNs) that have been shown to be susceptible to adversarial perturbations and exhibit unwanted biases and ethical issues. To assess the security of ASRs, we propose techniques that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Apart from that, to figure out why ASRs(always blackbox) are easily attacked, we provide explanation methods on ASRs that help increase our understanding of the system and ultimately help build trust in the system.","New York, NY, USA",,9.78145E+12,,,"Explanation, Blackbox Testing, Automatic Speech Recognition","Singapore, Singapore",,5,ESEC/FSE 2022
inproceedings,Explaining GBDT by Probabilistic Finite-State Automata,"Chen, Yinkai and Zhang, Rui and Qiu, Xin and Li, Xin and Deng, Yuxin",2022,,Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition,,,328?€?333,Association for Computing Machinery,10.1145/3497623.3497676,https://doi.org/10.1145/3497623.3497676,"Explainable artificial intelligence becomes vital for human users to understand and trust the decision-making process and results of machine learning methods. Unfortunately, most machine learning models are black-box and the algorithms running behind are opaque. In this work, we propose an approach to interpreting GBDT (Gradient Boosting Decision Tree Explanation) by extracting probabilistic finite-state automata from the trained model. Our method is inspired by and built upon a previous work that extracts probabilistic automata from RNN (Recurrent Neural Networks). To adapt the approach to our situation, we propose a series of techniques to ensure that the extracted probabilistic automaton approximates the GBDT model as accurately as possible. We conduct experiments on real-world datasets and our experimental results show that our method maintains a high level of fidelity of the extracted model as the size of the given GBDT model grows.","New York, NY, USA",,9.78145E+12,,,"Probabilistic finite-state automata, Interpretable machine learning, Gradient Boosting Decision Tree","Shanghai, China",,6,ICCPR '21
article,A general construction for abstract interpretation of higher-order automatic differentiation,"Laurel, Jacob and Yang, Rem and Ugare, Shubham and Nagel, Robert and Singh, Gagandeep and Misailovic, Sasa",2022,Proc. ACM Program. Lang.,,6,OOPSLA2,,Association for Computing Machinery,10.1145/3563324,https://doi.org/10.1145/3563324,"We present a novel, general construction to abstractly interpret higher-order automatic differentiation (AD). Our construction allows one to instantiate an abstract interpreter for computing derivatives up to a chosen order. Furthermore, since our construction reduces the problem of abstractly reasoning about derivatives to abstractly reasoning about real-valued straight-line programs, it can be instantiated with almost any numerical abstract domain, both relational and non-relational. We formally establish the soundness of this construction. We implement our technique by instantiating our construction with both the non-relational interval domain and the relational zonotope domain to compute both first and higher-order derivatives. In the latter case, we are the first to apply a relational domain to automatic differentiation for abstracting higher-order derivatives, and hence we are also the first abstract interpretation work to track correlations across not only different variables, but different orders of derivatives. We evaluate these instantiations on multiple case studies, namely robustly explaining a neural network and more precisely computing a neural network?€?s Lipschitz constant. For robust interpretation, first and second derivatives computed via zonotope AD are up to 4.76\texttimes{","New York, NY, USA",161,,,Oct-22,"Differentiable Programming, Abstract Interpretation",,,29,
inproceedings,Automatic Extraction of Effective Relations in Knowledge Graph for a Recommendation Explanation System,"Luo, Shi-Jun and Han, Hyoil and Chang, Qiong and Miyazaki, Jun",2023,,Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing,,,1754?€?1761,Association for Computing Machinery,10.1145/3555776.3577732,https://doi.org/10.1145/3555776.3577732,"A knowledge graph represents a network of real-world entities (i.e., objects, events, or concepts) and illustrates the relationship between entities. A recommender system can improve its reasoning and explainability using the knowledge graph. In this paper, we propose a hybrid and modular approach that combines path ranking with graph embedding; it can automatically eliminate the ineffective relations among entities and generate a better relation set for the explanation system. We conducted a user survey for performance evaluation and proved that our proposed approach provided the same quality of explanations for recommended items as our previous approach, which manually selected relations from the knowledge graph.","New York, NY, USA",,9.78145E+12,,,"error detection, knowledge graph, recommender system","Tallinn, Estonia",,8,SAC '23
inproceedings,Automatic Speech Recognition and Natural Language Understanding for Emotion Detection in Multi-party Conversations,"Popovic, Ilja and Culibrk, Dubravko and Mirkovic, Milan and Vukmirovic, Srdjan",2020,,Proceedings of the 1st International Workshop on Multimodal Conversational AI,,,31?€?38,Association for Computing Machinery,10.1145/3423325.3423737,https://doi.org/10.1145/3423325.3423737,"Conversational emotion and sentiment analysis approaches rely on Natural Language Understanding (NLU) and audio processing components to achieve the goal of detecting emotions and sentiment based on what is being said. While there has been marked progress in pushing the state-of-the-art of theses methods on benchmark multimodal data sets, such as the Multimodal EmotionLines Dataset (MELD), the advances still seem to lag behind what has been achieved in the domain of mainstream Automatic Speech Recognition (ASR) and NLU applications and we were unable to identify any widely used products, services or production-ready systems that would enable the user to reliably detect emotions from audio recordings of multi-party conversations. Published, state-of-the-art scientific studies of multi-view emotion recognition seem to take it for granted that a human-generated or edited transcript is available as input to the NLU modules, providing no information of what happens in a realistic application scenario, where audio only is available and the NLU processing has to rely on text generated by ASR. Motivated by this insight, we present a study designed to evaluate the possibility of applying widely-used state-of-the-art commercial ASR products as the initial audio processing component in an emotion-from-speech detection system. We propose an approach which relies on commercially available products and services, such as Google Speech-to-Text, Mozilla DeepSpeech and the NVIDIA NeMo toolkit to process the audio and applies state-of-the-art NLU approaches for emotion recognition, in order to quickly create a robust, production-ready emotion-from-speech detection system applicable to multi-party conversations.","New York, NY, USA",,9.78145E+12,,,"neural networks, natural language understanding, emotion recognition, automatic speech recognition, affective computing","Seattle, WA, USA",,8,MuCAI ?20
article,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Messina, Pablo and Pino, Pablo and Parra, Denis and Soto, Alvaro and Besa, Cecilia and Uribe, Sergio and And\'{\i",2022,ACM Comput. Surv.,,54,10s,,Association for Computing Machinery,10.1145/3522747,https://doi.org/10.1145/3522747,"Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to (1) Datasets, (2) Architecture Design, (3) Explainability, and (4) Evaluation Metrics. Our survey identifies interesting developments but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.","New York, NY, USA",203,,0360-0300,Jan-22,"explainable artificial intelligence, deep learning, medical images, natural language report, medical image captioning, Medical report generation",,,40,
inproceedings,Automatically Generated Supernodes for AST Interpreters Improve Virtual-Machine Performance,"Basso, Matteo and Bonetta, Daniele and Binder, Walter",2023,,Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences,,,1?€?13,Association for Computing Machinery,10.1145/3624007.3624050,https://doi.org/10.1145/3624007.3624050,"Abstract syntax tree (AST) interpreters allow implementing programming languages in a straight-forward way. However, AST interpreters implemented in object-oriented languages, such as e.g. in Java, often suffer from two serious performance issues. First, these interpreters commonly implement AST nodes by leveraging class inheritance and polymorphism, leading to many polymorphic call sites in the interpreter implementation and hence lowering interpreter performance. Second, widely used implementations of these interpreters throw costly runtime exceptions to model the control flow. Even though Just-in-Time (JIT) compilation mitigates these issues, performance in the first stages of the program execution remains poor. In this paper, we propose a novel technique to improve both interpreter performance and steady-state performance, lowering also the pressure on the JIT compiler. Our technique automatically generates AST supernodes ahead-of-time, i.e., we automatically generate compound AST-node classes that encode the behavior of several other primitive AST nodes before the execution of the application. Our technique extracts common control-flow structures from an arbitrary, given set of ASTs, such as e.g. the functions of popular packages. It is based on matchmaking of AST structures, instantiation of matching supernodes, and replacement of the corresponding AST subtrees with the instantiated supernodes at load-time. We implement our technique in the GraalVM JavaScript engine, showing that our supernodes lead to an average interpreter speedup of 1.24x, an average steady-state speedup of 1.14x, and an average just-in-time compilation speedup of 1.33x on the web-tooling benchmark suite.","New York, NY, USA",,9.7984E+12,,,"Truffle, Performance Optimization, JavaScript, Interpreter, GraalVM, Code Generation, AST","Cascais, Portugal",,13,GPCE 2023
inproceedings,?€?You Really Get Me?€?: Conversational AI Agents That Can Truly Understand and Help Users,"Zhou, Michelle",2020,,Proceedings of the 14th ACM Conference on Recommender Systems,,,3,Association for Computing Machinery,10.1145/3383313.3418436,https://doi.org/10.1145/3383313.3418436,"Have you watched the movie Her? Have you ever wondered or wished to have an AI companion like Samantha, who could tell you what you really are, whom your best teammate may be, and which career path would be best for you? In this talk, Michelle will present a framework for building hyper-personalized, conversational Artificial Intelligent (AI) agents who can deeply understand users and responsibly guide user behavior in both virtual and real world. Through live demos, she will highlight two technical advances of the framework: (1) evidence-based personality inference and (2) model-based conversation generation. Michelle will discuss real-world applications of these agents and the wider implications of enabling hyper-personalized conversational AI agents for businesses and individuals.","New York, NY, USA",,9.78145E+12,,,,"Virtual Event, Brazil",,1,RecSys '20
inproceedings,Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models,"Sarsa, Sami and Denny, Paul and Hellas, Arto and Leinonen, Juho",2022,,Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1,,,27?€?43,Association for Computing Machinery,10.1145/3501385.3543957,https://doi.org/10.1145/3501385.3543957,"This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.","New York, NY, USA",,9.78145E+12,,,"Robosourcing, Resource generation, Programming exercises, OpenAI Codex, Natural language generation, Large language models, GPT-3, Exercise generation, Code explanations, CS1, Automated feedback","Lugano and Virtual Event, Switzerland",,17,ICER '22
inproceedings,EXAM: An Explainable Attention-based Model for COVID-19 Automatic Diagnosis,"Shi, Wenqi and Tong, Li and Zhuang, Yuchen and Zhu, Yuanda and Wang, May D.",2020,,"Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics",,,,Association for Computing Machinery,10.1145/3388440.3412455,https://doi.org/10.1145/3388440.3412455,"The ongoing coronavirus disease 2019 (COVID-19) is still rapidly spreading and has caused over 7,000,000 infection cases and 400,000 deaths around the world. To come up with a fast and reliable COVID-19 diagnosis system, people seek help from machine learning area to establish computer-aided diagnosis systems with the aid of the radiological imaging techniques, like X-ray imaging and computed tomography imaging. Although artificial intelligence based architectures have achieved great improvements in performance, most of the models are still seemed as a black box to researchers. In this paper, we propose an Explainable Attention-based Model (EXAM) for COVID-19 automatic diagnosis with convincing visual interpretation. We transform the diagnosis process with radiological images into an image classification problem differentiating COVID-19, normal and community-acquired pneumonia (CAP) cases. Combining channel-wise and spatial-wise attention mechanism, the proposed approach can effectively extract key features and suppress irrelevant information. Experiment results and visualization indicate that EXAM outperforms recent state-of-art models and demonstrate its interpretability.","New York, NY, USA",36,9.78145E+12,,,"radiological imaging, image classification, explainability, automatic diagnosis, attention mechanism, COVID-19","Virtual Event, USA",,6,BCB '20
inproceedings,LongTale: Toward Automatic Performance Anomaly Explanation in Microservices,"Li, Richard and Du, Min and Wang, Zheng and Chang, Hyunseok and Mukherjee, Sarit and Eide, Eric",2022,,Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering,,,5?€?16,Association for Computing Machinery,10.1145/3489525.3511675,https://doi.org/10.1145/3489525.3511675,"Performance troubleshooting is notoriously difficult for distributed microservices-based applications. A typical root-cause diagnosis for performance anomaly by an analyst starts by narrowing down the scope of slow services, investigates into high-level performance metrics or available logs in the slow components, and finally drills down to an actual cause. This process can be long, tedious, and sometimes aimless due to the lack of domain knowledge and the sheer number of possible culprits. This paper introduces a new machine-learning-driven performance analysis system called LongTale that automates the troubleshooting process for latency-related performance anomalies to facilitate the root cause diagnosis and explanation. LongTale builds on existing application-layer tracing in two significant aspects. First, it stitches application-layer traces with corresponding system stack traces, which enables more informative root-cause analysis. Second, it utilizes a novel machine-learning-driven analysis that feeds on the combined data to automatically uncover the most likely contributing factor(s) for given performance slowdown. We demonstrate how LongTale can be utilized in different scenarios, including abnormal long-tail latency explanation and performance interference analysis.","New York, NY, USA",,9.78145E+12,,,"tail latency, performance analysis, cross-layer tracing","Beijing, China",,12,ICPE '22
inproceedings,Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,"Huang, Jiaxin and Meng, Yu and Han, Jiawei",2022,,Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,,605?€?614,Association for Computing Machinery,10.1145/3534678.3539443,https://doi.org/10.1145/3534678.3539443,"We study the problem of few-shot Fine-grained Entity Typing (FET), where only a few annotated entity mentions with contexts are given for each entity type. Recently, prompt-based tuning has demonstrated superior performance to standard fine-tuning in few-shot scenarios by formulating the entity type classification task as a ''fill-in-the-blank'' problem. This allows effective utilization of the strong language modeling capability of Pre-trained Language Models (PLMs). Despite the success of current prompt-based tuning approaches, two major challenges remain: (1) the verbalizer in prompts is either manually designed or constructed from external knowledge bases, without considering the target corpus and label hierarchy information, and (2) current approaches mainly utilize the representation power of PLMs, but have not explored their generation power acquired through extensive general-domain pre-training. In this work, we propose a novel framework for few-shot FET consisting of two modules: (1) an entity type label interpretation module automatically learns to relate type labels to the vocabulary by jointly leveraging few-shot instances and the label hierarchy, and (2) a type-based contextualized instance generator produces new instances based on given instances to enlarge the training set for better generalization. On three benchmark datasets, our model outperforms existing methods by significant margins.","New York, NY, USA",,9.78145E+12,,,"entity typing, few-shot learning, prompt-based learning","Washington DC, USA",,10,KDD '22
inproceedings,Explaining transformer-based models for automatic short answer grading,"Poulton, Andrew and Eliens, Sebas",2022,,Proceedings of the 5th International Conference on Digital Technology in Education,,,110?€?116,Association for Computing Machinery,10.1145/3488466.3488479,https://doi.org/10.1145/3488466.3488479,"Over recent years, advances in natural language processing have brought ever more advanced and expressive language models to the world. With open-source implementations and model registries, these state-of-the-art models are freely available to anyone, and the successful application of transfer learning has meant benchmarks on previously difficult tasks can be beaten with relative ease. In this regard, Automatic Short Answer Grading (ASAG) is no different. Unfortunately, an infallible ASAG system is beyond the reach of current models, and so there is an onus on any ASAG implementation to keep a human in the loop to ensure answers are being accurately graded. To assist the humans in the loop, one may apply various explainability methods to a model prediction to give clues as to why the model came to its conclusion. However, amongst the many available models and explainability techniques, which ones provide the best accuracy and most intuitive explanations? This work proposes a framework by which this decision can be made, and assesses several popular transformer-based models with various explainability methods on the widely used benchmark dataset from Semeval-2013.","New York, NY, USA",,9.78145E+12,,,,"Busan, Republic of Korea",,7,ICDTE '21
article,Troi: Towards Understanding Users Perspectives to Mobile Automatic Emotion Recognition System in Their Natural Setting,"Dissanayake, Vipula and Tang, Vanessa and Elvitigala, Don Samitha and Wen, Elliott and Wu, Michelle and Nanayakkara, Suranga",2022,Proc. ACM Hum.-Comput. Interact.,,6,MHCI,,Association for Computing Machinery,10.1145/3546738,https://doi.org/10.1145/3546738,"Emotional Self-Awareness (ESA) plays a vital role in physical and mental well-being. Recent advancements in artificial intelligence technologies have shown promising emotion recognition results, opening new opportunities to build systems to support ESA. However, little research has been done to understand users' perspectives on artificial-intelligence-based emotion recognition systems. We introduce Troi, an automatic emotion recognition mobile app using wearable signals. With Troi, we ran a multi-day user study with 12 users to understand user preference parameters, such as perceived accuracy, confidence, preferred emotion representations, effect of self-awareness of emotions, and real-time use cases. Further, we extend our study to evaluate the machine learning model in-the-wild to understand behaviours in-the-wild. We found that users perceived accuracy of the emotion recognition model is higher than the actual model prediction accuracy; there was no strong preference for one specific emotion representation, and users' self-awareness of emotions improved over time.","New York, NY, USA",203,,,Sep-22,"physiological signals, mobile application, in-the-wild study, automatic emotion recognition",,,22,
inproceedings,AdaAX: Explaining Recurrent Neural Networks by Learning Automata with Adaptive States,"Hong, Dat and Segre, Alberto Maria and Wang, Tong",2022,,Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,,574?€?584,Association for Computing Machinery,10.1145/3534678.3539356,https://doi.org/10.1145/3534678.3539356,"Recurrent neural networks (RNN) are widely used for handling sequence data. However, their black-box nature makes it difficult for users to interpret the decision-making process. We propose a new method to construct deterministic finite automata to explain RNN. In an automaton, states are abstracted from hidden states produced by the RNN, and the transitions represent input symbols. Thus, users can follow the paths of transitions, called patterns, to understand how a prediction is produced. Existing methods for extracting automata partition the hidden state space at the beginning of the extraction, which often leads to solutions that are either inaccurate or too large in size to comprehend. Unlike previous methods, our approach allows the automata states to be formed adaptively during the extraction. Instead of defining patterns on pre-determined clusters, our proposed model, AdaAX, identifies small sets of hidden states determined by patterns with finer granularity in data. Then these small sets are gradually merged to form states, allowing users to trade fidelity for lower complexity. Experiments show that our automata can achieve higher fidelity while being significantly smaller in size than baseline methods on synthetic and complex real datasets.","New York, NY, USA",,9.78145E+12,,,"automata, model diagnosis, post-hoc explanations, recurrent neural networks","Washington DC, USA",,11,KDD '22
article,ViSig: Automatic Interpretation of Visual Body Signals Using On-Body Sensors,"Cao, Yifeng and Dhekne, Ashutosh and Ammar, Mostafa",2023,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,,7,1,,Association for Computing Machinery,10.1145/3580797,https://doi.org/10.1145/3580797,"Visual body signals are designated body poses that deliver an application-specific message. Such signals are widely used for fast message communication in sports (signaling by umpires and referees), transportation (naval officers and aircraft marshallers), and construction (signaling by riggers and crane operators), to list a few examples. Automatic interpretation of such signals can help maintaining safer operations in these industries, help in record-keeping for auditing or accident investigation purposes, and function as a score-keeper in sports. When automation of these signals is desired, it is traditionally performed from a viewer's perspective by running computer vision algorithms on camera feeds. However, computer vision based approaches suffer from performance deterioration in scenarios such as lighting variations, occlusions, etc., might face resolution limitations, and can be challenging to install. Our work, ViSig, breaks with tradition by instead deploying on-body sensors for signal interpretation. Our key innovation is the fusion of ultra-wideband (UWB) sensors for capturing on-body distance measurements, inertial sensors (IMU) for capturing orientation of a few body segments, and photodiodes for finger signal recognition, enabling a robust interpretation of signals. By deploying only a small number of sensors, we show that body signals can be interpreted unambiguously in many different settings, including in games of Cricket, Baseball, and Football, and in operational safety use-cases such as crane operations and flag semaphores for maritime navigation, with &gt; 90% accuracy. Overall, we have seen substantial promise in this approach and expect a large body of future follow-on work to start using UWB and IMU fused modalities for the more general human pose estimation problems.","New York, NY, USA",4,,,Mar-23,"IMU, UWB, body signals, fallback communication, gestures, on-body sensors, postures, sports automation, visual signalling",,,27,
